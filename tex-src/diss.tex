% vim: spell spelllang=en_us

\documentclass[
  %oneside,
  twoside,
  BCOR=12mm,
  %openany
]{scrbook} % remove draft option to show pgf plots

%\usepackage[l2tabu,orthodox]{nag} % warn about bad habits :)

\usepackage[T1]{fontenc} % font encoding (allows hyphenation with umlaut)
\usepackage[utf8]{inputenc} % allow utf-8 characters
\usepackage{lmodern} % nice font
\usepackage[english]{babel} % hyphenation, bibliography, chapter, section, ...
\usepackage{csquotes} % context sensitive quotes
\usepackage{amsmath,amsfonts,amssymb} % ams
\usepackage{amsthm} % enhanced \newtheorem
%\usepackage{microtype} % nicer letter spacing
\usepackage{scrhack} % scrbook + float fix http://tex.stackexchange.com/questions/51867/koma-warning-about-toc
\allowdisplaybreaks % allow breaks in align, ...

% manual hyphenation (separated by spaces)
\hyphenation{
  ma-the-ma-tisch-en
  Schr\"o-ding-er
  Eli-sa-beth
}

% configure bibliography
\usepackage[backend=biber,safeinputenc,firstinits=true,doi=false,isbn=false,url=false,maxbibnames=5]{biblatex}
\addbibresource{diss.bib}

% insert git revision
\input{revision.tex}

\newcommand{\doctitle}{Recycling Krylov subspace methods for sequences of linear systems}
\newcommand{\docsubtitle}{Analysis and applications}
\newcommand{\docauthor}{Andr\'e Gaul}
\newcommand{\docdate}{\revision}
% set title, subtitle, author, ... for titlepage
\title{\doctitle}
\subtitle{\docsubtitle}
\author{\docauthor}
\date{\docdate}

\usepackage{caption}
\usepackage[font=normalsize]{subcaption}
\renewcommand{\topfraction}{0.75}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{calc}
\usetikzlibrary{intersections}
\usetikzlibrary{patterns}

% uncomment next 3 lines to disable caching of pgf plots
%\usepgfplotslibrary{external}
%\tikzexternalize[mode=list and make]{main}
%\tikzsetexternalprefix{cache/}

%--------------------
% maybe these can removed in the final version (smileys, skulls, ...)
% maybe you have to run pdflatex once instead of lualatex!
%\usepackage{skull} % \skull
%\usepackage{marvosym} % \Smiley{}
% 'debug' mode:
%\usepackage{todonotes} % \todo{}
%\usepackage{showkeys} % show ref keys
%\usepackage{refcheck} % check for numbered but unlabeled refs and unreferenced
%                      % labels
%--------------------

\usepackage{bbm} % allow for double stroked small letters
\usepackage{MnSymbol}
\usepackage{color}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow} % cell spanning multiple rows
\usepackage{xstring}
\usepackage{ifthen}
\usepackage{mathdots} % nicer ddots in matrices
\usepackage{enumitem} % resumed enumerates
\setlist[enumerate,2]{ref=\theenumi.\alph*)} % ref nested like 2.a)

\usepackage{ccicons} % creative commons icons
%\usepackage{titling} % custom title page
\usepackage{numprint} % pretty print numbers
\usepackage{varwidth} % center itemize/description

\usepackage{algorithm} % algorithm floating env
\usepackage{algpseudocode} % algorithmicx
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\newcommand{\pushcode}[1][1]{\hskip#1\dimexpr\algorithmicindent\relax}
\newcommand{\algindent}[1]{\dimexpr\algorithmicindent*#1\relax}
\newcommand{\algwidth}[1]{\dimexpr\linewidth-\algorithmicindent*#1\relax}
\newcommand{\algmulti}[2][]{\parbox[t]{\algwidth{#1}}{#2\strut}}

\usepackage{listings} % actual code
\lstset{
  language=Python, % Python, Python, Python!
  basicstyle=\ttfamily, % monospaced font
  breaklines=true, % break lines
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  xleftmargin=0.5cm,
}

% http://tex.stackexchange.com/questions/33979/include-a-line-break-in-algorithmic-while-maintaining-indentation
%\usepackage{varwidth} % for multilines in algorithms (like minipages)

\usepackage[normalem]{ulem} % used for strikeout \sout
% The bm package defines a command \bm which makes its argument bold.
% The argument may be any maths object from a single symbol to an expression.
% This is closely related to the specification of the \boldsymbol command in
% AMS-LaTeX, but \bm is rather more careful in the way it does things.
%\usepackage{bm}
\usepackage{chngcntr} % number figures, algorithms by chapter, e.g., Algorithm 2.1
\counterwithin{figure}{chapter}
\counterwithin{algorithm}{chapter}
\counterwithin{table}{chapter}

\usepackage{hyperref} % load as last package! contains url package
\hypersetup{
  draft=false,
  pdftitle={\doctitle~--~\docsubtitle},
  pdfauthor={\docauthor},
  colorlinks=true,
  linkcolor=black,
  citecolor=black,
  filecolor=black,
  urlcolor=black
}

% uppercase cali: operators on infinite-dimensional spaces
\newcommand\operatorinf[1]{\ensuremath{\mathsf{#1}}}
% uppercase bold: operators on finite-dimensional spaces (also matrices)
\newcommand\operatorfin[1]{\ensuremath{\mathbf{#1}}}
% lowercase: vector
\newcommand\vect[1]{\ensuremath{#1}}
% uppercase: tuples of vectors
\newcommand\vecttuple[1]{\ensuremath{#1}}
% uppercase: vector-spaces
\newcommand\vectspace[1]{\ensuremath{\mathcal#1}}
% input autogenerated commands
\input{autogen_commands.tex}

% number sets
\newcommand\N{\ensuremath{\mathbb{N}}}
\newcommand\R{\ensuremath{\mathbb{R}}}
\newcommand\C{\ensuremath{\mathbb{C}}}
\newcommand\Polys{\ensuremath{\mathbb{P}}}

% other helpers
\newcommand\DEF{\ensuremath{\mathrel{\mathop:}=}}
\newcommand\FED{\ensuremath{\mathrel{=\mathop:}}}
\newcommand\igralnl[4]{\ensuremath{\int\nolimits_{#1}^{#2} #3 \, \mathrm{d} #4}}
\newcommand\iu{\ensuremath{\mathbbm{i}}} % imaginary unit
\newcommand\e{\ensuremath{\mathrm{e}}} % Euler's number
\newcommand\clos[1]{\ensuremath{\overline{#1}}} % closure
\newcommand\conj[1]{\ensuremath{\overline{#1}}} % complex conjugate
\newcommand\tp{\ensuremath{\mathsf{T}}} % transpose
\newcommand\htp{\ensuremath{\mathsf{H}}} % hermitian transpose
\newcommand\inv{\ensuremath{{-1}}} % inverse
\newcommand\adj{\ensuremath{{\star}}} % adjoint
\newcommand\idx{\ensuremath{{(i)}}}
\newcommand\til[1]{\ensuremath{\widetilde{#1}}}
\newcommand\wh[1]{\ensuremath{\widehat{#1}}}
\newcommand\restr[2]{\ensuremath{\left.{#1}\right|_{#2}}}
\newcommand\id{\ensuremath{\mathsf{id}}}
\newcommand\zr{\ensuremath{\mathsf{0}}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand\LRA{\ensuremath{\Longrightarrow}}
\newcommand\lra{\ensuremath{\longrightarrow}}
\newcommand\LLRA{\ensuremath{\Longleftrightarrow}}
\newcommand\llra{\ensuremath{\longleftrightarrow}}
\newcommand\RA{\ensuremath{\Rightarrow}}
\newcommand\ra{\ensuremath{\rightarrow}}
\newcommand{\Span}[1]{\ensuremath\lsem{#1}\rsem}
\newcommand\eff{\ensuremath{\text{eff}}}
\newcommand\CG{\ensuremath{\text{CG}}} % conjugate gradient
\newcommand\MR{\ensuremath{\text{MR}}} % minimal residual
\newcommand\Laplace{\ensuremath{\Delta}}
\newcommand\PetrovAndOrGalerkin{\mbox{(Petrov--)}\allowbreak Galerkin}

% inner product
%  \ip{x}{y}    results in <x,y>
%  \ip[M]{x}{y} results in <x,y>_M
\newcommand{\ip}[3][]{\ensuremath{{\left\langle{#2},{#3}\right\rangle}_{#1}}}
% ip dots
\newcommand{\ipdots}[1][]{\ensuremath{{\left\langle{\cdot},{\cdot}\right\rangle}_{#1}}}
% norm
\newcommand{\nrm}[2][]{\ensuremath{\left\|#2\right\|_{#1}}}
\newcommand{\nrmdots}[1][]{\ensuremath{\left\|\cdot\right\|_{#1}}}
% 2-norm
\newcommand{\nrmeuc}[1]{\nrm[2]{#1}}
% norm induced by inner product
\newcommand{\nrmip}[1]{\nrm[\ipdots]{#1}}
% absolute value
\newcommand{\abs}[1]{\left|#1\right|}

% input experiment
\newcommand{\inputplot}[1]{\input{figures/#1.tikz}}
% \protect is from http://www.latex-community.org/forum/viewtopic.php?f=5&t=12497
\newcommand{\inputraw}[1]{\protect\input{raw/#1}}

% ------------------------------------------------------------------------------
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\In}{In}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Real}{Re}
\DeclareMathOperator{\Imag}{Im}
\DeclareMathOperator{\AMG}{AMG}
% ------------------------------------------------------------------------------
\newlength\figurewidth
\newlength\figureheight
% ------------------------------------------------------------------------------
\newtheorem{thm}{Theorem}[chapter] % numbering by chapter, e.g., Theorem 2.12
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{ex}[thm]{Example}
\newtheorem{ass}[thm]{Assumption}
\newtheorem{setup}[thm]{Setup}

% ------------------------------------------------------------------------------
\begin{document}
\pagenumbering{alph}
\begin{titlepage}
  \null
  \vspace{1cm}
  \begin{center}
    \huge\sffamily\bfseries
    \doctitle
  \end{center}
  \begin{center}
    \Large\sffamily\bfseries
    \docsubtitle
  \end{center}
  \vspace{0.75cm}
  \begin{center}
    vorgelegt von Diplom-Mathematiker\\
    \medskip
    {\sffamily\bfseries\Large\docauthor}\\
    \medskip
    geboren in K\"oln
  \end{center}
  \vspace{0.75cm}
  \begin{center}
    Von der Fakult\"at II -- Mathematik und Naturwissenschaften\\
    der Technischen Universit\"at Berlin\\
    zur Erlangung des akademischen Grades\\
    Doktor der Naturwissenschaften\\
    -- Dr.\ rer.\ nat. --\\
    vorgelegte Dissertation.
  \end{center}
  \vspace{0.75cm}
  \begin{center}
    \begin{varwidth}{\textwidth}
      Promotionsausschuss:
      \begin{description}[style=multiline,
                          leftmargin=3.5cm,font=\normalfont,noitemsep]
        \item[Vorsitzende:] Prof.\ Dr.\ Noemi Kurt (TU Berlin)
        \item[1. Gutachter:] Prof.\ Dr.\ J\"org Liesen (TU Berlin)
        \item[2. Gutachter:] Prof.\ Dr.\ Reinhard Nabben (TU Berlin)
        \item[3. Gutachter:] Prof.\ Dr.\ Kees Vuik (TU Delft)
      \end{description}

      Tag der wissenschaftlichen Aussprache: 3. Juli 2014
    \end{varwidth}
  \end{center}
  \vspace{0.65cm}
  \begin{center}
    Berlin 2014

    \vfill

    D 83\\
    {\textcolor{white}{\sffamily\docdate}}
  \end{center}
\end{titlepage}

\frontmatter
\pagenumbering{roman}


\chapter*{Abstract}

In several applications, one needs to solve a sequence of linear systems with
changing matrices and right hand sides. This thesis concentrates on the analysis
and application of recycling Krylov subspace methods for solving such sequences
efficiently. The well-definedness of deflated CG, MINRES and GMRES methods and
their relationship to augmentation is analyzed. Furthermore, the effects
of perturbations on projections, spectra of deflated operators and Krylov
subspace methods are studied. The analysis leads to convergence bounds for
deflated methods which provide guidance for the automatic selection of recycling
data. A novel approach is based on approximate Krylov subspaces and also
gives valuable insight in the case of non-normal operators. Numerical
experiments with nonlinear Schr\"odinger equations show that the overall time
consumption is reduced by up to 40\% by the derived automatic recycling
strategies.

\vfill

\begin{center}
  \ccbysa\\
  \footnotesize
  This work is licensed under the Creative Commons Attribution-ShareAlike 4.0
  International License. To view a copy of this license, visit
  \url{http://creativecommons.org/licenses/by-sa/4.0/deed.en_US}.
\end{center}

\chapter*{Acknowledgements}

I deeply wish to thank Jenny for all the love and joy we experienced and will
experience in the future -- this thesis would not have been written without you!
\textcolor{white}{<3}

This work greatly benefited from an enormous number of people whom I wish to
thank. First of all, my advisors J\"org Liesen and Reinhard Nabben deserve
special thanks for their support, research ideas and inspiring discussions.
Together with Volker Mehrmann, I also thank them for keeping away from me most
of the administrative tasks and I appreciate the complete freedom for my
research. I am also very grateful that the DFG Forschungszentrum MATHEON
supported my work in the MATHEON project C29. Furthermore, I thank Martin
Gutknecht for the fruitful collaboration and Kees Vuik for accepting to
referee my thesis. Special thanks go to my friend Nico Schl\"omer not only
for the collaboration which had a tremendous impact on this work.

I am glad to have met my friends and colleagues who made my time at TU Berlin so
amazing, in particular Daniel Arroyo, Manuel Baumann, Carlos Echeverr\'ia Serur,
Luis Garc\'ia Ramos, Jan Heiland \& Xiao Ai Zhou, Sebastian Holtz \& family,
Elisabeth Kamm \& Piero and Martin Sassi, Ute Kandler, Sophia Kohle, Robert
Luce, Sonja Peschutter \& Henning M\"oldner, Daniel Pfisterer, Federico Poloni,
Timo Reis, Thorsten Rohwedder \& family, Olivier S\`ete and Patrick Winkert.
Also the activists of the warm-hearted Freifunk and c-base families deserve a
thank you for welcoming me and for making such important technical and political
progress. Finally, I wish to thank my parents Jutta and Franz-Josef, my brother
Mirko, Anna West and my whole family for the lasting support.

This thesis and the accompanying software package KryPy~\cite{krypy}
benefited from the universe of free software packages around the Python
programming language, in particular NumPy, SciPy~\cite{scipy},
matplotlib~\cite{Hun07}, matplotlib2tikz~\cite{matplotlib2tikz},
shapely~\cite{shapely}, IPython~\cite{PerG07},
FEniCS/DOLFIN~\cite{LogW10,LogWH12} and PyAMG~\cite{pyamg}. Furthermore, the
\LaTeX{} typesetting language and the PGFplots package helped to shape the
appearance of this work.


\tableofcontents
\mainmatter
\pagenumbering{arabic}

\addchap{Notation}
\label{ch:not}

\begin{description}[style=multiline,leftmargin=3.2cm,font=\normalfont,noitemsep]
  \item[$\N$, $\R$, $\C$]
    Set of non-negative integers, real numbers and complex numbers.
  \item[$\N_+$, $\R_{\geq}$, $\R_+$]
    Set of positive integers, non-negative real numbers and positive real
    numbers.
  \item[$\iu$]
    Imaginary unit.
  \item[$\Real$, $\Imag$]
    Real and imaginary part.
  \item[$\Polys_n$]
    Set of polynomials of degree at most $n$.
  \item[$\Polys_{n,0}$]
    Set of polynomials of degree at most $n$ with value 1 at the origin:
    $\Polys_{n,0}=\{p \in\Polys_n~|~p(0)=1\}$.
  \item[$\Polys_{n,\infty}$]
    Set of monic polynomials of degree $n$:
    $\Polys_{n,\infty}=\{p\in\Polys_n~|~p(\lambda)
    =\lambda^n+\sum_{i=0}^{n-1}\alpha_i\lambda^i,~
    \alpha_0,\ldots,\alpha_{n-1}\in\C\}$. % see \cite{GreT94} for naming
  \item[$\alpha$]
    Scalar (Greek lowercase).
  \item[$\vsV$]
    Vector space (calligraphic uppercase).
  \item[$\vv$]
    Element of vector space $\vv\in\vsV$ (lowercase).
  \item[$\vtV$]
    Tuple of vectors $\vtV=[\vv_1,\ldots,\vv_k]\in\vsV^k$ (uppercase). \\
    Also used as linear operator
    $\vtV:\C^k\lra\vsV,~\vtV\vx\DEF\sum_{i=1}^k\vv_i\vx_i$.
  \item[$\vtV^\adj$]
    Adjoint of $\vtV$, i.e.,
    $\vtV^\adj:\vsV\lra\C^k,~\vtV^\adj\vv\DEF\ip{\vtV}{\vv}$.
  \item[$\Span{\vtV}=\Span{\vv_1,\ldots,\vv_k}$]
    Linear span of $\vtV=[\vv_1,\ldots,\vv_k]\in\vsV^k$: \\
    $\Span{\vtV}=\Span{\vv_1,\ldots,\vv_k}=\spn\{\vv_1,\ldots,\vv_k\}$.
  \item[$\ip{\vv}{\vw}$]
    Inner product with $\ip{\alpha\vv}{\vw}=\conj{\alpha}\ip{\vv}{\vw}$ and
    $\ip{\vv}{\alpha\vw}=\alpha\ip{\vv}{\vw}$.
  \item[$\ip{\vtV}{\vtW}$]
    Block inner product for $\vtV\in\vsH^m$ and $\vtW\in\vsH^n$:
    \[
      \ip{\vtV}{\vtW}=\mat{ \ip{\vv_1}{\vw_1} & \cdots & \ip{\vv_1}{\vw_n}\\
                         \vdots            & \ddots & \vdots\\
                         \ip{\vv_m}{\vw_1} & \cdots & \ip{\vv_m}{\vw_n}}
                         \in\C^{m,n}.
    \]
  \item[$\vsV\oplus\vsW$]
    Direct sum of two subspaces $\vsV,\vsW\subseteq\vsH$ with
    $\vsV\cap\vsW=\{0\}$.
  \item[$\vsL(\vsV,\vsW)$]
    Vector space of bounded linear operators between Hilbert spaces $\vsV$
    and $\vsW$:
    $\vsL(\vsV,\vsW) = \{\oiL:\vsV\lra\vsW~\text{linear and bounded}\}$.
  \item[$\oiL$]
    Linear operator between Vector spaces (sans-serif uppercase).
  \item[$\ofL$]
    Matrix (bold uppercase).
  \item[$\id$]
    Identity operator.
  \item[$\ofI_n$]
    Identity matrix $\ofI_n\in\C^{n,n}$.
  %$\zr$    & Zero operator. \\
  \item[$\oiL^\adj$, $\ofL^\htp$, $\ofL^\tp$]
    Adjoint operator, Hermitian transpose matrix and transpose matrix.
  \item[$\oiP_{\vsV,\vsW}$]
    Projection onto $\vsV$ along $\vsW$.
  \item[$\vsN(\oiL)$]
    Null space of $\oiL$.
  \item[$\vsR(\oiL)$]
    Range of $\oiL$.
  \item[$\Lambda(\oiL)$]
    Spectrum of $\oiL\in\vsL(\vsV,\vsV)$.
  \item[$\Sigma(\oiL)$]
    Singular values of $\oiL$.
  \item[$\kappa(\oiL)$]
    Condition number of linear operator $\oiL\in\vsL(\vsV,\vsW)$ with bounded
    inverse: $\kappa(\oiL)=\nrm{\oiL}\nrm{\oiL^\inv}$.
\end{description}


\chapter{Introduction}
\label{ch:intro}

In a wide range of applications, one needs to solve a sequence of linear systems
\begin{equation}
  \ofA^{(i)}\vx^{(i)}=\vb^{(i)},
  \quad i\in\{1,\dots,M\},
  \label{eq:int:seq}
\end{equation}
with matrices $\ofA^{(i)}\in\C^{N,N}$ and right hand sides $\vb^{(i)}\in\C^N$.
Such sequences arise, e.g., in the solution of nonlinear
equations with Newton's method, time-stepping schemes for time-dependent partial
differential equations and parameter optimization.  For very large and sparse
matrices, Krylov subspace methods are often an attractive choice for solving
linear systems, e.g., when direct methods fail because of exceeding requirements
of storage or computation time, and when other iterative schemes such as
multigrid methods are not applicable.

This thesis is devoted to the exploration and analysis of possibilities to
improve the convergence behavior of Krylov subspace methods in situations where
a sequence of linear systems has to be solved. A natural approach for sequential
tasks is \emph{recycling} which -- in the context of Krylov subspace methods --
means to re-use information that has been computed in the solution process of a
previous linear system in order to speed up the solution process for subsequent
linear systems.

The idea of recycling for sequences of linear systems is not new. Here, only a
brief and incomplete overview on the literature is given and it is referred to
chapter~\ref{ch:rec} for more details and pointers to further literature.
Recycling Krylov subspace methods for sequences of linear systems have been
pushed forward since 2006 most notably by Kilmer and de Sturler~\cite{KilS06},
Parks et al.~\cite{ParSMJM06} and Wang, de Sturler and Paulino~\cite{WanSP07}.
The history of the employed techniques goes back to the mid 1990s when
restarted and nested Krylov subspace methods have been proposed, e.g., by
Morgan~\cite{Mor95,Mor96}, de Sturler~\cite{Stu96} and Erhel, Burrage and
Pohl~\cite{ErhBP96}. Two ideas predominate in these works: \emph{augmentation}
and \emph{deflation}. In augmented methods, the search space is enlarged by a
subspace that is supposed to contain useful information about the solution.
In deflated methods, the matrix of the linear system is modified with a
projection in order to achieve faster convergence. The origins of the latter
approach are to be found in 1987 and 1988 in the works of
Nicolaides~\cite{Nic87} and Dost\'{a}l~\cite{Dos88}.

A multitude of algorithmic realizations of augmented and deflated Krylov
subspace methods has been proposed since then. Yet, a large number of questions
concerning the \emph{mathematical} principles and the relationship between these
methods remains open.

\paragraph{Scope and goals.}

The intention of this thesis is to close several gaps both on the side of
mathematical theory and on the side of practical applications of recycling
Krylov subspace methods. This includes a thorough analysis of the building
blocks deflation and augmentation. A special focus lies on perturbation theory
for projections, deflated matrices and Krylov subspace methods in general. For
practical applications, the results provide guidance for the automatic selection
of recycling data. In
this work, the CG, MINRES and GMRES methods are considered because of their
attractive optimality and finite termination properties (in exact arithmetic).
In practice, a preconditioner is usually used in order to accelerate a Krylov
subspace method. Deflation and augmentation techniques can often only unfold
their potential if they are used in addition to preconditioning. Here, the topic
of preconditioning is left out in large parts since, in general, it requires a
problem-dependent approach. Therefore, it is assumed that the
occurring linear systems already are in preconditioned form. Although special
emphasis is put on
mathematical aspects, this thesis is accompanied by the free software Python
package KryPy~\cite{krypy} which offers easy-to-use and extensively tested
implementations of deflated and recycling CG, MINRES and GMRES methods as well
as all algorithms that are discussed in this thesis. In the following, an
overview of this thesis is provided where new contributions are highlighted.

\paragraph{Outline.}

In chapter~\ref{ch:back}, the notation is introduced and well-known results
about projections, angles between subspaces and Krylov subspace methods are
recalled in the setting of a possibly infinite-dimensional Hilbert space and
possibly singular operators. The matrix $\ofA^{(i)}$ in \eqref{eq:int:seq} may
thus be replaced by a linear operator $\oiA^{(i)}$ throughout this thesis. The
notation does not differ significantly from standard linear algebra notation and
a brief overview of the notation in section~\ref{sec:back:pre} is provided on
page~\pageref{ch:not}. With the general Hilbert space presentation, the author
wishes to make the contributions readily applicable to a broader range of
problems such as the numerical solution of PDEs where Krylov subspace methods
can operate in the natural function space and inner product of the problem. The
inclusion of singular operators is of importance in the analysis of deflated
methods in chapter~\ref{ch:rec}.  Because treatments of projections, angles
between subspaces and Krylov subspace methods in general Hilbert spaces with
possibly singular operators are rare or scattered across the literature on
(numerical) linear algebra and functional analysis, the presentation in
chapter~\ref{ch:back} is quite extensive in order to ease the reading of the
main part in chapter~\ref{ch:rec}. Experienced readers may safely skip
chapter~\ref{ch:back} and only use it as a reference on demand.

Chapter~\ref{ch:rec} constitutes the main part of this thesis. The following
questions are addressed:
\begin{enumerate}
  \item How can data be incorporated in Krylov subspace methods in
    order to accelerate them?
    (Section~\ref{sec:rec:strat})
  \item When are deflated methods well defined and what is the relationship to
    augmentation?
    (Section~\ref{sec:rec:def})
  \item How are projections, spectra of deflated operators and Krylov subspace
    methods affected by perturbations?
    (Section~\ref{sec:rec:per})
  \item What is the optimal choice of deflation vectors?
    (Section~\ref{sec:rec:sel})
  \item How do the convergence bounds derived in this chapter perform in
    numerical experiments with non-normal operators?
    (Section~\ref{sec:rec:cd})
\end{enumerate}

In section~\ref{sec:rec:strat}, an overview is given of existing strategies to
speed up Krylov subspace methods for sequences of linear systems by means of
adapting initial guesses, updating preconditioners or augmenting search spaces.

Section~\ref{sec:rec:def} expands on deflated Krylov subspace methods based on
CG, MINRES and GMRES\@. For two popular choices of projections, the question of
well-definedness, i.e., if the method yields well-defined iterates and
terminates with a solution, is systematically answered in
sections~\ref{sec:rec:def:mr} and \ref{sec:rec:def:minres}.
Theorem~\ref{thm:rec:def:mr:cond} was shown by the author, Gutknecht, Liesen and
Nabben~\cite{GauGLN13} and extends a result of Brown and Walker~\cite{BroW97} by
characterizing all linear systems with singular operators for which GMRES is
well defined. For all considered deflated variants of CG, MINRES and
GMRES, the same condition on the deflation subspace guarantees well-definedness.
Examples~\ref{ex:rec:def:break1} and \ref{ex:rec:def:break2} illustrate the
possibility of breakdowns of deflated GMRES and MINRES methods if the condition
on the deflation subspace is violated. In section~\ref{sec:rec:def:aug}, a
general equivalence theorem by the author, Gutknecht, Liesen and
Nabben~\cite{GauGLN13} is provided which shows that two widely used variants of
deflated CG and GMRES/MINRES implicitly perform augmentation. Furthermore, it is
shown in section~\ref{sec:rec:def:imp} that the necessary correction of iterates
is equivalent to the correction of the initial guess if an appropriate
projection is used as a right ``preconditioner''. An overview of equivalent
variants of well-defined deflated CG, MINRES and GMRES methods is provided in
corollary~\ref{cor:rec:def:imp:overview}.

Several new results for perturbed projections, deflated operators and Krylov
subspace methods are presented in section~\ref{sec:rec:per}. In
section~\ref{sec:rec:per:proj}, a new bound on the normwise difference
$\nrm{\oiP-\oiQ}$ of two \emph{arbitrary} projections $\oiP$ and $\oiQ$ in terms
of angles between the ranges and null spaces of the projections is presented.
The result is stated for a general Hilbert space and also holds for
infinite-dimensional subspaces. The new bound generalizes and sharpens a bound by
Berkson~\cite{Ber63} where the null spaces of both projections have to coincide.

For a finite-dimensional Hilbert space, section~\ref{sec:rec:per:def} analyzes
the spectrum of deflated operators. Theorem~\ref{thm:rec:per:def:spectrum} is a
new result that characterizes the \emph{entire} spectrum of a deflated operator
for \emph{any} invertible operator $\oiA$ by the behavior of $\oiA^\inv$ on the
orthogonal complement of the deflation subspace. The theorem does not require
$\oiA$ to be self-adjoint or positive definite and thus generalizes a result of
Nicolaides~\cite{Nic87} who characterized the smallest nonzero and largest
eigenvalue of a deflated self-adjoint and positive-definite operator $\oiA$. The
remaining part of the section concentrates on the case of self-adjoint but not
necessarily positive-definite operators. The inertia of a deflated operator is
characterized by theorem~\ref{thm:rec:per:def:inertia}.  Two further theorems
provide inclusion intervals for the eigenvalues of deflated operators in the
case where the deflation subspace is an approximate invariant subspace. In
theorem~\ref{thm:rec:per:def:angle}, the new angle-based bound on the normwise
difference of projections yields a bound on the deviation of the eigenvalues of
the deflated operator from certain eigenvalues of the original operator. The
bound involves a spectral interval gap condition and depends linearly on the
norm of the residual of the approximate eigenpairs, e.g., the Ritz residual.
However, a quadratic dependence on the Ritz residual can be observed in
numerical experiments, see example~\ref{ex:rec:per:def:angle} and
figure~\ref{fig:rec:per:def:angle}. A quadratic Ritz residual bound for each
individual eigenvalue of the deflated operator is provided in
theorem~\ref{thm:rec:per:def:quad}. Furthermore, the bound only depends on a
spectral gap which is more permissive and yields sharper bounds than the
spectral interval gap. The theorem draws on a quadratic residual bound for
eigenvalues by Mathias~\cite{Mat98} and apparently is the first result that can
explain why relatively poor approximations to invariant subspaces perform well
as deflation subspaces in certain situations.

Section~\ref{sec:rec:per:kry} discusses the behavior of Krylov subspace methods
for solving a linear system $\oiA\vx=\vb$ under perturbations of the operator
$\oiA$ and the right hand side $\vb$. Example~\ref{ex:rec:per:kry:stag} shows
that a widespread opinion is not true in general, namely that small
perturbations of the operator and the right hand side only lead to small
perturbations of the convergence behavior of Krylov subspace methods. In
theorem~\ref{thm:rec:per:kry:full}, a recently published result by Sifuentes,
Embree and Morgan~\cite{SifEM13} on the behavior of GMRES with perturbed
matrices is generalized in order to allow perturbations in the right hand side
and the initial guess. The theorem is based on the pseudospectrum of $\oiA$ and
is of importance in the later section~\ref{sec:rec:sel:kry} for the evaluation
of deflation vector candidates.

The actual selection of recycling vectors in the situation of a sequence of
linear systems is treated in section~\ref{sec:rec:sel}. Having solved a linear
system (possibly with deflation), a natural question for the selection of
deflation vectors is: which choice of deflation vectors performs best for the
\emph{same} linear system? The influence of changes in the operator and right
hand side can then be examined in a second step.

The first subsection~\ref{sec:rec:sel:ritz} provides formulas for the efficient
computation of Ritz and harmonic Ritz pairs with the corresponding residual
norms from the subspaces that are available after a run of the deflated Krylov
subspace methods from section~\ref{sec:rec:def}. This subsection can be seen as
a reference for implementations and can be skipped if implementational details
are not of interest.

In section~\ref{sec:rec:sel:pri}, the quadratic residual bound from
theorem~\ref{thm:rec:per:def:quad} is used to derive inclusion intervals for the
eigenvalues of the next deflated operator in a sequence of linear systems if the
deflation vectors are chosen as Ritz vectors from the subspaces that were
constructed during the solution process of the current linear system, cf.\
theorem~\ref{thm:rec:sel:pri}. For a given set of Ritz vectors, the eigenvalue
inclusion bounds can be used with a priori bounds in order to estimate the
convergence behavior of the deflated Krylov subspace method for the next linear
system.

Section~\ref{sec:rec:sel:kry} proposes a novel approach for assessing a given
set of Ritz vectors that is based on the construction of approximate Krylov
subspaces and is also valid for non-self-adjoint operators. Given a deflation
subspace candidate, the question is: how can the convergence behavior of a
Krylov subspace method with this deflation subspace be characterized by only
using already computed data? The Krylov subspace that would be constructed is
not contained in the available subspaces in general. However, a Krylov subspace
for a nearby operator and right hand side can be constructed.
Given any subspace $\vsV$ and the action of $\oiA$ on this subspace,
theorem~\ref{thm:rec:sel:kry:pert} shows how the operator $\oiA$ can be
optimally perturbed \emph{in each step} such that the provided subspace $\vsV$
constitutes a Krylov subspace of the perturbed operator with respect to a
\emph{specific} initial vector $\vv\in\vsV$. Theorem~\ref{thm:rec:sel:kry:def}
shows that an Arnoldi relation for such an approximate Krylov subspace is
readily available for the given deflation subspace candidate. In
theorem~\ref{thm:rec:sel:kry:gmres}, a new residual bound for deflated
GMRES/MINRES is presented based on the approximate Krylov subspace technique.
The resulting bound features a residual norm that is efficiently computable and
a pseudospectral term that takes account of the perturbations that arise from
the approximate Krylov subspace and the fact that the next linear system has to
be considered. Example~\ref{ex:rec:sel:kry:minres} shows that -- unlike
asymptotic bounds -- the new bound is able to capture different phases of the
GMRES convergence, e.g., from a transient phase of stagnation to a fast residual
norm reduction.

Numerical experiments with a non-normal operator resulting from a finite element
discretization of the convection-diffusion equation confirm the usefulness of
the new approximate Krylov subspace bound in section~\ref{sec:rec:cd}. It is
observed that the cheaply computable part of the bound is able to capture the
actual convergence behavior way beyond the point where the full bound is not
able to provide significant information due to the growth of the pseudospectral
term.

While the experiments in chapter~\ref{ch:rec} are mostly based on academic
examples and model problems in order to illustrate the mathematical theory,
chapter~\ref{ch:nls} presents an application of the proposed recycling
strategies to a more realistic problem. The
experiments show how the numerical solution of nonlinear Schr\"odinger equations
can benefit from the use of recycling MINRES methods to solve linear systems
that arise from Newton's method. The results in this chapter as well as the
underlying code PyNosh~\cite{pynosh} have been developed jointly by Schl\"omer
and the author and have been published to a great extent in~\cite{GauS13}. The
numerical experiments are shown for the Ginzburg--Landau equation which is an
important instance of nonlinear Schr\"odinger equations. In addition to a fixed
choice of deflation vectors that was used in~\cite{GauS13}, automatic recycling
strategies are employed here that are based on the new bounds in this thesis.
The recycling strategies do not require user interaction and yield the same
reduction of the overall solution time as the manually optimized number of
deflation vectors.

All experiments in this thesis can be reproduced with the available source
code\footnote{\url{https://github.com/andrenarchy/phdthesis}} and the free
software packages KryPy~\cite{krypy}, PseudoPy~\cite{pseudopy} and
PyNosh~\cite{pynosh} that emerged from this thesis and the related work
in~\cite{GauS13}.


\chapter{Background: projections and Krylov subspace methods}
\label{ch:back}

This chapter gives an overview of Krylov subspace methods and recalls important
results that are helpful in the analysis in chapter~\ref{ch:rec}.
Projections play a major role in this thesis due to the fact that the
studied Krylov subspace methods CG, MINRES and GMRES fulfill an optimality
condition which can be characterized mathematically by projections.
Furthermore, the deflation techniques that are the subject of
chapter~\ref{ch:rec} are also based on projections. For this reason the first
sections of this chapter are dedicated to the characterization of projections
and angles between subspaces of Hilbert spaces. The author found that many
important results on this topic are scattered across the literature on
(numerical) linear algebra and functional analysis. In order to keep the later
sections and chapters comprehensible, the presentation is a bit more extensive
than in most treatises of Krylov subspace methods. Furthermore, the results in
sections~\ref{sec:back:pre}--\ref{sec:back:arnoldi} are stated for a general
(possibly infinite-dimensional) Hilbert space $\vsH$ for the sake of generality
and reusability. Especially the arbitrary inner product is of relevance in
the application to the nonlinear Schr\"{o}dinger equations in
chapter~\ref{ch:nls}. When it comes to practical numerical aspects in
later sections, a transition to the finite-dimensional case $\dim\vsH<\infty$
is made. Krylov subspaces in the setting of a possibly infinite-dimensional
Hilbert space with an arbitrary inner product have been considered
in a similar manner in the works of Eiermann, Ernst and Schneider~\cite{EieES00}
and Eiermann and Ernst~\cite{EieE01}. In contrast to these works, the
presentation in this chapter does not assume that the operators at hand are
nonsingular. Taking into account possibly singular operators is
of major importance in the analysis of deflated Krylov subspace methods in
chapter~\ref{ch:rec}. Iterative methods have also been considered in the context
of Hilbert spaces by Kirby~\cite{Kir10} and for the derivation of stopping
criteria by Arioli, Noulard and Russo~\cite{AriNR01} and Arioli, Loghin and
Wathen~\cite{AriLW05}. A recent work by M\'alek and Strako{\v{s}}~\cite{MalS14}
stresses the need for a unified treatment of functional analysis and numerical
linear algebra in the solution of partial differential equations.


\section{Preliminaries}
\label{sec:back:pre}

This section introduces the basic notation and recalls basic properties of
bounded linear operators between Hilbert spaces. If questions concerning the
notation arise while reading, a glance at the notational overview on
page~\pageref{ch:not} may be worthwhile.  In the following, $\vsH$ denotes a
Hilbert space with inner product $\ipdots[\vsH]:\vsH\times\vsH\lra\C$ where
$\ip[\vsH]{\vv}{\alpha\vw}=\alpha\ip[\vsH]{\vv}{\vw}$ and
$\ip[\vsH]{\alpha\vv}{\vw}=\conj{\alpha}\ip[\vsH]{\vv}{\vw}$ for
$\vv,\vw\in\vsH$ and $\alpha\in\C$. The induced norm $\nrmdots[\vsH]$ is defined
by $\nrm[\vsH]{\vv}\DEF\sqrt{\ip[\vsH]{\vv}{\vv}}$ for $\vv\in\vsH$.  If $\vsG$
is also a Hilbert space, then a linear
operator $\oiL:\vsH\lra\vsG$ is called bounded if there exists an
$\alpha\in\R$ with $\nrm[\vsG]{\oiL\vv}\leq\alpha\nrm[\vsH]{\vv}$ for all
$\vv\in\vsH$. By $\vsL(\vsH,\vsG)$, the vector space of bounded linear operators
from $\vsH$ to $\vsG$ is denoted and $\vsL(\vsH)\DEF\vsL(\vsH,\vsH)$. For
an $\oiL\in\vsL(\vsH,\vsG)$ the sets $\vsR(\oiL)$ and
$\vsN(\oiL)$ denote the range and the null space of the operator and the
operator norm is defined by
$\nrm{\oiL}\DEF\sup_{0\neq\vv\in\vsH}\frac{\nrm[\vsG]{\oiL\vv}}{\nrm[\vsH]{\vv}}$.
The identity operator is denoted by $\id$.
For $\vv,\vw\in\vsH$, the condition $\vv\perp_\vsH\vw$ is equivalent to
$\ip[\vsH]{\vv}{\vw}=0$. For a subspace $\vsV\subseteq\vsH$, the orthogonal
complement of $\vsV$ is defined by
$\vsV^\perp\DEF\{\vz\in\vsH~|~\vz\perp_\vsH\vv~\text{for all}~\vv\in\vsV\}$ and
the closure is defined by $\clos{\vsV}\DEF\{\vz\in\vsH~|~\text{there exists a
sequence}~(\vv_n)_{n\in\N}~\text{in}~\vsV~\text{with}~\vv_n\ra\vz\}$. The
subspace $\vsV$ is called closed if $\vsV=\clos{\vsV}$. A linear operator
$\oiL\in\vsL(\vsH)$ can be applied to a subspace with
$\oiL\vsV\DEF\{\oiL\vv~|~\vv\in\vsV\}$. Two subspaces $\vsV,\vsW\subseteq\vsH$
form a direct sum $\vsV\oplus\vsW$ if $\vsV\cap\vsW=\{0\}$.
For tuples of vectors
$\vtV=[\vv_1,\dots,\vv_m]\in\vsH^m$ and $\vtW=[\vw_1,\dots,\vw_n]\in\vsH^n$, the
block inner product is defined as the matrix
\[
  \ip[\vsH]{\vtV}{\vtW}
  = \mat{\ip[\vsH]{\vv_1}{\vw_1} & \cdots & \ip[\vsH]{\vv_1}{\vw_n}\\
         \vdots                  & \ddots & \vdots\\
         \ip[\vsH]{\vv_m}{\vw_1} & \cdots & \ip[\vsH]{\vv_m}{\vw_n}}
         \in\C^{m,n}
\]
and the subspace that is spanned by the elements of a tuple is denoted by
$\Span{\vtV}=\Span{\vv_1,\dots,\vv_m}=\spn\{\vv_1,\dots,\vv_m\}$. If
$\vtV\in\vsH^m$ is interpreted as a linear operator $\vtV:\C^m\lra\vsH$, its
adjoint $\vtV^\adj:\vsH\lra\C^m$ is defined by $\vtV^\adj\vz=\ip{\vtV}{\vz}$ for
$\vz\in\vsH$. For two tuples $\vtV\in\vsH^m$ and $\vtW\in\vsH^n$, the adjoint
of $[\vtV,\vtW]$ is given by $[\vtV,\vtW]^\adj=[\vtV^\adj,\vtW^\adj]^\tp$.
Subscripts are dropped for reasons of readability if it is unambiguous.


\begin{lemma}
  \label{lem:adjoint}
  Let $\vsG$ and $\vsH$ be two Hilbert spaces and let $\oiL\in\vsL(\vsG,\vsH)$.
  Then the following holds:
  \begin{enumerate}
    \item There exists a unique bounded linear operator
      $\oiL^\adj\in\vsL(\vsH,\vsG)$, the adjoint of $\oiL$, with
      $\ip[\vsH]{\oiL\vv}{\vw} = \ip[\vsG]{\vv}{\oiL^\adj\vw}$ for all
      $\vv\in\vsG$ and $\vw\in\vsH$ and
      $\nrm{\oiL}=\nrm{\oiL^\adj}$.
    \item $\vsN(\oiL^\adj)= \vsR(\oiL)^\perp$ and
      $\clos{\vsR(\oiL^\adj)} = \vsN(\oiL)^\perp$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Cf.~\cite[Satz V.5.2]{Wer07}.
\end{proof}

\begin{lemma}
  \label{lem:orthsum}
  Let $\vsH$ and $\vsG$ be two Hilbert spaces and $\oiL,\oiM\in\vsL(\vsG,\vsH)$
  with closed ranges such that $\vsR(\oiL)\perp_\vsH \vsR(\oiM)$ and
  $\vsN(\oiL)^\perp \perp_\vsG \vsN(\oiM)^\perp$.
  Then the sum $\oiL+\oiM$ satisfies
  $\nrm{\oiL+\oiM} = \max\{\nrm{\oiL},\nrm{\oiM}\}$.
\end{lemma}

\begin{proof}
  It is first shown that
  $\nrm{\oiL+\oiM}\leq\max\{\nrm{\oiL},\nrm{\oiM}\}$.
  \begin{align*}
    \nrm{\oiL+\oiM}^2
    &= \sup_{\substack{\vz\in\vsG\\\nrm[\vsG]{\vz}=1}}
      \nrm[\vsH]{ (\oiL+\oiM)\vz}^2
    = \sup_{\substack{\vv\in\vsN(\oiL)^\perp,\vw\in\vsN(\oiM)^\perp\\
      \nrm[\vsG]{\vv+\vw}=1}}
      \nrm[\vsH]{ (\oiL+\oiM)(\vv+\vw)}^2 \\
    &= \sup_{\substack{\vv\in\vsN(\oiL)^\perp,\vw\in\vsN(\oiM)^\perp\\
      \nrm[\vsG]{\vv+\vw}=1}}
      \nrm[\vsH]{ \oiL\vv+\oiM\vw}^2
    = \sup_{\substack{\vv\in\vsN(\oiL)^\perp,\vw\in\vsN(\oiM)^\perp\\
      \nrm[\vsG]{\vv+\vw}=1}}
      \left(\nrm[\vsH]{ \oiL\vv}^2 + \nrm[\vsH]{\oiM\vw}^2\right) \\
    &\leq \sup_{\substack{\vv\in\vsN(\oiL)^\perp,\vw\in\vsN(\oiM)^\perp\\
      \nrm[\vsG]{\vv+\vw}=1}}
      \left(\nrm[\vsH]{ \oiL}^2 \nrm[\vsG]{\vv}^2 + \nrm[\vsH]{\oiM}^2
      \nrm[\vsG]{\vw}^2\right) \\
    &\leq \max\{\nrm{ \oiL}^2,\nrm{\oiM}^2\}
      \sup_{\substack{\vv\in\vsN(\oiL)^\perp,\vw\in\vsN(\oiM)^\perp\\
      \nrm[\vsG]{\vv+\vw}=1}}
      \left(\nrm[\vsG]{\vv}^2 + \nrm[\vsG]{\vw}^2\right) \\
    &\leq \max\{\nrm{ \oiL}^2,\nrm{\oiM}^2\}
      \sup_{\substack{\vv\in\vsN(\oiL)^\perp,\vw\in\vsN(\oiM)^\perp\\
      \nrm[\vsG]{\vv+\vw}=1}}
      \nrm[\vsG]{\vv + \vw}^2
    = \max\{\nrm{ \oiL}^2,\nrm{\oiM}^2\}.
  \end{align*}
  Without loss of generality
  $\nrm{\oiL}=\max\{\nrm{\oiL},\nrm{\oiM}\}$.
  Then the proof is complete with the following computation:
  \begin{align*}
    \nrm{\oiL+\oiM}^2
    = \sup_{\substack{\vz\in\vsG\\\vz\neq 0}}
      \frac{\nrm[\vsH]{(\oiL+\oiM)\vz}^2}{\nrm[\vsG]{\vz}^2}
    = \sup_{\substack{\vz\in\vsG\\\vz\neq 0}}
      \frac{\nrm[\vsH]{\oiL\vz}^2+\nrm[\vsH]{\oiM\vz}^2}{\nrm[\vsG]{\vz}^2}
    \geq \nrm{\oiL}^2.
  \end{align*}
\end{proof}


\section{Projections}
\label{sec:back:proj}

In this section, the basic properties of projections on a Hilbert space are
presented. Most books on (numerical) linear algebra and functional analysis
include projections~\cite{Kat95,Yos95,Saa03,Wer07,Saa11}, but many are
restricted to orthogonal projections or only cover a subset of characterizations
that are of importance in later sections. All results in this and the
following section are known but scattered in the literature and are repeated
here with short proofs for convenience. A quite general treatment of projections
in Hilbert spaces can be found in chapter~7 of the book by
Gal{\'a}ntai~\cite{Gal04}.

\begin{definition}
  \label{def:projpre}
  A linear operator $\oiP\in\vsL(\vsH)$ is called a \emph{projection} if
  $\oiP^2=\oiP$.
\end{definition}

The next two lemmas show that a projection $\oiP\in\vsL(\vsH)$ can be
characterized completely by its range and null space.

\begin{lemma}
  \label{lem:projdecomp}
  Let $\oiP\in\vsL(\vsH)$ be a projection. Then
  $\vsH=\vsR(\oiP)\oplus\vsN(\oiP)$, where both $\vsR(\oiP)$ and $\vsN(\oiP)$ are
  closed subspaces.
\end{lemma}

\begin{proof}
  Each $\vz\in\vsH$ can be decomposed as $\vz=\oiP\vz + (\vz -\oiP\vz)$, where
  $\oiP\vz\in\vsR(\oiP)$ and $\vz-\oiP\vz\in\vsN(\oiP)$ because
  $\oiP(\vz-\oiP\vz)=\oiP\vz - \oiP^2\vz=\oiP\vz-\oiP\vz=0$.
  Let $\vv\in\vsR(\oiP)\cap\vsN(\oiP)$ arbitrary. Then $\vv=\oiP\vv$ and
  $\oiP\vv=0$ yield $\vv=0$ which shows that
  $\vsR(\oiP)\cap\vsN(\oiP)=\{0\}$ and the decomposition is thus unique.

  The null space of a bounded linear operator is always closed. In order
  to see that also its range is closed one can proceed as
  in~\cite[Theorem~7.13]{Gal04}. Let $\vw=\lim_{n\ra\infty}\oiP\vv_n
  \in\clos{\vsR(\oiP)}$ with $\vv_n\in\vsH$. Then $\oiP\vw =
  \lim_{n\ra\infty}\oiP^2\vv_n = \lim_{n\ra\infty}\oiP\vv_n = \vw$ and thus
  $\vw\in\vsR(\oiP)$.
\end{proof}

\begin{lemma}
  \label{lem:projprescribe}
  Let $\vsV,\vsW\subseteq\vsH$ be two closed subspaces such that
  $\vsH=\vsV\oplus\vsW$. Then there exists a unique projection $\oiP\in\vsH$
  with $\vsR(\oiP)=\vsV$ and $\vsN(\oiP)=\vsW$.
\end{lemma}

\begin{proof}
  Cf.~\cite[Chapter~III~\S 5.4]{Kat95} or \cite[Theorem~7.17]{Gal04}.
\end{proof}

In the light of these two lemmas the following notation for projections is
introduced:

\begin{definition}
  \label{def:proj}
  For two closed subspaces $\vsV,\vsW\subseteq\vsH$ with $\vsV\oplus\vsW=\vsH$
  the operator $\oiP_{\vsV,\vsW}\in\vsL(\vsH)$ is defined as the unique
  projection with range $\vsR(\oiP_{\vsV,\vsW})=\vsV$ and null space
  $\vsN(\oiP_{\vsV,\vsW})=\vsW$.  The projection $\oiP_{\vsV,\vsW}$ is called
  the \emph{projection onto $\vsV$ along $\vsW$}.
\end{definition}

The adjoint and complementary operator of a projection are investigated in the
following lemma.

\begin{lemma}
  \label{lem:projadjoint}
  Let $\vsV,\vsW\subseteq\vsH$ be two closed subspaces such that
  $\vsH=\vsV\oplus\vsW$. Then:
  \begin{enumerate}
    \item Also the orthogonal complements form a direct sum:
      $\vsH=\vsV^\perp\oplus \vsW^\perp$. \label{lem:projadjoint:orth}
    \item The adjoint operator of the projection $\oiP_{\vsV,\vsW}$ is given
      by $\oiP_{\vsV,\vsW}^\adj=\oiP_{\vsW^\perp,\vsV^\perp}$ and satisfies
      $\nrm{\oiP_{\vsW^\perp,\vsV^\perp}} = \nrm{\oiP_{\vsV,\vsW}}$.
      \label{lem:projadjoint:adjoint}
    \item The complementary operator of the projection $\oiP_{\vsV,\vsW}$ is
      given by $\id - \oiP_{\vsV,\vsW} = \oiP_{\vsW,\vsV}$. If
      $\vsV,\vsW\neq\vsH$, then
      $\nrm{\oiP_{\vsW,\vsV}} = \nrm{\oiP_{\vsV,\vsW}}$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  \begin{enumerate}
    \item It is first shown that $\oiP_{\vsV,\vsW}^\adj$ is a projection and
      then 1.\ and 2.\ are obtained by determining its range and null space.  In
      order to see that $\oiP_{\vsV,\vsW}^\adj$ actually is a projection, note
      that
      \begin{align*}
        \nrm{\oiP_{\vsV,\vsW}^\adj - (\oiP_{\vsV,\vsW}^\adj)^2}^2
        &= \sup_{\nrm{\vz}=1} \nrm{\left(\oiP_{\vsV,\vsW}^\adj -
          (\oiP_{\vsV,\vsW}^\adj)^2\right)\vz}^2 \\
        &= \sup_{\nrm{\vz}=1} \ip{\vz}{(\oiP_{\vsV,\vsW}
          - \oiP_{\vsV,\vsW}^2)\left(\oiP_{\vsV,\vsW}^\adj -
          (\oiP_{\vsV,\vsW}^\adj)^2\right)\vz}
        = 0.
      \end{align*}
      It follows from lemma~\ref{lem:adjoint} that
      $\nrm{\oiP_{\vsV,\vsW}^\adj} = \nrm{\oiP_{\vsV,\vsW}}$ and thus the
      adjoint operator $\oiP_{\vsV,\vsW}^\adj$ is a projection. Furthermore, the lemma reveals that
      $\vsN(\oiP_{\vsV,\vsW}^\adj) = \vsR(\oiP_{\vsV,\vsW})^\perp =
      \vsV^\perp$ and $\clos{\vsR(\oiP_{\vsV,\vsW}^\adj)} =
      \vsN(\oiP_{\vsV,\vsW})^\perp = \vsW^\perp$. Because
      $\oiP_{\vsV,\vsW}^\adj$ is a projection it can be
      deduced from lemma~\ref{lem:projdecomp} that
      $\vsR(\oiP_{\vsV,\vsW}^\adj)=\clos{\vsR(\oiP_{\vsV,\vsW}^\adj)} =
      \vsW^\perp$ and that $\vsH = \vsV^\perp\oplus \vsW^\perp$. Thus
      $\oiP_{\vsV,\vsW}^\adj = \oiP_{\vsW^\perp, \vsV^\perp}$.
    \item[3.] Let $\vz=\vv+\vw\in\vsH$ be arbitrary with $\vv\in\vsV$ and
      $\vw\in\vsW$. Then $\vv=\oiP_{\vsV,\vsW}\vv$ and
      $\vw=\oiP_{\vsW,\vsV}\vw$ and the following equation can be obtained:
      \begin{align*}
        (\id-\oiP_{\vsV,\vsW})\vz
        &= \vz - \oiP_{\vsV,\vsW}(\vv+\vw)
        = \vz - \oiP_{\vsV,\vsW}\vv
        = \vz - \vv
        = \vw\\
        &= \oiP_{\vsW,\vsV}\vw
        = \oiP_{\vsW,\vsV}(\vv+\vw)
        = \oiP_{\vsW,\vsV}\vz.
      \end{align*}
      For the norm equality $\nrm{\oiP_{\vsW,\vsV}} = \nrm{\oiP_{\vsV,\vsW}}$
      see, e.g., Szyld~\cite{Szy06}.
  \end{enumerate}
\end{proof}

The norm equality $\nrm{\oiP_{\vsW,\vsV}} = \nrm{\oiP_{\vsV,\vsW}}$ in
lemma~\ref{lem:projadjoint} has been proven in many ways, e.g., by Del
Pasqua~\cite{Del55} in 1955, Ljance~\cite{Lja58} in 1958 and Kato~\cite{Kat60}.
An extensive and readable overview of several proofs is given by Szyld
in~\cite[Theorem~2.1]{Szy06}.
% TODO: mistake in Szy06?

The next lemma characterizes projections where the range and null space are given
as sums of subspaces that satisfy certain orthogonality constraints.

\begin{lemma}
  \label{lem:projsum}
  Let $\vsV,\vsW,\vsX,\vsY\subseteq\vsH$ be closed subspaces such that
  $\vsH=\vsV\oplus\vsW^\perp=\vsX\oplus\vsY^\perp$, $\vsV\perp\vsY$ and
  $\vsX\perp\vsW$.
  Then also $\vsH=\vsV\oplus\vsX\oplus(\vsW \oplus \vsY)^\perp$ and
  \[
    \oiP_{\vsV+\vsX, (\vsW+\vsY)^\perp}
    = \oiP_{\vsV,\vsW^\perp} + \oiP_{\vsX,\vsY^\perp}.
  \]
  If furthermore $\vsV\perp\vsX$ and $\vsW\perp\vsY$, then
  \[
    \nrm{\oiP_{\vsV+\vsX, (\vsW+\vsY)^\perp}}
    = \max\{\nrm{\oiP_{\vsV,\vsW^\perp}}, \nrm{\oiP_{\vsX,\vsY^\perp}}\}.
  \]
\end{lemma}

\begin{proof}
  From the orthogonality conditions it can be seen that
  $\vsH
  =\vsV \oplus \vsW^\perp
  =\vsV \oplus (\vsW^\perp\cap\vsX) \oplus (\vsW^\perp\cap\vsX^\perp)
  =\vsV \oplus \vsX \oplus (\vsW^\perp\cap\vsX^\perp)
  $ and thus $\vsV+\vsX=\vsV\oplus\vsX$ indeed is a direct sum. Analogously,
  $\vsW+\vsY=\vsW\oplus\vsY$ holds because
  $\vsH
  =\vsW \oplus \vsV^\perp
  =\vsW \oplus (\vsV^\perp\cap\vsY) \oplus (\vsV^\perp\cap\vsY^\perp)
  =\vsW \oplus \vsY \oplus (\vsV^\perp\cap\vsY^\perp)$. Now it is verified that
  the operator $\oiQ\DEF\oiP_{\vsV,\vsW^\perp} + \oiP_{\vsX,\vsY^\perp}$ is a
  projection by computing
  \[
    \oiQ^2
    =\oiP_{\vsV,\vsW^\perp}^2 + \oiP_{\vsV,\vsW^\perp}\oiP_{\vsX,\vsY^\perp}
      + \oiP_{\vsX,\vsY^\perp}\oiP_{\vsV,\vsW^\perp}
      + \oiP_{\vsX,\vsY^\perp}^2
    =\oiP_{\vsV,\vsW^\perp} + \oiP_{\vsX,\vsY^\perp}
    =\oiQ.
  \]
  Obviously $(\vsW\oplus\vsY)^\perp \subseteq \vsN(\oiQ)$. However,
  $\oiQ\vz\neq 0$ for every nonzero $\vz\in\vsW\oplus\vsY$ because $\vsV$ and
  $\vsX$ form a direct sum $\vsV\oplus\vsX$. Thus the null space of $\oiQ$ is
  $\vsN(\oiQ)=(\vsW\oplus\vsY)^\perp$. Furthermore, it can be seen that
  $\oiQ\vsW=\vsV$ and $\oiQ\vsY=\vsX$ and thus $\vsR(\oiQ)=\vsV\oplus\vsX$ and
  $\oiQ=\oiP_{\vsV+\vsX,(\vsW+\vsY)^\perp}$.

  The norm property directly follows from lemma~\ref{lem:orthsum}.
\end{proof}

An important class of projections are orthogonal projections where the null
space is the orthogonal complement of the range:

\begin{definition}
  \label{def:orthproj}
  For a closed subspace $\vsV\subseteq\vsH$ the \emph{orthogonal projection}
  onto $\vsV$ is defined by $\oiP_\vsV\DEF\oiP_{\vsV,\vsV^\perp}$. A projection
  that is not orthogonal is called an \emph{oblique projection}.
\end{definition}

Orthogonal projections have attractive properties: they are self-adjoint and for
any vector $\vz\in\vsH$ and a closed subspace $\vsV\subseteq\vsH$, the
orthogonal projection $\oiP_\vsV$ provides the vector in $\vsV$ that is closest
to $\vz$:

\begin{thm}
  \label{thm:back:minimizer}
  Let $\vsV\subseteq\vsH$ be a closed subspace. Then
  \begin{enumerate}
    \item $\oiP_\vsV$ is self-adjoint, i.e., $\oiP_\vsV^\adj=\oiP_\vsV$.
    \item If $\vz\in\vsH$, then
    \[
      \nrm{\vz - \oiP_\vsV \vz} = \inf_{\vv\in\vsV}\nrm{\vz-\vv}.
    \]
  \end{enumerate}
\end{thm}

\begin{proof}
  The first statement immediately follows from the definition of an orthogonal
  projection and statement~\ref{lem:projadjoint:adjoint} of
  lemma~\ref{lem:projadjoint}. For the second statement the following holds for
  $\vv\in\vsV$
  \[
    \nrm{\vz-\vv}^2
    = \nrm{\vz-\oiP_\vsV\vz + \oiP_\vsV\vz-\vv}^2
    = \nrm{\vz-\oiP_\vsV\vz}^2 + \nrm{\oiP_\vsV\vz-\vv}^2
    \geq \nrm{\vz-\oiP_\vsV\vz}^2,
  \]
  where the second equality holds because
  $\vz-\oiP_\vsV\vz=\oiP_{\vsV^\perp}\vz\perp\oiP_\vsV\vz-\vv=\oiP_\vsV(\vz-\vv)$.
\end{proof}

The orthogonal projection onto the sum of two subspaces can be represented as
the sum of two projections as demonstrated in the next lemma.

\begin{lemma}
  \label{lem:back:projsum-ortho}
  Let $\vsV,\vsW\subseteq\vsH$ be closed subspaces. Then
  \[
    \oiP_{\vsV+\vsW} = \oiP_\vsV + \oiP_{\oiP_{\vsV^\perp}\vsW}
  \]
\end{lemma}

\begin{proof}
  The subspace $\vsV+\vsW$ can be decomposed into
  \[
    \vsV+\vsW
    = \oiP_\vsV (\vsV+\vsW) + \oiP_{\vsV^\perp}(\vsV+\vsW)
    = \vsV + \oiP_{\vsV^\perp}\vsW.
  \]
  The result then follows with lemma~\ref{lem:projsum} because
  $\vsV\perp\oiP_{\vsV^\perp}\vsW$.
\end{proof}

In later sections and chapter~\ref{ch:rec}, the ranges and null spaces of
projections are defined via the action of an operator on subspaces.
In these cases, the following lemma can be helpful.

\begin{lemma}
  \label{lem:projcommute}
  Let $\oiL\in\vsL(\vsH)$ and let $\vsV,\vsW\subseteq\vsH$ be two closed
  subspaces such that $\oiL\vsV\oplus\vsW^\perp=\vsH$ and $\restr{\oiL}{\vsV}$
  is invertible with bounded inverse.
  Then
  \[
    \vsV\oplus(\oiL^\adj\vsW)^\perp=\vsH
    \qquad \text{and} \qquad
    \oiP_{\oiL\vsV,\vsW^\perp} \oiL = \oiL \oiP_{\vsV,(\oiL^\adj
    \vsW)^\perp}.
  \]
\end{lemma}

\begin{proof}
  Let
  $\oiQ\DEF (\restr{\oiL}{\vsV})^\inv \oiP_{\oiL\vsV,\vsW^\perp}\oiL$
  and first note that
  $\oiL\oiQ = \oiP_{\oiL\vsV,\vsW^\perp}\oiL$ and that
  \[
    \oiQ^2
    =(\restr{\oiL}{\vsV})^\inv \oiP_{\oiL\vsV,\vsW^\perp}\oiL
      (\restr{\oiL}{\vsV})^\inv \oiP_{\oiL\vsV,\vsW^\perp}\oiL
    = (\restr{\oiL}{\vsV})^\inv \oiP_{\oiL\vsV,\vsW^\perp}^2 \oiL
    = (\restr{\oiL}{\vsV})^\inv \oiP_{\oiL\vsV,\vsW^\perp} \oiL
    = \oiQ
  \]
  is a projection because $(\restr{\oiL}{\vsV})^\inv$,
  $\oiP_{\oiL\vsV,\vsW^\perp}$ and $\oiL$ are bounded. Now it is shown that
  $\vsV\oplus(\oiL^\adj\vsW)^\perp$, $\vsR(\oiQ)=\vsV$ and
  $\vsN(\oiQ)=\vsR(\id-\oiQ)=(\oiL^\adj\vsW)^\perp$ which can be seen from the
  decomposition of any $\vz\in\vsH$ into $\vz=\oiQ\vz + (\id-\oiQ)\vz$. Then
  $\oiQ\vz=(\restr{\oiL}{\vsV})^\inv\oiP_{\oiL\vsV,\vsW^\perp}\oiL\vz \in\vsV$
  and
  $(\id-\oiQ)\vz\in(\oiL^\adj\vsW)^\perp$ can be concluded from the fact that
  for any $\vw\in\vsW$
  \begin{align*}
    \ip{\oiL^\adj\vw}{(\id-\oiQ)\vz}
    &= \ip{\vw}{\oiL(\id-\oiQ)\vz}
    = \ip{\vw}{(\id - \oiP_{\oiL\vsV,\vsW^\perp})\oiL\vz}
    = \ip{\vw}{\oiP_{\vsW^\perp,\oiL\vsV}\oiL\vz}
    = 0
  \end{align*}
  holds. Thus $\oiQ=\oiP_{\vsV,(\oiL^\adj\vsW)^\perp}$.
\end{proof}

An interesting question is: what is the inverse of a projection if it is
restricted to the orthogonal complement of its null space? The answer is
trivial for orthogonal projections since the restriction is just the identity
operator. In the general case, the answer reveals an interesting link between
oblique and orthogonal projections that is used in the analysis of
perturbed projections in section~\ref{sec:rec:per:proj}. The next lemma provides
the inverse and some other fundamental relationships between oblique
and orthogonal projections.

\begin{lemma}
  \label{lem:proj:inv}
  For two closed subspaces $\vsV,\vsW\subseteq\vsH$ with $\vsV\oplus\vsW=\vsH$,
  the following statements hold:
  \begin{enumerate}
    \item \label{lem:proj:inverse}
      The operator
      \[
        \oiQ_{\vsV,\vsW}\DEF
        \left\{
          \begin{aligned}
            \vsV&\to\vsW^\perp\\
            \vv&\mapsto \oiP_{\vsW^\perp} \vv
          \end{aligned}
        \right.
        \quad
        \text{with norm}
        \quad
        \nrm{\oiQ_{\vsV,\vsW}} = \nrm{\oiP_{\vsW^\perp}\oiP_\vsV}
      \]
      is invertible and its inverse is given by
      \[
        \oiQ_{\vsV,\vsW}^\inv :
        \left\{
          \begin{aligned}
            \vsW^\perp&\to\vsV\\
            \vw&\mapsto \oiP_{\vsV,\vsW} \vw
          \end{aligned}
        \right.
        \quad
        \text{with norm}
        \quad
        \nrm{\oiQ_{\vsV,\vsW}^\inv} = \nrm{\oiP_{\vsV,\vsW}}.
      \]
    \item $\oiP_{\vsV,\vsW} = \oiQ_{\vsV,\vsW}^\inv \oiP_{\vsW^\perp}$ with
      $\oiQ_{\vsV,\vsW}^\inv$ as in~\ref{lem:proj:inverse}.
    \item $\oiP_{\vsV,\vsW} \oiP_\vsV = \oiP_\vsV$.
      \label{lem:proj:orth}
    \item $\oiP_{\vsV,\vsW} \oiP_{\vsW^\perp} = \oiP_{\vsV,\vsW}$.
      \label{lem:proj:orth2}
  \end{enumerate}
\end{lemma}

\begin{proof}
  \begin{enumerate}
    \item Let $\vv\in\vsV$ be arbitrary. Then the statement follows from
      \[
        \vv - \oiP_{\vsV,\vsW} \oiQ_{\vsV,\vsW}\vv
        = \vv - \oiP_{\vsV,\vsW} \oiP_{\vsW^\perp}\vv
        = \oiP_{\vsV,\vsW} (\id - \oiP_{\vsW^\perp})\vv
        = \oiP_{\vsV,\vsW} \oiP_\vsW\vv
        = 0.
      \]
    \item It can be seen with statement~\ref{lem:proj:inverse} that
      \[
        \oiP_{\vsV,\vsW} - \oiQ_{\vsV,\vsW}^\inv\oiP_{\vsW^\perp}
        = \oiP_{\vsV,\vsW} - \oiP_{\vsV,\vsW}\oiP_{\vsW^\perp}
        = \oiP_{\vsV,\vsW} (\id - \oiP_{\vsW^\perp})
        = \oiP_{\vsV,\vsW} \oiP_\vsW
        = 0.
      \]
    \item $\oiP_\vsV - \oiP_{\vsV,\vsW}\oiP_\vsV
      = (\id-\oiP_{\vsV,\vsW}) \oiP_\vsV
      = \oiP_{\vsW,\vsV} \oiP_\vsV
      = 0$.
    \item $\oiP_{\vsV,\vsW} - \oiP_{\vsV,\vsW}\oiP_{\vsW^\perp}
      = \oiP_{\vsV,\vsW} ( \id - \oiP_{\vsW^\perp})
      = \oiP_{\vsV,\vsW} \oiP_\vsW
      = 0$.
  \end{enumerate}
\end{proof}

The following results by Buckholtz~\cite{Buc97,Buc00} give necessary and
sufficient conditions for two subspaces to be complementary and thus for the
existence of projections, cf.\
lemmas~\ref{lem:projdecomp} and \ref{lem:projprescribe}. These conditions are
stated in terms of
orthogonal and oblique projections onto the subspaces and their orthogonal
complements.

\begin{thm}
  \label{thm:complement}
  Let $\vsX,\vsY\subseteq\vsH$ be two closed subspaces. Then the following
  statements are equivalent:
  \begin{enumerate}
    \item $\vsH=\vsX\oplus\vsY$.
    \item \label{thm:complement:inverse}
      The operator $\oiP_\vsX - \oiP_\vsY$ is invertible and its inverse is
      given by $\oiP_{\vsX,\vsY}+\oiP_{\vsY^\perp,\vsX^\perp}
      -\id$.
    \item %$\sin\theta_{\max}(\vsX,\vsY^\perp)=
      $\nrm{\oiP_\vsX + \oiP_\vsY -\id} < 1$.
  \end{enumerate}
\end{thm}

\begin{proof}
  The proof of the equivalences was given in~\cite{Buc00} while the inverse
  in statement~\ref{thm:complement:inverse} was presented in~\cite{Buc97}.
\end{proof}

For bases of finite-dimensional subspaces $\vsV$ and $\vsW$, the following
lemma provides a way to check if $\vsV$ and $\vsW^\perp$ form a direct sum and
provides an explicit representation of the corresponding projection.

\begin{thm}
  \label{thm:back:proj_sing}
  Let $\vtV,\vtW\in\vsH^n$ be such that $\vsV=\Span{\vtV}$ and
  $\vsW=\Span{\vtW}$ have dimension $n<\infty$. Then the following statements
  are equivalent:
  \begin{enumerate}
    \item $\vsH=\vsV\oplus\vsW^\perp$.
    \item $\ip{\vtW}{\vtV}\in\C^{n,n}$ is nonsingular and the projection
      $\oiP_{\vsV,\vsW^\perp}$ can be represented by
      $\oiP_{\vsV,\vsW^\perp}\vz=\vtV\ip{\vtW}{\vtV}^\inv\ip{\vtW}{\vz}$ for
      $\vz\in\vsH$.
    \item $\nrm{\oiP_\vsV - \oiP_\vsW}<1$. \label{thm:back:proj_sing:norm}
  \end{enumerate}
\end{thm}

\begin{proof}
  1.\LRA 2.: Assume that $\ip{\vtW}{\vtV}$ is singular. Then there exists a
  nonzero $\vt\in\C^n$ such that $\ip{\vtW}{\vtV}\vt=0$ which is equivalent to
  $\vx\DEF\vtV\vt\in\vsW^\perp$. Thus there is a nonzero
  $\vx\in\vsV\cap\vsW^\perp$ which is a contradiction to
  $\vsH=\vsV\oplus\vsW^\perp$. The representation of $\oiP_{\vsV,\vsW^\perp}$
  can be verified by a trivial calculation.

  2.\LRA 1.: Follows directly from the fact that the projection
  $\oiP_{\vsV,\vsW^\perp}$ is well defined.

  3.\LLRA1.: Follows from theorem~\ref{thm:complement}.
\end{proof}

In the next section, it is shown that the norm $\nrm{\oiP_\vsV-\oiP_\vsW}$ in
condition~\ref{thm:back:proj_sing:norm} can be characterized as the sine of the
maximal canonical angle $\theta_{\max}(\vsV,\vsW)$ between the subspaces $\vsV$
and $\vsW$. Because the maximal canonical angle adds some intuition and also
does not depend on the basis of the subspaces, the condition
$\theta_{\max}(\vsV,\vsW)<\frac{\pi}{2}$ is used extensively in
chapter~\ref{ch:rec} as an equivalent condition to the ones in
theorem~\ref{thm:back:proj_sing}.

\begin{rmk}[Matrix representation of projections]
  \label{rmk:back:proj_repr}
  If $\ofV,\ofW\in\C^{m,n}$ are two matrices such that $\ofW^\htp\ofV$ is
  nonsingular, then the projection onto $\vsV\DEF\Span{\ofV}$ along
  $\vsW^\perp\DEF\Span{\ofW}^\perp$ with respect to the Euclidean inner product
  is well defined and can be represented by
  \begin{equation}
    \oiP_{\vsV,\vsW^\perp} = \ofV(\ofW^\htp\ofV)^\inv\ofW^\htp.
    \label{eq:back:proj_repr}
  \end{equation}
  Note that care has to be taken when implementing the application of a
  projection to a vector. The representations in
  theorem~\ref{thm:back:proj_sing} and equation~\eqref{eq:back:proj_repr} may
  be problematic in the presence of round-off errors and it is referred to
  section~\ref{sec:back:round-off} for a brief discussion of numerically sound
  algorithms for the application of projections.
\end{rmk}


\section{Angles and gaps between subspaces}
\label{sec:back:ang}

In a Hilbert space, the inner product $\ipdots$ can be used to measure the angle
between two vectors and the concept of angles can be extended to subspaces.
It is well-known in the literature that certain properties of projections on
Hilbert spaces can be characterized by angles between the involved subspaces,
i.e., the ranges and null spaces of the projections. Besides adding a helpful
geometric intuition to subspaces, angles play an important role in the analysis
of approximate invariant subspaces such as in the theory of Davis and
Kahan~\cite{DavK70} which is used in section~\ref{sec:rec:per:def} in order
to analyze the spectrum of deflated operators. Furthermore, the basic results of
this section pave the way for an apparently new result on the behavior of
projections with perturbed ranges and null spaces in
section~\ref{sec:rec:per:proj}.

The next definition introduces the minimal canonical angle between two closed
subspaces of a Hilbert space.

\begin{definition}
  \label{def:minangle}
  For two closed and nonzero subspaces $\vsV,\vsW\subseteq\vsH$ the
  \emph{minimal canonical angle} $\theta_{\min}(\vsV,\vsW)\in[0,\frac{\pi}{2}]$
  between $\vsV$ and $\vsW$ is defined by
  \[
    \cos \theta_{\min}(\vsV,\vsW)
    = \sup_{\substack{\vv\in\vsV, \nrm{\vv}=1 \\
      \vw\in\vsW, \nrm{\vw}=1}} |\ip{\vv}{\vw}|.
  \]
\end{definition}

The well-definedness of the minimal canonical angle directly follows from the
Cauchy--Schwarz inequality. The following lemma gathers basic properties of the
minimal canonical angle and shows that it can be expressed as the norm of the
product of orthogonal projections.

\begin{lemma}
  \label{lem:orthproj}
  Let $\vsV,\vsW\subseteq\vsH$ be closed and nonzero subspaces. Then:
  \begin{enumerate}
    \item $\theta_{\min}(\vsV,\vsW) = \theta_{\min}(\vsW,\vsV)$.
      \label{lem:orthproj:symm}
    \item $|\ip{\vv}{\vw}|\leq \nrm{\vv} \nrm{\vw}
      \cos \theta_{\min}(\vsV,\vsW)$ for all $\vv\in\vsV$ and
      $\vw\in\vsW$ (Cauchy--Schwarz inequality for subspaces).
    \item $\nrm{\oiP_\vsV \oiP_\vsW} = \nrm{\oiP_\vsW \oiP_\vsV} = \cos
      \theta_{\min}(\vsV,\vsW)$.
      \label{lem:orthproj:prod}
    \item $\nrm{\oiL\oiP_{\vsV+\vsW}}
      \leq \nrm{\oiL\oiP_{\vsV}} + \nrm{\oiL\oiP_{\vsW}} $
      for $\oiL\in\vsL(\vsH)$.
    \item If $\vsV\neq\vsH$ and $\dim\vsW=1$ then
      $\cos^2 \theta_{\min}(\vsV,\vsW) +
      \cos^2 \theta_{\min}(\vsV^\perp,\vsW) = 1$.
      \label{lem:orthproj:onedim}
  \end{enumerate}
\end{lemma}

\begin{proof}
  \begin{enumerate}
    \item Is a direct consequence of the inner product's symmetry.
    \item If $\vv=0$ or $\vw=0$ the statement is clear. For $\vv\neq0$ and
      $\vw\neq0$ the following holds:
      \begin{align*}
        |\ip{\vv}{\vw}|
        &= \nrm{\vv} \nrm{\vw}
          \abs{\ip{\frac{\vv}{\nrm{\vv}}}{\frac{\vw}{\nrm{\vw}}} }
        \leq \nrm{\vv} \nrm{\vw}
          \sup_{\substack{\vx\in\vsV,\nrm{\vx}=1\\
          \vy\in\vsW,\nrm{\vy}=1}}
          \abs{\ip{\vx}{\vy}} \\
        &= \nrm{\vv} \nrm{\vw}
          \cos \theta_{\min}(\vsV,\vsW).
      \end{align*}
    \item Cf.\ Szyld~\cite[Lemma 5.1]{Szy06}.
    \item $
      \begin{aligned}[t]
        \nrm{\oiL\oiP_{\vsV+\vsW}}
        &= \nrm{\restr{\oiL}{\vsV+\vsW}}
        = \sup_{\substack{\vv\in\vsV,\vw\in\vsW\\ \nrm{\vv+\vw}=1}}
          \nrm{\oiL(\vv+\vw)}
        \leq \sup_{\substack{\vv\in\vsV,\vw\in\vsW\\ \nrm{\vv+\vw}=1}}
          \left(\nrm{\oiL\vv}+\nrm{\oiL\vw}\right)\\
        &\leq \sup_{\substack{\vv\in\vsV,\vw\in\vsW\\ \nrm{\vv+\vw}=1}}
          \nrm{\oiL\vv} +
          \sup_{\substack{\vv\in\vsV,\vw\in\vsW\\ \nrm{\vv+\vw}=1}}
          \nrm{\oiL\vw}
        = \sup_{\substack{\vv\in\vsV\\ \nrm{\vv}=1}}
          \nrm{\oiL\vv} +
          \sup_{\substack{\vw\in\vsW\\ \nrm{\vw}=1}}
          \nrm{\oiL\vw}\\
        &= \nrm{\restr{\oiL}{\vsV}} + \nrm{\restr{\oiL}{\vsW}}
        = \nrm{\oiL\oiP_\vsV} + \nrm{\oiL\oiP_\vsW}.
      \end{aligned}
      $
    \item Let $\vw\in\vsH$ such that $\nrm{\vw}=1$ and $\vsW=\Span{\vw}$.
      Then
      \begin{align*}
        1
        &= \nrm{\vw}^2
        = \nrm{(\oiP_\vsV + \oiP_{\vsV^\perp}) \vw}^2
        = \nrm{\oiP_\vsV \vw}^2 + \nrm{\oiP_{\vsV^\perp}\vw}^2
        = \nrm{\oiP_\vsV \oiP_\vsW}^2 +
          \nrm{\oiP_{\vsV^\perp}\oiP_\vsW}^2 \\
        &= \cos^2 \theta_{\min}(\vsV,\vsW)
          + \cos^2 \theta_{\min}(\vsV^\perp,\vsW).
      \end{align*}
  \end{enumerate}
\end{proof}

The following definition introduces the widely used concept of the gap between
two closed subspaces of a Hilbert space, see
also~\cite{Kat95,Gal04,Juj05,KnyJA10}.

\begin{definition}
  \label{def:gap}
  For two closed subspaces $\vsV,\vsW\subseteq\vsH$ the \emph{gap} (also
  \emph{maximal gap}, \emph{aperture} or \emph{opening}) $\Theta(\vsV,\vsW)$
  between $\vsV$ and $\vsW$ is defined by
  \[
    \Theta(\vsV,\vsW) = \nrm{\oiP_\vsV - \oiP_\vsW}.
  \]
\end{definition}

In the next lemma, basic properties of the gap between two subspaces are
gathered.

\begin{lemma}
  \label{lem:gap}
  Let $\vsV,\vsW,\vsZ\subseteq\vsH$ be closed subspaces. Then:
  \begin{enumerate}
    \item $\Theta(\vsV,\vsW)=\Theta(\vsW,\vsV)$. \label{lem:gap:symm}
    \item $\Theta(\vsV,\vsW)\in[0,1]$. \label{lem:gap:interval}
    \item $\Theta(\vsV,\vsW) \leq \Theta(\vsV,\vsZ) + \Theta(\vsZ,\vsW)$.
      \label{lem:gap:triangle}
    \item $\Theta(\vsV,\vsW)
      = \max \{
        \nrm{\oiP_{\vsV^\perp} \oiP_\vsW},
        \nrm{\oiP_{\vsW^\perp} \oiP_\vsV} \}$.
      \label{lem:gap:max}
    \item $\Theta(\vsV,\vsW) = \Theta(\vsV^\perp,\vsW^\perp)$.
      \label{lem:gap:complement}
  \end{enumerate}
\end{lemma}

\begin{proof}
  \begin{enumerate}
    \item Follows directly from the definition.
    \item $\Theta(\vsV,\vsW)\geq 0$ also follows from the definition and
      for $\vz\in\vsH$
      \begin{align}
        \nrm{ (\oiP_\vsV - \oiP_\vsW)\vz}^2
        &= \nrm{ \oiP_\vsV (\id -\oiP_\vsW) \vz - (\id -
          \oiP_\vsV)\oiP_\vsW\vz}^2 \notag \\
        &= \nrm{ \oiP_\vsV \oiP_{\vsW^\perp} \vz}^2 +
          \nrm{\oiP_{\vsV^\perp} \oiP_\vsW\vz}^2
          \label{eq:lem:gap:orthdecomp} \\
        &\leq \nrm{ \oiP_\vsV}^2 \nrm{\oiP_{\vsW^\perp} \vz}^2 +
          \nrm{\oiP_{\vsV^\perp}}^2 \nrm{\oiP_\vsW\vz}^2 \notag \\
        &\leq \nrm{\oiP_{\vsW^\perp} \vz}^2 +
          \nrm{\oiP_\vsW\vz}^2 \notag
        = \nrm{\vz}^2 \notag
      \end{align}
      and thus
      $
        \Theta(\vsV,\vsW)
        = \sup_{\nrm{\vz}=1} \nrm{ (\oiP_\vsV - \oiP_\vsW)\vz}
        \leq 1.
      $
    \item The norm triangle inequality yields:
      \begin{align*}
        \Theta(\vsV,\vsW)
        &= \nrm{\oiP_\vsV - \oiP_\vsW}
        = \nrm{\oiP_\vsV - \oiP_\vsZ + \oiP_\vsZ - \oiP_\vsW}
        \leq \nrm{\oiP_\vsV - \oiP_\vsZ} + \nrm{\oiP_\vsZ - \oiP_\vsW} \\
        &= \Theta(\vsV,\vsZ) + \Theta(\vsZ,\vsW).
      \end{align*}
    \item The proof can be found in~\cite[theorem~1.2]{Fel09} and is given
      here for the sake of completeness. For $\vz\in\vsH$ it can be concluded
      from equation \eqref{eq:lem:gap:orthdecomp} that
      \[
        \max \{
          \nrm{ \oiP_\vsV \oiP_{\vsW^\perp} \vz},
          \nrm{\oiP_{\vsV^\perp} \oiP_\vsW\vz} \}
        \leq \nrm{ (\oiP_\vsV - \oiP_\vsW)\vz}
      \]
      and
      \begin{align*}
        \nrm{ (\oiP_\vsV - \oiP_\vsW)\vz }^2
        &\leq \nrm{\oiP_\vsV \oiP_{\vsW^\perp}}^2
          \nrm{\oiP_{\vsW^\perp}\vz}^2
          + \nrm{\oiP_{\vsV^\perp} \oiP_\vsW}^2
          \nrm{\oiP_\vsW\vz}^2 \\
        &\leq \max \{ \nrm{\oiP_\vsV \oiP_{\vsW^\perp}}^2,
          \nrm{\oiP_{\vsV^\perp} \oiP_\vsW}^2 \}  (
          \nrm{\oiP_{\vsW^\perp}\vz}^2 +
          \nrm{\oiP_\vsW\vz}^2 ) \\
        &= \max \{ \nrm{\oiP_\vsV \oiP_{\vsW^\perp}}^2,
          \nrm{\oiP_{\vsV^\perp} \oiP_\vsW}^2 \}
          \nrm{\vz}^2.
      \end{align*}
      However, $\nrm{\oiP_\vsV \oiP_{\vsW^\perp}} =
      \nrm{\oiP_{\vsW^\perp} \oiP_\vsV}$ holds according to
      statement~\ref{lem:orthproj:prod} of lemma~\ref{lem:orthproj} and
      the above inequalities yield combined:
      \[
        \max \{
          \nrm{ \oiP_{\vsW^\perp} \oiP_\vsV \vz},
          \nrm{\oiP_{\vsV^\perp} \oiP_\vsW\vz} \}
        \leq
          \max \{ \nrm{\oiP_{\vsW^\perp} \oiP_\vsV},
          \nrm{\oiP_{\vsV^\perp} \oiP_\vsW} \}
          \nrm{\vz}.
      \]
      Employing the supremum over all $\vz\in\vsH$ completes the proof.
    \item $
      \begin{aligned}[t]
        \Theta(\vsV,\vsW)
        = \nrm{\oiP_\vsV - \oiP_\vsW}
        = \nrm{\oiP_{\vsV^\perp} - \oiP_{\vsW^\perp}}
        = \Theta(\vsV^\perp,\vsW^\perp).
      \end{aligned}
      $
  \end{enumerate}
\end{proof}

Statement~\ref{lem:gap:interval} of lemma~\ref{lem:gap} allows the gap between
two subspaces to be interpreted as the maximal canonical angle between two
subspaces:

\begin{definition}
  \label{def:maxangle}
  For two closed subspaces $\vsV,\vsW\subseteq\vsH$ the \emph{maximal
  canonical angle} $\theta_{\max}(\vsV,\vsW)\in[0,\frac{\pi}{2}]$ between
  $\vsV$ and $\vsW$ is defined by
  \[
    \sin \theta_{\max}(\vsV,\vsW)
    = \Theta(\vsV,\vsW) = \nrm{\oiP_\vsV - \oiP_\vsW}.
  \]
\end{definition}

The following lemma shows that the notation is not misleading because the
minimal canonical angle is actually less or equal to the maximal canonical
angle.

\begin{lemma}
  \label{lem:minmaxangle}
  Let $\vsV,\vsW\subseteq\vsH$ be closed subspaces. Then
  \[
    \theta_{\min}(\vsV,\vsW) \leq \theta_{\max}(\vsV,\vsW).
  \]
\end{lemma}

\begin{proof}
  Because the angles satisfy
  $\theta_{\min}(\vsV,\vsW),\theta_{\max}(\vsV,\vsW)\in[0,\frac{\pi}{2}]$, the
  proof is complete if $\sin \theta_{\min}(\vsV,\vsW) \leq
  \sin\theta_{\max}(\vsV,\vsW)$. From statement~\ref{lem:orthproj:prod} of
  lemma~\ref{lem:orthproj} it is known that
  \begin{align*}
    \sin^2 \theta_{\min}(\vsV,\vsW)
    &= 1 - \nrm{\oiP_\vsV \oiP_\vsW}^2
    = 1 - \nrm{\oiP_\vsW \oiP_\vsV}^2.
  \end{align*}
  For $\vz\in\vsW$ with $\nrm{\vz}=1$ the following equality holds:
  \[
    1
    = \nrm{\vz}^2
    = \nrm{\oiP_\vsW \vz}^2
    = \nrm{ (\oiP_\vsV + \oiP_{\vsV^\perp}) \oiP_\vsW \vz}^2
    = \nrm{ \oiP_\vsV \oiP_\vsW \vz}^2
    + \nrm{ \oiP_{\vsV^\perp} \oiP_\vsW \vz}^2
  \]
  and thus
  \begin{align*}
    \sin^2 \theta_{\min}(\vsV,\vsW)
    &= 1 - \nrm{\oiP_\vsV \oiP_\vsW}^2
    = 1 - \sup_{\substack{\vz\in\vsW\\ \nrm{\vz}=1}} \nrm{\oiP_\vsV
      \oiP_\vsW \vz}^2
    = \inf_{\substack{\vz\in\vsW\\ \nrm{\vz}=1}}
      \nrm{\oiP_{\vsV^\perp}\oiP_\vsW\vz}^2 \\
    &\leq \sup_{\substack{\vz\in\vsW\\ \nrm{\vz}=1}}
      \nrm{\oiP_{\vsV^\perp}\oiP_\vsW\vz}^2
    = \nrm{\oiP_{\vsV^\perp}\oiP_\vsW}^2.
  \end{align*}
  Analogously, $\sin \theta_{\min}(\vsV,\vsW) \leq
  \nrm{\oiP_{\vsW^\perp}\oiP_\vsV}$ and the following statement is obtained:
  \[
    \sin \theta_{\min}(\vsV,\vsW)
    \leq
      \max\{\nrm{\oiP_{\vsV^\perp}\oiP_\vsW},\nrm{\oiP_{\vsW^\perp}\oiP_\vsV}\}
    = \Theta(\vsV,\vsW)
    = \sin \theta_{\max}(\vsV,\vsW).
  \]
\end{proof}

So far angles between arbitrary closed subspaces have been discussed.
In the case of complementary subspaces, i.e., $\vsV\oplus\vsW=\vsH$, certain
angles can be expressed by norms of projections and relations between the
minimal and maximal canonical angles can be established.

\begin{lemma}
  \label{lem:proj}
  For two closed and nonzero subspaces $\vsV,\vsW\subseteq\vsH$ with
  $\vsV\oplus\vsW=\vsH$ the following holds:
  \begin{enumerate}
    \item $\nrm{\oiP_{\vsV,\vsW}}
      = \frac{1}{\sqrt{1 - \nrm{\oiP_\vsV \oiP_\vsW}^2}}$.
      \label{lem:proj:orthoblique}
    \item $\nrm{\oiP_\vsV \oiP_\vsW}
      = \nrm{\oiP_{\vsV^\perp} \oiP_{\vsW^\perp}}
      = \Theta(\vsV,\vsW^\perp)$.
      \label{lem:proj:orthosymm}
    \item $\theta_{\min}(\vsV,\vsW) = \theta_{\min}(\vsV^\perp,\vsW^\perp)$.
    \item $\theta_{\min}(\vsV,\vsW) + \theta_{\max}(\vsV,\vsW^\perp) =
      \frac{\pi}2$.
      \label{lem:proj:anglesum}
    \item $\cos \theta_{\min}(\vsV,\vsW)
      = \sin \theta_{\max}(\vsV,\vsW^\perp)$
      and
      $\sin \theta_{\min}(\vsV,\vsW)
      = \cos \theta_{\max}(\vsV,\vsW^\perp)$.
      \label{lem:proj:mintomax}
    \item $
      \begin{aligned}[t]
        \nrm{\oiP_{\vsV,\vsW}}
        = \frac{1}{\cos \theta_{\max}(\vsV,\vsW^\perp)}.
      \end{aligned}
      $
      \label{lem:proj:norm}
    \item If $\oiL\in\vsL(\vsH)$ then
      \[
        \cos \theta_{\max}(\vsV,\vsW^\perp)
          \nrm{\oiL\oiP_{\vsV,\vsW}}
        \leq \nrm{\oiL\oiP_\vsV}
        \leq \cos \theta_{\min}(\vsV,\vsW^\perp)
          \nrm{\oiL\oiP_{\vsV,\vsW}}.
      \]
  \end{enumerate}
\end{lemma}

\begin{proof}
  \begin{enumerate}
    \item Similar to the proof of lemma~\ref{lem:minmaxangle}, the fact is used
      that for $\vz\in\vsW$ with $\nrm{\vz}=1$ the equality
      $1 = \nrm{ \oiP_\vsV \oiP_\vsW \vz}^2
      + \nrm{ \oiP_{\vsV^\perp} \oiP_\vsW \vz}^2$
      holds. With statement~\ref{lem:proj:inverse} of lemma~\ref{lem:proj:inv}
      (note that also $\vsV^\perp\oplus\vsW^\perp=\vsH$, cf.\
      statement~\ref{lem:projadjoint:orth} of lemma~\ref{lem:projadjoint}), the
      following equation is obtained:
      \begin{align*}
        1 - \nrm{\oiP_\vsV \oiP_\vsW}^2
        &= 1 - \sup_{\substack{\vz\in\vsW \\ \nrm{\vz}=1}}
          \nrm{\oiP_\vsV \oiP_\vsW \vz}^2
        = \inf_{\substack{\vz\in\vsW \\ \nrm{\vz}=1}}
          \nrm{\oiP_{\vsV^\perp} \oiP_\vsW \vz}^2
        = \frac{1}{\sup_{\substack{\vz\in\vsW\\ \vz\neq 0}}
          \frac{\nrm{\vz}^2}{\nrm{\oiP_{\vsV^\perp} \oiP_\vsW \vz}^2}} \\
        &= \frac{1}{\sup_{\substack{\vz\in\vsW\\ \vz\neq 0}}
          \frac{\nrm{\vz}^2}{\nrm{\oiQ_{\vsW,\vsV} \vz}^2}}
        = \frac{1}{\sup_{\substack{\vz\in\vsV^\perp\\ \vz\neq 0}}
          \frac{\nrm{\oiQ_{\vsW,\vsV}^\inv \vz}^2}{\nrm{\vz}^2}}
        = \frac{1}{\nrm{\oiQ_{\vsW,\vsV}^\inv}^2}
        = \frac{1}{\nrm{\oiP_{\vsW,\vsV}}^2}.
      \end{align*}
      The proof is complete by recognizing that
      $\nrm{\oiP_{\vsW,\vsV}}=\nrm{\oiP_{\vsV,\vsW}}$, cf.\
      lemma~\ref{lem:projadjoint}.
    \item By exchanging $\vsV$ with $\vsV^\perp$ and $\vsW$ with
      $\vsW^\perp$ in statement~\ref{lem:proj:orthoblique} the equation
      \[
        \nrm{\oiP_{\vsV^\perp} \oiP_{\vsW^\perp}}^2 =
        1 - \frac{1}{\nrm{\oiP_{\vsV^\perp,\vsW^\perp}}^2}
      \]
      follows.  However, $\oiP_{\vsV^\perp,\vsW^\perp}$ is the complementary
      adjoint projection of $\oiP_{\vsV,\vsW}$ with
      $\nrm{\oiP_{\vsV,\vsW}}=\nrm{\oiP_{\vsV^\perp,\vsW^\perp}}$ (cf.\
      lemma~\ref{lem:projadjoint}) and thus $\nrm{\oiP_\vsV
      \oiP_\vsW} = \nrm{\oiP_{\vsV^\perp} \oiP_{\vsW^\perp}}$.
      The last equality follows from lemma~\ref{lem:gap}.
    \item Follows directly from the previous statement and
      statement~\ref{lem:orthproj:prod} of lemma~\ref{lem:orthproj}.
    \item This statement has been shown by Ljance~\cite{Lja58}. Here,
      a different proof is given.  Due to the fact that
      $\theta_{\min}(\vsV,\vsW),\theta_{\max}(\vsV,\vsW)\in[0,\frac{\pi}{2}]$,
      it suffices to show that
      \[
        \sin\theta_{\max}(\vsV,\vsW^\perp)
        = \sin\left(\frac{\pi}2 - \theta_{\min}(\vsV,\vsW)\right)
        = \cos \theta_{\min}(\vsV,\vsW).
      \]
      This can now easily be shown with
      statement~\ref{lem:gap:max} of lemma~\ref{lem:gap},
      statement~\ref{lem:proj:orthosymm} of this lemma and
      statement~\ref{lem:orthproj:prod} of lemma~\ref{lem:orthproj}:
      \[
        \sin\theta_{\max}(\vsV,\vsW^\perp)
        = \max \{ \nrm{\oiP_{\vsV^\perp} \oiP_{\vsW^\perp}},
          \nrm{\oiP_\vsV \oiP_\vsW} \}
        = \nrm{\oiP_\vsV \oiP_\vsW}
        = \cos \theta_{\min}(\vsV,\vsW).
      \]
    \item Follows from the previous result.
    \item From statements~\ref{lem:proj:orthoblique} and
      \ref{lem:proj:orthosymm} of this lemma the equation can be shown by
      \begin{align*}
        \nrm{\oiP_{\vsV,\vsW}}
        = \frac{1}{\sqrt{1-\nrm{\oiP_\vsV \oiP_\vsW}^2}}
        = \frac{1}{\sqrt{1 - \sin^2 \theta_{\max}(\vsV,\vsW^\perp)}}
        = \frac{1}{\cos \theta_{\max}(\vsV,\vsW^\perp)}.
      \end{align*}
    \item The first inequality follows from
      statement~\ref{lem:proj:norm} and
      \begin{align*}
        \nrm{\oiL \oiP_{\vsV,\vsW}}
        = \nrm{\oiL \oiP_\vsV \oiP_{\vsV,\vsW}}
        \leq \nrm{\oiL \oiP_\vsV} \nrm{\oiP_{\vsV,\vsW}}
        = \nrm{\oiL \oiP_\vsV}
          \frac{1}{\cos \theta_{\max}(\vsV,\vsW^\perp)}.
      \end{align*}
      The second inequality can be verified with
      statements~\ref{lem:proj:orth} and \ref{lem:proj:orth2}:
      \begin{align*}
        \nrm{\oiL \oiP_\vsV}
        &= \nrm{\oiL \oiP_{\vsV,\vsW} \oiP_\vsV}
        = \nrm{\oiL \oiP_{\vsV,\vsW} \oiP_{\vsW^\perp} \oiP_\vsV} \\
        &\leq \nrm{\oiL \oiP_{\vsV,\vsW}} \nrm{\oiP_{\vsW^\perp}
          \oiP_\vsV}
        = \cos \theta_{\min} (\vsV,\vsW^\perp) \nrm{\oiL
          \oiP_{\vsV,\vsW}}.
      \end{align*}
  \end{enumerate}
\end{proof}

In addition to the well-known results in this and the preceding section, an
apparently new result on the norm of the difference of two arbitrary projections
is presented in section~\ref{sec:rec:per:proj}.


\section{Galerkin and Petrov--Galerkin methods}
\label{sec:back:galerkin}

The characterization and analysis of Krylov subspace methods in this thesis rely
to a great extent on the projection framework for solving a linear system which
was introduced by Saad~\cite{Saa81}. This projection framework is in fact a
Petrov--Galerkin method which is defined in this section. The notation is
mostly adopted from the book of Liesen and Strako\v{s}~\cite{LieS13}. As before,
instead of the Euclidean inner product, a Hilbert space with arbitrary inner
product is used.  This setting has also been discussed in the extensive article
by Eiermann and Ernst~\cite{EieE01}. However, in contrast to
\cite{LieS13,EieE01}, most statements made in this and the following sections do
not require that the linear operator is nonsingular. Therefore, proofs are
given for theorems that are well-known for the nonsingular case in the
literature. The issue of singular operators reappears in the context of
deflated Krylov subspace methods in chapter~\ref{ch:rec}.

For this and the following sections, let a linear system
\begin{equation}
  \oiA\vx=\vb
  \label{eq:back:ls}
\end{equation}
be given with $\oiA\in\vsL(\vsH)$ and $\vb\in\vsH$. It is assumed that the
linear system~\eqref{eq:back:ls} is consistent, i.e., $\vb\in\vsR(\oiA)$. Then a
solution $\vx$ of \eqref{eq:back:ls} can be approximated by
\begin{equation}
  \vy=\vx_0 + \vs\quad\text{with}\quad\vs\in\vsS,
  \label{eq:back:search}
\end{equation}
where $\vx_0\in\vsH$ is a given initial approximation and $\vsS\subseteq\vsH$
is an $n$-dimensional subspace, also referred to as the \emph{search space}. Now
one has $n$ degrees of freedom in the choice of $\vs$ in
\eqref{eq:back:search} and thus a constraint is imposed on the residual
\begin{equation}
  \vr\DEF\vb-\oiA\vy \perp \vsC,
  \label{eq:back:constr}
\end{equation}
where $\vsC\subseteq\vsH$ also is an $n$-dimensional subspace, the
\emph{constraint space}.

\begin{definition}[\PetrovAndOrGalerkin{} method]
  Let $\oiA\in\vsL(\vsH)$, $\vx_0,\vb\in\vsH$. For two $n$-dimensional subspaces
  $\vsS,\vsC\subseteq\vsH$ the method described by \eqref{eq:back:search} and
  \eqref{eq:back:constr} is called a \emph{Galerkin method} if $\vsS=\vsC$ and a
  \emph{Petrov--Galerkin method} otherwise. A \PetrovAndOrGalerkin{} method is
  called \emph{well defined} if there exists a unique approximate solution $\vy$
  satisfying \eqref{eq:back:search} and \eqref{eq:back:constr}.
\end{definition}

The question of well-definedness of \PetrovAndOrGalerkin{} methods is answered in
the following theorem:

\begin{thm}
  \label{thm:back:pg-well}
  Let $\oiA\in\vsL(\vsH)$, $\vx_0,\vb\in\vsH$ and two $n$-dimensional subspaces
  $\vsS,\vsC\subseteq\vsH$ be given. Then the following statements are
  equivalent:
  \begin{enumerate}
    \item The \PetrovAndOrGalerkin{} method with search space $\vsS$ and constraint
      space $\vsC$ is well defined.
    \item $\ip{\vtC}{\oiA\vtS}$ is non-singular for any $\vtS,\vtC\in\vsH^n$
      with $\Span{\vtS}=\vsS$ and $\Span{\vtC}=\vsC$.
    \item $\oiA\vsS\oplus\vsC^\perp=\vsH$.
  \end{enumerate}
\end{thm}

\begin{proof}
  1.\LLRA 2.: Let $\vs=\vtS\vt\in\vsS$ fulfill \eqref{eq:back:search} and
  \eqref{eq:back:constr} and let $\vtS,\vtC\in\vsH^n$ with
  $\Span{\vtS}=\vsS$ and $\Span{\vtC}=\vsC$. Condition
  \eqref{eq:back:constr} is equivalent to
  \[
    \ip{\vtC}{\oiA\vtS}\vt = \ip{\vtC}{\vb - \oiA\vx_0}.
  \]
  Thus the uniqueness of $\vs$ and $\vy$ is equivalent to the nonsingularity of
  $\ip{\vtC}{\oiA\vtS}$.

  2.\LLRA 3.: Has been proven in theorem~\ref{thm:back:proj_sing}.
\end{proof}

\begin{cor}
  \label{cor:back:pg-repr}
  Let $\oiA\in\vsL(\vsH)$, $\vx_0,\vb\in\vsH$ and two $n$-dimensional subspaces
  $\vsS,\vsC\subseteq\vsH$ be given such that the Petrov--Galerkin method with
  search space $\vsS$ and constraint space $\vsC$ is well defined.

  Then the approximate solution $\vy$ and the corresponding residual $\vr$ that
  satisfy \eqref{eq:back:search} and \eqref{eq:back:constr} are given by
  \begin{align}
    \vy&=\vx_0 + \vtS\ip{\vtC}{\oiA\vtS}^\inv\ip{\vtC}{\vr_0}
    \label{eq:back:pg-sol}
    \\
    \text{and}\quad \vr&=\vb-\oiA\vy=\oiP_{\vsC^\perp,\oiA\vsS}\vr_0,
    \label{eq:back:pg-res}
  \end{align}
  where $\vr_0=\vb-\oiA\vx_0$ is the initial residual. Furthermore, the linear
  system \eqref{eq:back:ls} is solved by $\vy$ if and only if
  $\vr_0\in\oiA\vsS$.
\end{cor}

\begin{proof}
  Let $\vtS,\vtC\in\vsH^n$ with $\Span{\vtS}=\vsS$ and $\Span{\vtC}=\vsC$.
  From the proof of theorem~\ref{thm:back:pg-well} it follows that $\vy=\vx_0
  + \vtS\ip{\vtC}{\oiA\vtS}^\inv\ip{\vtC}{\vr_0}$ and thus
  \[
    \vr
    =\vb-\oiA\vy
    =\vr_0 - \oiA\vtS\ip{\vtC}{\oiA\vtS}^\inv\ip{\vtC}{\vr_0}
    =(\id - \oiP_{\oiA\vsS,\vsC^\perp})\vr_0
    =\oiP_{\vsC^\perp,\oiA\vsS}\vr_0.
  \]
\end{proof}

From corollary~\ref{cor:back:pg-repr}, the link between Petrov--Galerkin methods
and projections becomes apparent. Equation \eqref{eq:back:pg-res} shows that the
residual is $\vr=\oiP_{\vsC^\perp,\oiA\vsS}\vr_0$ and because of the definition
of orthogonal and oblique projections (cf.\ definition~\ref{def:orthproj}) a
projection method is called \emph{orthogonal} if $\oiA\vsS=\vsC$ and
\emph{oblique} otherwise.

\begin{rmk}
  By using the above definition for orthogonal and oblique projection methods,
  the author follows Liesen and Strako\v{s}~\cite{LieS13} in breaking with the
  tradition of using the term \emph{orthogonal} for the case $\vsS=\vsC$ and the
  term \emph{oblique} otherwise.
\end{rmk}

Given a search space $\vsS$, there are two popular choices for the constraint
space $\vsC$: $\vsC=\vsS$ and $\vsC=\oiA\vsS$. The first choice is used in the
CG method if $\oiA$ is self-adjoint and positive semidefinite, see
section~\ref{sec:back:cg}, and results in a minimal $\oiA$-norm of the error.
The second choice is used in the GMRES and MINRES methods, see
section~\ref{sec:back:mr}, and results in a minimal residual norm.  Conditions
for the well-definedness of a Petrov--Galerkin method with these spaces are
given in the following lemma.

\begin{lemma}[Well-definedness and optimality]
  \label{lem:back:well-opt}
  Consider a linear system $\oiA\vx=\vb$ with $\oiA\in\vsL(\vsH)$, $\vx\in\vsH$
  and $\vb\in\vsR(\oiA)$. Furthermore, let $\vx_0\in\vsH$ be an initial guess
  and let $\vsS\subseteq\vsH$ be an $n$-dimensional subspace. The Petrov--Galerkin
  method with search space $\vsS$ and constraint space $\vsC$ is well defined
  and defines a unique approximate solution $\vy\in\vx_0+\vsS$ if one of the following
  conditions holds:
  \begin{enumerate}
    \item \label{lem:back:well-opt:hpd}
      $\vsC=\vsS$, $\vsS\cap\vsN(\oiA)\neq\{0\}$ and $\oiA$ is self-adjoint
      and positive semidefinite. Then
      \begin{equation*}
        \nrm[\oiA]{\vx - \vy} = \inf_{\vz\in\vx_0+\vsS}\nrm[\oiA]{\vx - \vz},
      \end{equation*}
      where $\nrmdots[\oiA]$ is the norm (or semi-norm if $\oiA$ is singular)
      defined by $\nrm[\oiA]{\vz}=\sqrt{\ip{\vz}{\oiA\vz}}$.
    \item \label{lem:back:well-opt:general}
      $\vsC=\oiA\vsS$ and $\vsS\cap\vsN(\oiA)=\{0\}$. Then
      \begin{equation*}
        \nrm{\vb-\oiA\vy} = \inf_{\vz\in\vx_0+\vsS}\nrm{\vb-\oiA\vz}.
      \end{equation*}
  \end{enumerate}
\end{lemma}

\begin{proof}
  \begin{enumerate}
    \item Let $\vtS\in\vsH^n$ with $\Span{\vtS}=\vsS$ and
      assume that $\ip{\vtS}{\oiA\vtS}$ is singular, i.e., there exists a
      nonzero $\vt\in\C^n$ with $\ip{\vtS}{\oiA\vtS\vt}=0$.
      Because $\oiA$ is self-adjoint and positive semidefinite, it has a unique
      self-adjoint and positive-semidefinite square root $\oiA^\frac{1}{2}$,
      cf. \cite[Korollar VII.1.16]{Wer07}. Then with $\vs\DEF\vtS\vt\neq 0$ the
      following holds
      \[
        0
        = \ip{\vs}{\oiA\vs}
        =\ip{\oiA^\frac{1}{2}\vs}{\oiA^\frac{1}{2}\vs}
        =\nrm{\oiA^\frac{1}{2}\vs}^2
      \]
      and hence $\oiA^\frac{1}{2}\vs=0$. Applying $\oiA^\frac{1}{2}$
      yields $\oiA\vs=0$ which is a contradiction to $\vsS\cap\vsN(\oiA)=\{0\}$.
      The well-definedness follows from the equivalence of items 1.\ and 2.\ in
      theorem~\ref{thm:back:pg-well}.

      It remains to show the optimality. Therefore, let
      $\vsR\DEF\vsN(\oiA)^\perp$ and let the inner product
      $\ipdots[\oiA|_\vsR]:\vsR\times\vsR\lra\C$
      be defined by $\ip[\oiA|_\vsR]{\vx}{\vy}=\ip{\vx}{\oiA|_\vsR\vy}$. Note
      that the restriction $\oiA|_\vsR$ is self-adjoint and positive definite.
      Then the following holds with the representation of $\vy$
      in~\eqref{eq:back:pg-sol}:
      \begin{align*}
        \nrm[\oiA]{\vx-\vy}
        &= \nrm[\oiA|_\vsR]{\oiP_\vsR(\vx-\vy)}\\
        &= \nrm[\oiA|_\vsR]{\oiP_\vsR(\vx-\vx_0)
          - \oiP_\vsR \vtS\ip[\oiA|_\vsR]{\oiP_\vsR\vtS}{\oiP_\vsR\vtS}^\inv
          \ip[\oiA|_\vsR]{\oiP_\vsR\vtS}{\oiP_{\vsR}(\vx-\vx_0)} }\\
        &= \nrm[\oiA|_\vsR]{\oiP_{(\oiP_\vsR\vsS)^{\perp_{\vsA|_\vsR}}}
          \oiP_\vsR(\vx-\vx_0)}.
      \end{align*}
      Because $\oiP_{(\oiP_\vsR\vsS)^{\perp_{\vsA|_\vsR}}}$ is the
      orthogonal projection onto $\oiP_\vsR\vsS$ with respect to the inner
      product $\ipdots[\oiA|_\vsR]$, theorem~\ref{thm:back:minimizer}
      shows that
      \begin{align*}
        \nrm[\oiA]{\vx-\vy}
        &= \inf_{\vz\in\oiP_\vsR\vsS}\nrm[\oiA|_\vsR]{\oiP_\vsR(\vx-\vx_0) -
          \vz}
        = \inf_{\vz\in\vsS} \nrm[\oiA|_\vsR]{\oiP_\vsR(\vx-(\vx_0+\vz))} \\
        &= \inf_{\vz\in\vsS} \nrm[\oiA]{\vx - (\vx_0+\vz)}.
      \end{align*}
    \item The well-definedness follows from the equivalence of items 1. and 3.
      in theorem~\ref{thm:back:pg-well}. The optimality follows again from
      theorem~\ref{thm:back:minimizer}.
  \end{enumerate}
\end{proof}

\section{Ritz pairs and harmonic Ritz pairs}
\label{sec:back:ritz}

Analogous to the Petrov--Galerkin approximation for linear systems in the last
section, a projection framework can be used to determine approximations to
eigenvalues and eigenvectors. There is a close relationship between these
so-called Ritz values and vectors and the convergence of Krylov subspace methods
for linear systems. This relationship becomes apparent in subsequent
sections.  The presentation in this section is based on the contribution of Saad
in chapter 3.2 in~\cite{BaiDDRV00}.

If an operator $\oiA\in\vsL(\vsH)$ and two $n$-dimensional subspaces
$\vsS,\vsC\subseteq\vsH$ are given, then a basis of approximate eigenvectors
\begin{equation}
  \vw_1,\ldots,\vw_n\in\vsS
  \label{eq:back:ritz-vec}
\end{equation}
and approximate eigenvalues
\begin{equation*}
  \mu_1,\ldots,\mu_n\in\C
\end{equation*}
can be determined by requiring
\begin{equation}
  \oiA\vw_i - \mu_i\vw_i \perp \vsC
  \quad\text{for all}~i\in\{1,\dots,n\}.
  \label{eq:back:ritz-con}
\end{equation}
Note the analogy of the conditions
\eqref{eq:back:ritz-vec}--\eqref{eq:back:ritz-con} to the Petrov--Galerkin
approach for linear systems, cf.\
conditions~\eqref{eq:back:search}--\eqref{eq:back:constr}. If two bases of the
subspaces are given, i.e., $\vtS,\vtC\in\vsH^n$ with $\Span{\vtS}=\vsS$ and
$\Span{\vtC}=\vsC$, then the approximate eigenvectors can be represented by
$\vtW=[\vw_1,\ldots,\vw_n]=\vtS\ofU$ for a
$\ofU=[\vu_1,\ldots,\vu_n]\in\C^{n,n}$ and the conditions
\eqref{eq:back:ritz-vec}--\eqref{eq:back:ritz-con} are equivalent to finding an
invertible $\ofU\in\C^{n,n}$ and $\ofD_\mu=\diag(\mu_1,\ldots,\mu_n)\in\C^{n,n}$ that satisfy
\begin{equation}
  \ip{\vtC}{\oiA\vtS}\ofU = \ip{\vtC}{\vtS} \ofU \ofD_\mu.
  \label{eq:back:ritz-eigen}
\end{equation}
This essentially means that the generalized eigenvalue problem
\[
  \ip{\vtC}{\oiA\vtS}\vu = \mu \ip{\vtC}{\vtS}\vu
\]
has to be solved.

As in the previous section, two cases are of particular importance: $\vsS=\vsC$
and $\oiA\vsS=\vsC$. In the case $\vsS=\vsC$, the above procedure is known as
the \emph{Rayleigh-Ritz} procedure and the eigenvector and eigenvalue
approximations are called \emph{Ritz vectors} and \emph{Ritz values}.

\begin{definition}[Ritz pair]
  \label{def:back:ritz}
  Let $\oiA\in\vsL(\vsH)$ and let $\vsS\subseteq\vsH$ be an $n$-dimensional
  subspace. The vector $\vw\in\vsS\setminus\{0\}$ is called a \emph{Ritz vector}
  and $\mu\in\C$ is called a \emph{Ritz value} of $\oiA$ with respect to $\vsS$
  if
  \[
    \oiA\vw - \mu\vw \perp \vsS.
  \]
  The pair $(\vw,\mu)$ is called a \emph{Ritz pair}.
\end{definition}

\begin{rmk}
  \label{rmk:back:ritz}
  In practice, an orthonormal basis is used for the computation of Ritz pairs.
  If $\vtS\in\vsH^n$ with $\ip{\vtS}{\vtS}=\ofI_n$, then the standard eigenvalue
  problem
  \[
    \ip{\vtS}{\oiA\vtS}\ofU=\ofU\ofD_\mu
  \]
  has to be solved, cf.\ equation~\eqref{eq:back:ritz-eigen}.
\end{rmk}

The following lemma is well-known and characterizes the residual of Ritz
pairs.

\begin{lemma}
  \label{lem:back:ritz-res}
  Let $\oiA$ and $\vsS$ be as in definition~\ref{def:back:ritz} and let
  $(\vw,\mu)$ be a Ritz pair of $\oiA$ with respect to $\vsS$ with
  $\nrm{\vw}=1$.

  Then
  \[
    \nrm{\oiA\vw - \mu\vw}
    = \sqrt{\nrm{\oiA\vw}^2 - |\mu|^2}.
  \]
\end{lemma}

\begin{proof}
  First note that $\ip{\vw}{\oiA\vw}=\mu$. The statement then follows from
  $\oiA\vw-\mu\vw\perp\vsS$ and $\vw\in\vsS$ because
  \[
    \nrm{\oiA\vw-\mu\vw}^2
    = \ip{\oiA\vw}{\oiA\vw-\mu\vw}
    = \nrm{\oiA\vw}^2 - \mu\ip{\oiA\vw}{\vw}
    = \nrm{\oiA\vw}^2 - |\mu|^2.
  \]
\end{proof}

In the case $\oiA\vsS=\vsC$, the generalized eigenvalue
problem~\eqref{eq:back:ritz-eigen} becomes
\begin{equation}
  \ip{\oiA\vtS}{\oiA\vtS}\ofU = \ip{\oiA\vtS}{\vtS}\ofU\ofD_\mu,
  \label{eq:back:ritz-harm}
\end{equation}
which deserves some more attention. If $\dim\oiA\vsS=\dim\vsS=n$, then the
matrix $\ip{\oiA\vtS}{\oiA\vtS}$ is Hermitian and positive definite, but
$\ip{\oiA\vtS}{\vtS}$ may be singular and thus infinite eigenvalues may occur.
Instead of \eqref{eq:back:ritz-harm}, the generalized eigenvalue problem
\begin{equation}
  \ip{\oiA\vtS}{\vtS}\ofU = \ip{\oiA\vtS}{\oiA\vtS}\ofU\ofD_\sigma
  \label{eq:back:ritz-harm-nonsing}
\end{equation}
can be solved, where $\ofD_\sigma=\diag(\sigma_1,\ldots,\sigma_n)$. This
generalized eigenvalue problem is equivalent to the standard eigenvalue problem
\[
  \ip{\oiA\vtS}{\oiA\vtS}^\inv\ip{\oiA\vtS}{\vtS}\ofU = \ofU\ofD_\sigma
\]
and thus all $\sigma_1,\ldots,\sigma_n$ are finite. Trivially,
$\vtS=\oiA|_{\oiA\vsS}^\inv \oiA \vtS$ holds and
equation~\eqref{eq:back:ritz-harm-nonsing} is equivalent to
\[
  \oiA|_{\oiA\vsS}^\inv \oiA \vtS\vu_i - \sigma_i\oiA\vtS\vu_i \perp \oiA\vsS
\]
for all $i\in\{1,\ldots,n\}$. This motivates the following definition:

\begin{definition}[Harmonic Ritz pair]
  \label{def:back:ritz-harm}
  Let $\oiA\in\vsL(\vsH)$ and let $\vsS\subseteq\vsH$ be an $n$-dimensional
  subspace such that $\dim\oiA\vsS=n$. Let $(\vz,\sigma)\in\oiA\vsS\times\C$
  with $\vz=\oiA\vw\neq 0$ be a Ritz pair of $\oiA|_{\oiA\vsS}^\inv$ with
  respect to $\oiA\vsS$, i.e.
  \[
    \oiA|_{\oiA\vsS}^\inv \vz - \sigma \vz \perp \oiA\vsS.
  \]
  Then $\vw$ is called a \emph{harmonic Ritz vector} and 
  \[
    \mu\DEF
    \begin{cases}
      \frac{1}{\sigma} &\text{if}~\sigma\neq 0\\
      \infty &\text{else}
    \end{cases}
  \]
  is called a \emph{harmonic Ritz value} of $\oiA$ with respect to $\vsS$.
  The pair $(\vw,\mu)$ is called a \emph{harmonic Ritz pair}.
\end{definition}

Definition~\ref{def:back:ritz-harm} allows for infinite harmonic Ritz values
which occur if and only if the matrix $\ip{\oiA\vtS}{\vtS}$ is singular. This
situation may arise in practical applications, e.g., in the GMRES method the
occurrence of an infinite harmonic Ritz value is equivalent to exact stagnation
of GMRES, cf.\ section~\ref{sec:back:mr}.

Note that conditions \eqref{eq:back:ritz-vec}--\eqref{eq:back:ritz-con} with
$\oiA\vsS=\vsC$ may be ambiguous in the case of an infinite harmonic Ritz value
but definition~\ref{def:back:ritz-harm} is not.

With respect to the numerical computation of harmonic Ritz values, the
generalized eigenvalue problem~\eqref{eq:back:ritz-harm-nonsing} can be used.
Zero eigenvalues indicate an infinite harmonic Ritz value and can be treated
specially in order to avoid a division by zero.

Harmonic Ritz pairs were introduced in 1991 by Morgan~\cite{Mor91} for symmetric
matrices under the name \emph{modified Rayleigh-Ritz procedure}.  In the
literature, harmonic Ritz values have also been called \emph{roots of kernel
polynomials} by Freund~\cite{Fre92} or \emph{pseudo-Ritz values} by Freund,
Golub and Nachtigal~\cite{FreGN92}.  The term \emph{harmonic Ritz pair}
originates from~\cite{PaiPV95}, where Paige, Parlett and van der Vorst showed
for a symmetric matrix, that the harmonic Ritz values are weighted harmonic
means of the eigenvalues of $\oiA$. In~\cite{Mor91} and subsequent publications,
\emph{harmonic Ritz values} were promoted as better approximations to interior
eigenvalues. However, in~\cite{PaiPV95} it was shown in the context of eigenpair
approximations for symmetric matrices from Krylov subspaces (cf.\
section~\ref{sec:back:krylov}), that a harmonic Ritz value $\widehat{\mu}^{(n)}$
at iteration $n$ always lies between two (regular) Ritz values $\mu^{(n)}$ and
$\mu^{(n+1)}$ at iterations $n$ and $n+1$. Thus, the convergence of regular and
harmonic Ritz values to an eigenvalue of $\oiA$ takes place at about the same
iteration $n$ and the benefits of harmonic Ritz values seem to be marginal in
this respect.  Nevertheless, \emph{harmonic Ritz values} play an important role
in the analysis of minimal residual methods, cf.\ section~\ref{sec:back:mr}.

The harmonic Ritz \emph{vectors} deserve some attention and are a common
source of confusion. With regard to definition~\ref{def:back:ritz-harm},
$(\vz,\mu)=(\oiA\vw,\mu)$ is first considered as the approximate eigenpair
in the original work on harmonic Ritz pairs by Morgan~\cite{Mor91}.
He then noticed that the pair $(\vw,\mu)$ may be a better approximation
to eigenpairs close to the origin because $\vw$ can be seen as the result
of one step of inverse iteration applied to $\vz$. However, the implicit
application of one step of inverse iteration destroys an important property in
the case of a self-adjoint operator $\oiA$: the orthogonality of the approximate
eigenvectors. In order to see this, let $\vw_1,\dots,\vw_n$ be a basis of
harmonic Ritz vectors of $\vsS$. Then the basis
$\vz_1=\oiA\vw_1,\dots,\vz_n=\oiA\vw_n$ forms an orthogonal basis of $\oiA\vsS$
because in the generalized eigenvalue problem~\eqref{eq:back:ritz-harm-nonsing}
the columns of the eigenvector matrix $\ofU$ are
$\ip{\oiA\vtS}{\oiA\vtS}$-orthogonal. However, the harmonic Ritz vector
basis $\vw_1,\dots,\vw_n$ is not orthogonal in general.

The residuals of harmonic Ritz pairs can be characterized with the following
lemma which is due to Morgan~\cite{Mor91} and Stewart~\cite{Ste01}.

\begin{lemma}
  \label{lem:back:ritz-harm-res}
  Let $\oiA$ and $\vsS$ be as in definition~\ref{def:back:ritz-harm}, let
  $(\vw,\mu)$ be a harmonic Ritz pair of $\oiA$ with respect to $\vsS$. Furthermore,
  assume that $\nrm{\vw}=1$, $\mu\neq\infty$ and let $\rho=\ip{\vw}{\oiA\vw}$.

  Then
  \[
    \nrm{\oiA\vw - \mu\vw}
    = \sqrt{|\mu(\mu - \rho)|}
    \leq |\mu|.
  \]
\end{lemma}

\begin{proof}
  The original proofs can be found in~\cite{Mor91,Ste01}. The equality follows
  from the fact that $\oiA\vw-\mu\vw\perp\oiA\vsS$ because $\oiA\vw\in\oiA\vsS$
  and thus
  \[
    \nrm{\oiA\vw-\mu\vw}^2
    = - \ip{\mu\vw}{\oiA\vw-\mu\vw}
    = \conj{\mu} (\mu\ip{\vw}{\vw} - \ip{\vw}{\oiA\vw})
    = \conj{\mu} (\mu - \rho).
  \]
  The inequality then follows with $\conj{\mu}(\mu-\rho)=|\mu|^2 -
  \conj{\mu}\rho\leq|\mu|^2$. Note that $\conj{\mu}\rho\in\R$ and thus $\mu$ and
  $\rho$ have to lie on a line in the complex plane through the origin.
\end{proof}

Note that $\rho$ is the Rayleigh quotient of $\oiA$ with respect to $\vw$ in
lemma~\ref{lem:back:ritz-harm-res} and can also be characterized as the Ritz
value of $\oiA$ with respect to the subspace $\Span{\vw}$. The following theorem
appears to be new and establishes a simple yet useful connection between the
harmonic Ritz residual norm $\nrm{\oiA\vw-\mu\vw}$ and the Ritz residual norm
$\nrm{\oiA\vw-\rho\vw}$.

\begin{thm}
  \label{thm:back:ritz-harm-res}
  Let the assumptions of lemma~\ref{lem:back:ritz-harm-res} hold.

  Then $\frac{\mu}{\rho}\in\R$ with $\frac{\mu}{\rho}\geq 0$ and
  \[
    \nrm{\oiA\vw - \mu\vw}
    = \sqrt{\frac{\mu}{\rho}} \nrm{\oiA\vw - \rho\vw}.
  \]
\end{thm}

\begin{proof}
  First note that $\rho\neq0$ (otherwise the corresponding $\sigma$ in
  definition~\ref{def:back:ritz-harm} is zero) and that
  $\mu=\frac{\nrm{\oiA\vw}^2}{\conj{\rho}}$. Then with
  $\oiA\vw-\mu\vw\perp\oiA\vsS$ and $\oiA\vw\in\oiA\vsS$ the following equation
  is obtained:
  \begin{align*}
    \nrm{\oiA\vw-\mu\vw}^2
    &= -\ip{\mu\vw}{\oiA\vw-\mu\vw}
    = \conj{\mu} \ip{\vw}{\mu\vw - \oiA\vw}\\
    &= \conj{\mu} (\mu - \rho)
    = \conj{\mu} \left( \frac{\nrm{\oiA\vw}^2}{\conj{\rho}} - \rho\right)
    = \frac{\conj{\mu}}{\conj{\rho}} (\nrm{\oiA\vw}^2 - |\rho|^2) \\
    &= \frac{\mu}{\rho} \nrm{\oiA\vw-\rho\vw}^2.
  \end{align*}
  The last equality holds because $\frac{\conj{\mu}}{\conj{\rho}}$ has to be
  real and non-negative and $\nrm{\oiA\vw}^2 - |\rho|^2=\nrm{\oiA\vw-\rho\vw}^2$
  follows from lemma~\ref{lem:back:ritz-res}.
\end{proof}

\begin{rmk}
  The equations in lemma~\ref{lem:back:ritz-res} and
  lemma~\ref{lem:back:ritz-harm-res} are mathematically elegant and the right
  hand sides can be computed cheaply but the computations suffer from round-off errors. Even if
  the difference $\nrm{\oiA\vw}^2-|\mu|^2$ or $|\mu(\mu-\rho)|$ is small, e.g.,
  at the level of machine precision $\varepsilon$, the square root will only be
  of order $\sqrt{\varepsilon}$. In an implementation, a possible workaround is
  to first compute $\eta_1=\nrm{\oiA\vw}^2-|\mu|^2$ or $\eta_2=|\mu(\mu-\rho)|$
  and to compute the residual norm explicitly if the numerically computed values
  $\eta_1$ or $\eta_2$ satisfy $\eta_1\leq\varepsilon\nrm{\oiA}^2$ or
  $\eta_2\leq\varepsilon\nrm{\oiA}^2$. Note that no applications of $\oiA$ are
  required because if $\vw=\vtS\vu$ is the corresponding Ritz or harmonic Ritz
  vector, then $\nrm{\oiA\vw-\mu\vw}=\nrm{\vtT\vu-\mu\vtS\vu}$, where
  $\vtT=\oiA\vtS$ has already been computed for setting up the (generalized)
  eigenvalue problem. Furthermore, the Ritz or harmonic Ritz pairs with small
  $\eta_1$ or $\eta_2$ are usually the ones of interest and $\vw=\vtS\vu$ has to
  be computed anyway. The norm of $\oiA$ can often be approximated cheaply from
  the available quantities, e.g., by $\nrm[2]{\ip{\vtS}{\oiA\vtS}}$.

  Furthermore, theorem~\ref{thm:back:ritz-harm-res} allows to switch between the
  Ritz and harmonic Ritz residual norms and if one of them can be computed
  accurately, the other one can be obtained with basically the same level of
  accuracy.
\end{rmk}

\section{Krylov subspaces}
\label{sec:back:krylov}

This section introduces and characterizes Krylov subspaces and shows how they
fit naturally into the Petrov--Galerkin framework which was presented in
section~\ref{sec:back:galerkin}. The results of this section are well-known in
the finite-dimensional case and can be found, e.g., in the books of
Saad~\cite{Saa03} and Liesen and Strako{\v{s}}~\cite{LieS13}.

\begin{definition}[Krylov subspace]
  \label{def:back:krylov}
  Let $\oiA\in\vsL(\vsH)$, $\vv\in\vsH$ and $n\in\N$. The $n$-th Krylov
  subspace $\vsK_n(\oiA,\vv)$ is defined by
  \[
    \vsK_n(\oiA,\vv)\DEF\Span{\vv,\oiA\vv,\oiA^2\vv,\ldots,\oiA^{n-1}\vv}
  \]
  for $n\geq 1$ and $\vsK_0(\oiA,\vv)\DEF\{0\}$.
\end{definition}

Some basic properties of Krylov subspaces are apparent from the definition:

\begin{prop}
  Let $\oiA\in\vsL(\vsH)$ and $0\neq\vv\in\vsH$. Then the following hold:
  \begin{enumerate}
    \item Krylov subspaces form a nested sequence of subspaces, i.e.,
      $\vsK_{n-1}(\oiA,\vv)\subseteq\vsK_n(\oiA,\vv)$ for $n\geq 1$.
    \item Krylov subspaces and their elements can be represented by
      polynomials, i.e., $\vsK_n(\oiA,\vv)=\{p(\oiA)\vv~|~p\in\Polys_{n-1}\}$
      for $n\geq 1$.
    \item If $N\DEF\dim\vsH<\infty$, there exists a uniquely defined monic
      polynomial $p\in\Polys_d$ of minimal degree $d\in\{1,\ldots,N\}$ such
      that $p(\oiA)\vv=0$. This polynomial is called the \emph{minimal
      polynomial of $\vv$ with respect to $\oiA$}.
  \end{enumerate}
\end{prop}

\begin{proof}
  Items~1.\ and 2.\ are apparent from definition~\ref{def:back:krylov}. Item~3.\
  can be found in \cite[chapter 2.2]{LieS13}.
\end{proof}

\begin{definition}[grade of a vector]
  Let $\oiA\in\vsL(\vsH)$ and $0\neq\vv\in\vsH$. The \emph{grade of
  $\vv$ with respect to $\oiA$} is defined by
  \[
    d(\oiA,\vv)\DEF
    \begin{cases}
      d & \text{if}~\dim\vsK_{d+1}(\oiA,\vv)=d~\text{for a}~d\in\N_+,\\
      \infty & \text{if}~\dim\vsK_d(\oiA,\vv)=d~\text{for all}~d\in\N_+.
    \end{cases}
  \]
\end{definition}

Note that if $N\DEF\dim\vsH<\infty$, the grade of a nonzero vector $\vv$ with
respect to $\oiA$ equals the degree of the minimal polynomial of $\vv$ with
respect to $\oiA$ and thus $d(\oiA,\vv)\leq N$. The grade $d=d(\oiA,\vv)$
describes the index where the Krylov subspace becomes invariant, i.e.,
$\oiA\vsK_d(\oiA,\vv)\subseteq\vsK_d(\oiA,\vv)$.

In the following, let the \emph{index of an eigenvalue} $\lambda$ denote the
size of the largest Jordan block corresponding to the eigenvalue $\lambda$.

\begin{prop}
  \label{prop:back:invariance}
  Let $\oiA\in\vsL(\vsH)$ and let $0\neq\vv\in\vsH$ be of grade
  $d=d(\oiA,\vv)<\infty$
  with respect to $\oiA$. Then the following hold:
  \begin{enumerate}
    \item $\dim\vsK_n(\oiA,\vv)=n$ for $n\leq d$.
    \item The following statements are equivalent:
      \begin{enumerate}
        \item $\oiA\vsK_d(\oiA,\vv)=\vsK_d(\oiA,\vv)$.
        \item \label{prop:back:invariance:null}
          $\vsK_d(\oiA,\vv)\cap\vsN(\oiA)=\{0\}$.
        \item \label{prop:back:invariance:AK}
          $\vv\in\oiA\vsK_d(\oiA,\vv)$.
      \end{enumerate}
      If $\dim\vsH<\infty$ then also the following statement is equivalent
      to the above:
      \begin{enumerate}[resume]
        \item \label{prop:back:invariance:index}
          $\vv\in\vsR(\oiA^m)$, where $m$ is the index of the zero eigenvalue
          of $\oiA$.
      \end{enumerate}
    \item If $\oiA$ is nonsingular, then
      $\oiA\vsK_d(\oiA,\vv)=\vsK_d(\oiA,\vv)$.
  \end{enumerate}
\end{prop}

\begin{proof}
  \begin{enumerate}
    \item Clear from the definition of $d$.
    \item a)\LRA b): If there exists a nonzero
      $\vz\in\vsK_d(\oiA,\vv)\cap\vsN(\oiA)$, then
      $\dim\oiA\vsK_d(\oiA,\vv)\leq d-1$ which is a contradiction to
      $\dim\oiA\vsK_d(\oiA,\vv)=\dim\vsK_d(\oiA,\vv)=d$.

      b)\LRA c): From $\vsK_d(\oiA,\vv)\cap\vsN(\oiA)=\{0\}$ follows that
      $\dim\oiA\vsK_d(\oiA,\vv)=d$ and because of
      $\oiA\vsK_d(\oiA,\vv)\subseteq\vsK_d(\oiA,\vv)$ that
      $\oiA\vsK_d(\oiA,\vv)=\vsK_d(\oiA,\vv)$. Then $\vv\in\oiA\vsK_d(\oiA,\vv)$
      holds.

      c)\LRA a): If $\vv\in\oiA\vsK_d(\oiA,\vv)$ then
      $\oiA\vsK_d=\Span{\vv}+\Span{\oiA\vv,\ldots,\oiA^d\vv}=\vsK_d(\oiA,\vv)$
      because $\vv,\oiA\vv,\ldots,\oiA^{d-1}\vv$ are by the definition of
      $d=\dim\vsK_d(\oiA,\vv)$ linearly independent.

      a)\LRA d): It follows from a) that
      $\oiA^i\vsK_d(\oiA,\vv)=\vsK_d(\oiA,\vv)$ for any $i\in\N$. Thus also
      $\vv\in\vsK_d(\oiA,\vv)=\oiA^m\vsK_d(\oiA,\vv)\subseteq\vsR(\oiA^m)$.

      d)\LRA b): Let
      \[
        \oiA=\vtS\mat{\ofJ & \\ & \ofN}\vtS^\inv
      \]
      be a Jordan decomposition where $\ofJ$ is nonsingular and $\ofN$ is
      nilpotent, i.e., $\ofN^m=0$.  If $\vv\in\vsR(\oiA^m)$, then
      $\vv=\vtS\mat{\ofJ&\\&0}\vtS^\inv\vs$ for a nonzero $\vs\in\vsH$.
      Assume that b) does not hold, i.e., there exists a nonzero polynomial
      $p\in\Polys_{d-1}$ such that $\oiA p(\oiA)\vv=0$. Then
      \[
        0 = \vtS \mat{\ofJ p(\ofJ)&\\&0} \vtS^\inv\vv
      \]
      which is equivalent to
      \[
        0
        = \vtS \mat{p(\ofJ)&\\&0} \vtS^\inv\vv
        = p(\oiA)\vv
      \]
      because $\ofJ$ is nonsingular. However, this means that there exists a
      nonzero polynomial of degree less than $d$ with $p(\oiA)\vv=0$, which is
      a contradiction to the fact that $d$ is the degree of the minimal
      polynomial of $\vv$ with respect to $\oiA$.
    \item Follows directly from 2.b).
  \end{enumerate}
\end{proof}

The condition~\ref{prop:back:invariance:index} in the above proposition that
involves the index of the zero eigenvalue appeared in a work of Ipsen and
Meyer~\cite{IpsM98}, see also the discussion following
corollary~\ref{cor:back:krylov-well}.

The remaining part of this section shows how Krylov subspaces naturally fit
into the setting of the \PetrovAndOrGalerkin{} method for solving the linear
system~\eqref{eq:back:ls}, cf.\ section~\ref{sec:back:galerkin}. In the
projection framework of section~\ref{sec:back:galerkin}, two subspaces of $\vsH$
of equal dimension $n<\infty$ can be chosen: the search space $\vsS$ and the
constraint space $\vsC$. It was shown that the projection process described by
equations~\eqref{eq:back:search} and \eqref{eq:back:constr} is well defined if
and only if $\oiA\vsS\oplus\vsC^\perp=\vsH$ and a solution of the linear
system~\eqref{eq:back:ls} is found if and only if $\vr_0\in\oiA\vsS$, cf.\
theorem~\ref{thm:back:pg-well} and corollary~\ref{cor:back:pg-repr}.

Because the Krylov subspace built with the initial residual $\vr_0$, i.e.,
$\vsK_n(\oiA,\vr_0)$, eventually becomes invariant at index $d=d(\oiA,\vr_0)$,
the idea is to use the sequence of Krylov subspaces
$\vsK_1(\oiA,\vr_0)\subseteq\ldots\subseteq\vsK_d(\oiA,\vr_0)$ as search spaces
in a \PetrovAndOrGalerkin{} method.

\begin{definition}
  \label{def:back:krylov-well}
  A Krylov subspace method as described above is called \emph{well defined} if
  the \PetrovAndOrGalerkin{} method is well defined for each $n\in\{1,\dots,d\}$
  and it terminates with $\vx_d\in\vsH$ satisfying $\oiA\vx_d=\vb$.
\end{definition}

Lemma~\ref{lem:back:well-opt} states conditions under which the choices
$\vsC=\vsS$ or $\vsC=\oiA\vsS$ of the constraint space lead to a well-defined
\PetrovAndOrGalerkin{} method. The following corollary summarizes the results.

\begin{cor}
  \label{cor:back:krylov-well}
  Consider a consistent linear system $\oiA\vx=\vb$ with $\oiA\in\vsL(\vsH)$ and
  $\vb\in\vsH$. Furthermore, let $\vx_0\in\vsH$ be an initial guess
  with corresponding initial residual $\vr_0=\vb-\oiA\vx_0$
  such that $d=d(\oiA,\vr_0)<\infty$ and let
  $\vsK_d(\oiA,\vr_0)\cap\vsN(\oiA)=\{0\}$.  The sequence of iterates
  $(\vx_n)_{n\in\{1,\ldots,d\}}$ that satisfy
  \[
    \vx_n=\vx_0 + \vs_n \quad\text{with}\quad\vs_n\in\vsK_n(\oiA,\vr_0)
  \]
  and
  \[
    \vr_n\DEF\vb-\oiA\vx_n\perp\vsC_n
  \]
  is well defined and $\vx_d$ is a solution of the linear
  system~\eqref{eq:back:ls}, i.e., $\oiA\vx_d=\vb$, if one of the
  following conditions holds:
  \begin{enumerate}
    \item $\vsC_n=\vsK_n(\oiA,\vr_0)$ and $\oiA$ is self-adjoint and positive
      semidefinite.  Then the iterates $\vx_n$ satisfy the optimality property
      \[
        \nrm[\oiA]{\vx - \vx_n} =
        \min_{\vz\in\vx_0+\vsK_n(\oiA,\vr_0)}\nrm[\oiA]{\vx-\vz}.
      \]
    \item $\vsC_n=\oiA\vsK_n(\oiA,\vr_0)$. Then the iterates $\vx_n$ satisfy the
      optimality property
      \[
        \nrm{\vb-\oiA\vx_n} =
        \min_{\vz\in\vx_0+\vsK_n(\oiA,\vr_0)}\nrm{\vb-\oiA\vz}.
      \]
  \end{enumerate}
\end{cor}

\begin{proof}
  In both cases, the well-definedness and optimality property of the approximate
  solutions follows from lemma~\ref{lem:back:well-opt}. $\oiA\vx_d=\vb$
  follows from corollary~\ref{cor:back:pg-repr} and the equivalence of
  \ref{prop:back:invariance:null} and \ref{prop:back:invariance:AK}
  in proposition~\ref{prop:back:invariance}.
\end{proof}

Corollary~\ref{cor:back:krylov-well} shows that a well-defined Krylov
subspace method finds the exact solution in a finite number of steps if
$d(\oiA,\vr_0)<\infty$ and $\vsK_d(\oiA,\vr_0)\cap\vsN(\oiA)=\{0\}$.
The first condition is met, e.g., if $\vsH$ is finite-dimensional. In the
finite-dimensional case, the second condition is equivalent to
$\vr_0\in\vsR(\oiA^m)$, where $m$ is the index of the zero eigenvalue of $\oiA$,
cf.\ item~\ref{prop:back:invariance:index} in
proposition~\ref{prop:back:invariance}.  The observation that a solution is
contained in a Krylov subspace if and only if $\vr_0\in\vsR(\oiA^m)$ has already
been made by Ipsen and Meyer in~\cite{IpsM98}. Furthermore, they showed that the
unique solution in the Krylov subspace is the Drazin inverse solution,
see~\cite{Dra58,IpsM98}.

\begin{definition}[Drazin inverse]
  \label{def:back:drazin}
  Let $N\DEF\dim\vsH<\infty$ and let $\oiA\in\vsL(\vsH)$. The \emph{Drazin
  inverse} of $\oiA$ is defined as the unique $\oiA^D\in\vsL(\vsH)$ that
  satisfies
  \[
    \oiA^D\oiA\oiA^D = \oiA^D,
    \qquad \oiA^D\oiA=\oiA\oiA^D
    \qquad\text{and}\qquad
    \oiA^{m+1}\oiA^D=\oiA^m,
  \]
  where $m$ is the index of the zero eigenvalue of $\oiA$.
\end{definition}

\begin{prop}
  Let $\dim\vsH<\infty$ and let $\oiA\in\vsL(\vsH)$. If
  \[
    \oiA=\vtS\mat{\ofJ&\\&\ofN}\vtS^\inv
  \]
  is a Jordan decomposition of $\oiA$ with nonsingular $\ofJ$ and nilpotent
  $\ofN$, then the Drazin inverse of $\oiA$ is given by
  \[
    \oiA^D = \vtS \mat{\ofJ^\inv&\\&0} \vtS^\inv.
  \]
\end{prop}

\begin{thm}
  \label{thm:back:krylov-drazin}
  Let $\dim\vsH<\infty$ and let $\oiA\vx=\vb$ be a consistent linear system with
  $\oiA\in\vsL(\vsH)$ and $\vb\in\vsR(\oiA)$. Furthermore, let
  $\vx_0\in\vsH$ be an initial guess with corresponding initial residual
  $\vr_0=\vb-\oiA\vx_0$ of grade $d=d(\oiA,\vr_0)$.
  \begin{enumerate}
    \item There exists a $\vx_d\in\vsK_d(\oiA,\vr_0)$ with $\oiA\vx_d=\vb$ if
      and only if $\vr_0\in\vsR(\oiA^m)$, where $m$ is the index of the zero
      eigenvalue of $\oiA$.
    \item If a solution $\vx_d\in\vsK_d(\oiA,\vr_0)$ with $\oiA\vx_d=\vb$
      exists, then it is unique and $\vx_d=\oiA^D\vb$ is the Drazin inverse
      solution.
  \end{enumerate}
\end{thm}

\begin{proof}
  See theorems 2--3 in~\cite{IpsM98}.
\end{proof}


\section{Arnoldi and Lanczos relations}
\label{sec:back:arnoldi}

For $n\leq d(\oiA,\vv)$, the Krylov basis
$\vv,\oiA\vv,\oiA^2\vv,\ldots,\oiA^{n-1}\vv$ of $\vsK_n(\oiA,\vv)$ is
ill-conditioned in practice even for a moderate order of $n$ and should be
avoided in the presence of round-off errors. Instead, an orthonormal basis
can be used which can be obtained with one of the variants of the Arnoldi
algorithm~\cite{Arn51}, e.g., the Gram--Schmidt variant in
algorithm~\ref{alg:back:arn}.

\begin{algorithm}[ht]
  \begin{algorithmic}[1]
    \Require $\oiA\in\vsL(\vsH)$, $0\neq\vv\in\vsH$ and $n\in\N_+$.
    \State $\vv_1 = \frac{\vv}{\nrm{\vv}}$
    \For{$k=1,\ldots,n$}
      \State $\vz \gets \oiA\vv_k$
      \For{$m=1,\ldots,k$} \label{alg:back:arn:for}
        \State $h_{m,k} = \ip{\vv_m}{\vz}$
        \State $\vz \gets \vz - \vv_m h_{m,k}$
      \EndFor \label{alg:back:arn:rof}
      \State $h_{k+1,k} = \nrm{\vz}$
      \If{$h_{k+1,k}=0$}
         \State\Return
          $\vtV_k=[\vv_1,\ldots,\vv_k]\in\vsH^k$
          and $\ofH_k=[h_{i,j}]_{i,j=1,\ldots,k}\in\C^{k,k}$
          \label{alg:back:arn:inv}
      \EndIf
      \State $v_{k+1} = \frac{\vz}{h_{k+1,k}}$
    \EndFor
    \State\Return
      $\vtV_{n+1}=[\vv_1,\ldots,\vv_{n+1}]\in\vsH^{n+1}$
      and $\underline{\ofH}_n=[h_{i,j}]_{\substack{
        i=1,\ldots,n+1\\
        j=1,\ldots,n
      }}\in\C^{n+1,n}$
      \label{alg:back:arn:gen}
  \end{algorithmic}
  \caption{Arnoldi algorithm (modified Gram--Schmidt). Implemented
    in~\cite{krypy} as \lstinline{krypy.utils.Arnoldi}, also with iterated
    Gram--Schmidt (\lstinline{ortho='dmgs'}) and Householder
    (\lstinline{ortho='house'}) orthogonalization.}
  \label{alg:back:arn}
\end{algorithm}

In order to facilitate the characterization of the results of
the Arnoldi algorithm it is helpful to extend the definition of an upper
Hessenberg matrix to the non-square case:

\begin{definition}[extended upper Hessenberg matrix]
  $\underline{\ofH}=[h_{i,j}]\in\C^{n+1,n}$ is called an \emph{extended upper
  Hessenberg matrix} if $h_{i,j}=0$ for all $i>j+1$. An extended upper
  Hessenberg matrix $\underline{\ofH}$ is said to be \emph{unreduced} if
  $h_{i+1,i}\neq 0$ for $i\in\{1,\ldots,n\}$.
\end{definition}

In the following, an extended upper Hessenberg matrix
$\underline{\ofH}=[h_{i,j}]\in\C^{n+1,n}$ is denoted by an underline and
its upper square part $\ofH$ without, i.e.,
\[
  \underline{\ofH}=\mat{\ofH\\h_{n+1,n}e_n^\tp}.
\]

In order to describe the computed quantities of algorithm~\ref{alg:back:arn},
the following definition is helpful.

\begin{definition}[Arnoldi relation]
  \label{def:back:arn}
  Let $\oiA\in\vsL(\vsH)$, $0\neq\vv\in\vsH$ and $n\in\N_+$.
  \begin{enumerate}
    \item An \emph{Arnoldi relation} for $\vsK_n(\oiA,\vv)$ is defined by
      $\vtV_{n+1}=[\vv_1,\ldots,\vv_{n+1}]\in\vsH^{n+1}$ and
      $\underline{\ofH}\in\C^{n+1,n}$ if $\underline{\ofH}$ is an extended
      unreduced upper Hessenberg matrix and if the following conditions are met:
      \begin{align*}
        \vv_1 \in \Span{\vv},\qquad
        \ip{\vtV_{n+1}}{\vtV_{n+1}}=\ofI_{n+1}\quad
        \text{and}\quad \oiA\vtV_n = \vtV_{n+1} \underline{\ofH},
      \end{align*}
      where $\vtV_n=[\vv_1,\ldots,\vv_n]$.
    \item An \emph{invariant Arnoldi relation} for $\vsK_n(\oiA,\vv)$ is defined
      by $\vtV_n=[\vv_1,\ldots,\vv_n]\in\vsH^n$ and $\ofH\in\C^{n,n}$ if $\ofH$ is
      an unreduced upper Hessenberg matrix and if the following conditions are
      met:
      \begin{align*}
        \vv_1 \in \Span{\vv},\qquad
        \ip{\vtV_n}{\vtV_n}=\ofI_n \quad
        \text{and}\quad \oiA\vtV_n = \vtV_n \ofH.
      \end{align*}
  \end{enumerate}
  The vectors $\vv_1,\vv_2,\dots$ are called an \emph{Arnoldi basis} and
  $\underline{\ofH}$ and $\ofH$ are called an \emph{Arnoldi matrix}.
\end{definition}

In the above definition, it is not yet clear how an Arnoldi relation is linked
to the Krylov subspace $\vsK_n(\oiA,\vv)$. The next lemma shows that an Arnoldi
relation consists of a nested orthonormal basis of Krylov subspaces and a
projected version of the operator $\oiA$.

\begin{lemma}
  \label{lem:back:arn:equi}
  Let $\oiA\in\vsL(\vsH)$, $0\neq\vv\in\vsH$ and $n\in\N_+$.
  The following statements are equivalent for
  $\vtV_{n+1}=[\vv_1,\ldots,\vv_{n+1}]\in\vsH^{n+1}$ and
  $\underline{\ofH}\in\C^{n+1,n}$:
  \begin{enumerate}
    \item $\vtV_{n+1}$ and $\underline{\ofH}$ define an Arnoldi relation for
      $\vsK_n(\oiA,\vv)$.
    \item $\vv_1,\dots,\vv_k$ is an orthonormal basis of $\vsK_k(\oiA,\vv)$ for
      $k\in\{1,\dots,n+1\}$ and
      %$\Span{\vv_1,\dots,\vv_k}=\vsK_k(\oiA,\vv)$ for $k\in\{1,\ldots,n+1\}$,
      %$\ip{\vtV_{n+1}}{\vtV_{n+1}}=\ofI_{n+1}$ and
      $\underline{\ofH}=\ip{\vtV_{n+1}}{\oiA\vtV_n}$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  1.\LRA 2.: The orthonormality of $\vtV_{n+1}$ is clear. Because
  $\oiA\vtV_n=\vtV_{n+1}\underline{\ofH}$ holds by definition also
  $\ip{\vtV_{n+1}}{\oiA\vtV_n}=\ip{\vtV_{n+1}}{\vtV_{n+1}}\underline{\ofH}=\underline{\ofH}$
  holds. It remains to show that $\Span{\vv_1,\ldots,\vv_k} = \vsK_k(\oiA,\vv)$
  for $k\in\{1,\ldots,n+1\}$. For $k=1$ this is true by definition. Assume that
  $\Span{\vv_1,\ldots,\vv_k}=\vsK_k(\oiA,\vv)$ holds for a fixed $k\in\{1,\ldots,n\}$. Then
  $\vv_{k+1}=\frac{1}{h_{k+1,k}}(\oiA \vv_k - \sum_{i=1}^k
  h_{i,k}\vv_i)\in\vsK_{k+1}(\oiA,\vv)$. Because
  $\vv_1,\ldots,\vv_{k+1}\in\vsK_{n+1}(\oiA,\vv)$ are linearly independent and
  $\dim\vsK_{k+1}\leq k+1$ it follows that
  $\Span{\vv_1,\ldots,\vv_{k+1}}=\vsK_{k+1}(\oiA,\vv)$. By induction the first
  part of the proof is complete.

  2.\LRA 1.: Again, the orthonormality of $\vtV_{n+1}$ is clear and
  $\vv_1\in\Span{\vv}$ follows from $\Span{\vv_1}=\vsK_1(\oiA,\vv)=\Span{\vv}$.
  Furthermore,
  $\Span{\oiA\vtV_n}\subseteq\vsK_{n+1}(\oiA,\vv)=\Span{\vtV_{n+1}}$ holds and
  thus
  \[
    \oiA\vtV_n
    = \oiP_{\vsK_{n+1}(\oiA,\vv)}\oiA\vtV_n
    = \vtV_{n+1} \ip{\vtV_{n+1}}{\oiA\vtV_n}
    = \vtV_{n+1} \underline{\ofH}.
  \]
  Because of $\oiA\vv_j\in\vsK_{j+1}(\oiA,\vv)=\Span{\vtV_{j+1}}$ and the
  orthonormality of $\vtV_{n+1}$, the entries of $\underline{\ofH}$ are
  $h_{i,j}=\ip{\vv_i}{\oiA\vv_j}=0$ for $2\leq j+1<i\leq n+1$ and thus
  $\underline{\ofH}$ is an extended upper Hessenberg matrix. In order to show
  that it is also unreduced, it is assumed that $h_{i+1,i}=0$ for a
  $i\in\{1,\ldots,n\}$. For $j\in\{1,\ldots,i\}$ there exist polynomials
  $q_j\in\Polys_{j-1}$ of degree $j-1$ such that $\vv_j=q_j(\oiA)\vv$. Then it
  follows from $\oiA\vtV_n=\vtV_{n+1}\underline{\ofH}$ that
  \[
    0
    = \oiA\vv_i - \sum_{j=1}^i h_{j,i}\vv_j
    = \oiA q_i(\oiA)\vv - \sum_{j=1}^i h_{j,i} q_j(\oiA)\vv
    = p(\oiA)\vv,
  \]
  where $p\in\Polys_i$ is of degree $i$. Thus $d(\oiA,\vv)\leq i$ which is a
  contradiction to $d(\oiA,\vv)\geq\dim\vsK_{n+1}(\oiA,\vv)=n+1$.
\end{proof}

With the above definition, the two possible results of
algorithm~\ref{alg:back:arn} are characterized in the following lemma:

\begin{lemma}
  \label{lem:back:arn}
  Let $\oiA\in\vsL(\vsH)$, $0\neq\vv\in\vsH$ and $n\in\N_+$.
  \begin{enumerate}
    \item If algorithm~\ref{alg:back:arn} terminates in
      line~\ref{alg:back:arn:gen}, then $n<d(\oiA,\vv)$ and $\vtV_{n+1}$ and
      $\underline{\ofH}_n$ define an Arnoldi relation for $\vsK_n(\oiA,\vv)$.
    \item If algorithm~\ref{alg:back:arn} terminates in
      line~\ref{alg:back:arn:inv} at iteration $k$ then $k=d(\oiA,\vv)$ and
      $\vtV_k$ and $\ofH_k$ define an invariant Arnoldi relation for
      $\vsK_k(\oiA,\vv)$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  The proof is analogous to the proofs of propositions~6.4--6.6 in the book of
  Saad~\cite{Saa03}.
\end{proof}

The Arnoldi vectors can be represented as a polynomial in $\oiA$ whose zeros are
the Ritz values of $\oiA$ with respect to a Krylov subspace:

\begin{lemma}
  \label{lem:back:arn:ritzpoly}
  Let $\oiA\in\vsL(\vsH)$, $0\neq\vv\in\vsH$ and let
  $\vtV_{n+1}=[\vv_1,\ldots,\vv_{n+1}]\in\vsH^{n+1}$
  and $\underline{\ofH}_n\in\C^{n+1,n}$ define an Arnoldi relation for
  $\vsK_n(\oiA,\vv)$.

  Then
  \[
    \vv_{i+1}=\alpha_{i+1}\frac{p_i(\oiA)\vv}{\nrm{p_i(\oiA)\vv}}
  \]
  for all
  $i\in\{1,\ldots,n\}$, where $|\alpha_{i+1}|=1$ and $p_i\in\Polys_i$ is the
  characteristic polynomial of $\ofH_i$, i.e., $p_i(\lambda) =
  \prod_{k=1}^i(\lambda-\theta_k^{(i)})$ with the Ritz values
  $\theta_1^{(i)},\ldots,\theta_i^{(i)}$ of $\oiA$ with
  respect to $\vsK_i(\oiA,\vv)$.
\end{lemma}

\begin{proof}
  It has been shown by Saad in~\cite[Theorem 5.1]{Saa83} that the characteristic
  polynomial of $\ofH_i$ minimizes the norm $\nrm{p(\oiA)\vv}$ over all
  $p\in\Polys_{i,\infty}$. Thus
  \[
    \nrm{p_i(\oiA)\vv}
    = \min_{p\in\Polys_{i,\infty}} \nrm{p(\oiA)\vv}
    = \min_{\vw\in\vsK_i(\oiA,\vv)}\nrm{\oiA^i\vv - \vw}
    = \nrm{\oiP_{\vsK_i(\oiA,\vv)^\perp} \oiA^i\vv}
  \]
  and because the minimizer is unique
  $p_i(\oiA)\vv=\oiP_{\vsK_i(\oiA,\vv)^\perp} \oiA^i\vv$ holds.
  The proof is complete by recognizing that
  $\vv_{i+1}\in\Span{\oiP_{\vsK_i(\oiA,\vv)^\perp} \oiA^i\vv}$.
\end{proof}

Arnoldi relations as defined in definition~\ref{def:back:arn} are not unique and
different variants of the Arnoldi algorithm may generate different Arnoldi
relations for a Krylov subspace. Though the results of the Gram--Schmidt Arnoldi
algorithm (algorithm~\ref{alg:back:arn}) are uniquely determined, this is not
true for the Householder Arnoldi algorithm~\cite{Wal88} where some freedom is
left in the construction of the involved Householder transformations which
results in possibly differing Arnoldi relations. Furthermore, non-unique Arnoldi
relations are constructed by other means in section~\ref{sec:rec:sel:kry}
for Krylov subspaces with perturbed operators and initial vectors. The following
lemma characterizes all possible Arnoldi relations for a Krylov subspace.

\begin{lemma}
  \label{lem:back:arn:unique}
  Let $\oiA\in\vsL(\vsH)$, $0\neq\vv\in\vsH$ and let
  $\vtV_{n+1}\in\vsH^{n+1}$ and $\underline{\ofH}\in\C^{n+1,n}$ define an
  Arnoldi relation for $\vsK_n(\oiA,\vv)$. The following statements are
  equivalent for $\vtW_{n+1}\in\vsH^{n+1}$ and $\underline{\ofG}\in\C^{n+1,n}$:
  \begin{enumerate}
    \item $\vtW_{n+1}$ and $\underline{\ofG}$ define an Arnoldi relation for
      $\vsK_n(\oiA,\vv)$.
    \item $\vtW_{n+1}=\vtV_{n+1} \ofD_{n+1}$ and
      $\ofG=\overline{\ofD}_{n+1}\underline{\ofH}\ofD_n$, where
      $\ofD_k=\diag(d_1,\ldots,d_k)$ is a diagonal matrix with diagonal entries
      $d_i\in\C$ such that $|d_i|=1$ for $i\in\{1,\ldots,n+1\}$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Let $\vtV_{n+1}=[\vv_1,\ldots,\vv_{n+1}]$ and
  $\vtW_{n+1}=[\vw_1,\ldots,\vw_{n+1}]$.

  1.\LRA 2.: By lemma~\ref{lem:back:arn:equi}
  $\vsK_k(\oiA,\vv)=\Span{\vv_1,\ldots,\vv_k}=\Span{\vw_1,\ldots,\vw_k}$ holds
  for all $k\in\{1,\ldots,n+1\}$. It follows that $\Span{\vv_k}=\Span{\vw_k}$
  because $\vtV_{n+1}$ and $\vtW_{n+1}$ are orthogonal. Then $\vv_k=\vw_k d_k$
  with $|d_k|=1$ follows from $\nrm{\vv_k}=\nrm{\vw_k}=1$.

  2.\LRA 1.: It is clear that $\vw_1=\vv_1 d_1\in\Span{\vv}$,
  $\ip{\vtW_{n+1}}{\vtW_{n+1}}=\ofI_{n+1}$ and that $\underline{\ofG}$ is an
  extended unreduced upper Hessenberg matrix. The proof is complete by noticing
  that
  \[
    \oiA\vtW_n
    = \oiA\vtV_n\ofD_n
    = \vtV_{n+1}\underline{\ofH}\ofD_n
    = \vtV_{n+1}\ofD_{n+1}\overline{\ofD}_{n+1}\underline{\ofH}\ofD_n
    = \vtW_{n+1}\underline{\ofG}.
  \]
\end{proof}

\begin{rmk}
  Lemma~\ref{lem:back:arn:equi} and lemma~\ref{lem:back:arn:unique} also hold in
  the case of invariant Arnoldi relations and the proofs are analogous. The
  statements are omitted here for brevity.
\end{rmk}

The following proposition is a standard result concerning the evaluation of a
polynomial in $\oiA$ if an Arnoldi relation is known.

\begin{prop}
  \label{pro:back:arnoldi-poly}
  Let $\oiA\in\vsL(\vsH)$, $0\neq\vv\in\vsH$ and let
  $\vtV_{n+1}=[\vv_1,\dots,\vv_{n+1}]\in\vsH^{n+1}$ and
  $\underline{\ofH}\in\C^{n+1,n}$ define an Arnoldi relation for
  $\vsK_n(\oiA,\vv)$. Then the following holds for any polynomial $p\in\Polys_i$
  with $i<n$:
  \[
    p(\oiA)\vv_1 = \vtV_n p(\ofH_n) \ve_1 = \vtV_{i+1} p(\ofH_{i+1})\ve_1.
  \]
\end{prop}

\begin{proof}
  The proof can be found in~\cite[proposition~6.4]{Saa11}.
\end{proof}

If the linear operator $\oiA\in\vsL(\vsH)$ is self-adjoint and $\vtV_{n+1}$ and
$\underline{\ofH}_n$ define an Arnoldi relation for $\vsK_n(\oiA,\vv)$, then
trivially
\[
  \ofH_n
  = \ip{\vtV_n}{\oiA\vtV_n}
  = \ip{\oiA\vtV_n}{\vtV_n}
  = \ofH_n^\htp
\]
and thus $\ofH_n$ is Hermitian and tridiagonal. This represents a remarkable
special case of an Arnoldi relation and is referred to as a \emph{Lanczos
relation}:

\begin{definition}[Lanczos relation]
  \label{def:back:lan}
  Let $\oiA\in\vsL(\vsH)$, $0\neq\vv\in\vsH$, $n\in\N_+$ and let
  $\vtV_{n+1}\in\vsH^{n+1}$ and $\underline{\ofH}=\mat{\ofH\\h_{n+1,n}e_n^\tp}$
  define an Arnoldi relation for $\vsK_n(\oiA,\vv)$. If $\ofH=\ofH^\htp$, then
  $\vtV_{n+1}$ and $\underline{\ofH}$ define a \emph{Lanczos relation}. Because
  the (extended) Hessenberg matrix is tridiagonal, it is denoted by
  \begin{equation}
    \underline{\ofT}
    \DEF \underline{\ofH}
    = \mat{\ofT \\ \delta_{n+1}\ve_n^\tp}
    = \mat{
      \gamma_1 & \overline{\delta}_2 &                     &               & \\
      \delta_2 & \gamma_2            & \overline{\delta}_3 &               & \\
               & \ddots              & \ddots              & \ddots        & \\
               &                     & \delta_{n-1}        & \gamma_{n-1}  & \overline{\delta}_n \\
               &                     &                     & \delta_n      & \gamma_n \\
               &                     &                     &               & \delta_{n+1}
    }.
    \label{eq:back:lanmatrix}
  \end{equation}
\end{definition}

Analogous to the invariant Arnoldi relation from
definition~\ref{def:back:arn}, the invariant Lanczos relation can be
defined with a basis tuple $\vtV_n\in\vsH^n$ and a Hermitian tridiagonal matrix
$\ofT\in\C^{n,n}$. Note that according to lemma~\ref{lem:back:arn:unique}, an
equivalent Lanczos relation can be constructed where the tridiagonal matrix is
real and symmetric.

The importance of Lanczos relations is primarily of algorithmic nature: if
$\ofH_n$ is tridiagonal, then $h_{m,k}=0$ for $m<k-1$ and thus all but two
orthogonalizations in the Arnoldi algorithm can be omitted, i.e., the
\emph{for}-loop in line~\ref{alg:back:arn:for} of algorithm~\ref{alg:back:arn}
only runs from $k-1$ to $k$. The resulting algorithm is given in
algorithm~\ref{alg:back:lan} and is called the \emph{Lanczos
algorithm}, attributed to the works of Lanczos~\cite{Lan50,Lan52} in 1950 and
1952. However, there is a one-to-one correspondence between the Lanczos
algorithm and the Stieltjes recurrence for the computation of orthonormal
polynomials whose history goes far beyond the 20th century. An extensive
description of this link and profound historical remarks can be found
in chapter~3 of \cite{LieS13}.

\begin{algorithm}[ht]
  \begin{algorithmic}[1]
    \Require Self-adjoint $\oiA\in\vsL(\vsH)$, $0\neq\vv\in\vsH$ and $n\in\N$.
    \State $\vv_0 = 0$, $\delta_1=0$
    \State $\vv_1 = \frac{\vv}{\nrm{\vv}}$
    \For{$k=1,\ldots,n$}
      \State $\vz \gets \oiA\vv_k$
      \State $\vz \gets \vz - \delta_k \vv_{k-1}$
      \State $\gamma_k = \ip{\vv_k}{\vz}$
      \State $\vz \gets \vz - \gamma_k \vv_k$
      \State $\delta_{k+1} = \nrm{\vz}$
      \If{$\delta_{k+1}=0$}
        \State\Return
          $\vtV_k=[\vv_1,\ldots,\vv_k]\in\vsH^k$
          and $\ofT_k\in\R^{k,k}$ ($k\times k$-submatrix of
          \eqref{eq:back:lanmatrix})
      \EndIf
      \State $v_{k+1} = \frac{\vz}{\delta_{k+1}}$
    \EndFor
    \State\Return
    $\vtV_{n+1}=[\vv_1,\ldots,\vv_{n+1}]\in\vsH^{n+1}$
      and $\underline{\ofT}_n\in\R^{n+1,n}$ as in \eqref{eq:back:lanmatrix}
  \end{algorithmic}
  \caption{Lanczos algorithm. Implemented in~\cite{krypy} as
  \lstinline{krypy.utils.Arnoldi} (\lstinline{ortho='lanczos'}).}
  \label{alg:back:lan}
\end{algorithm}

In comparison with the \emph{full recurrence} in
the Arnoldi algorithm, the \emph{short recurrence} in the Lanczos algorithm
reduces the computation time and memory requirements to a constant per
iteration. However, in order to carry over the mathematical and computational
appeal to practical applications, counter measures have to be taken against the
Lanczos algorithm's high sensitivity to round-off errors. Some implications of
round-off errors and possible remedies are discussed briefly in
section~\ref{sec:back:round-off}.


\section{CG method}
\label{sec:back:cg}

In order to solve a linear system $\oiA\vx=\vb$ with a self-adjoint and
positive-definite operator $\oiA$, Hestenes and Stiefel introduced the
\emph{method of conjugate gradients} (CG method) in their seminal
paper~\cite{HesS52} in 1952. In contrast to the original publication, the
presentation in this section makes use of the Galerkin framework (cf.\
section~\ref{sec:back:galerkin}).

Throughout this section the operator $\oiA\in\vsL(\vsH)$ of the linear
system~\eqref{eq:back:ls} is assumed to be self-adjoint and positive
semidefinite and the linear system is assumed to be consistent, i.e.,
$\vb\in\vsR(\oiA)$. Furthermore, an initial guess $\vx_0\in\vsH$ is assumed to
be given such that the corresponding initial residual $\vr_0=\vb-\oiA\vx_0$
satisfies $d=d(\oiA,\vr_0)<\infty$, which holds, e.g., if $\dim\vsH<\infty$.

Corollary~\ref{cor:back:krylov-well} gives a sufficient condition for a Galerkin
method with Krylov subspaces as search and constraint spaces to be well defined
in the case where $\oiA$ is self-adjoint and positive semidefinite. The
assumption $\vsK_d(\oiA,\vr_0)\cap\vsN(\oiA) =\{0\}$ is satisfied since
$\vsK_d(\oiA,\vr_0)\subseteq\vsR(\oiA)\perp\vsN(\oiA)$ holds because $\oiA$ is
self-adjoint. Item 1.\ in corollary~\ref{cor:back:krylov-well} states that the
Galerkin method is well defined if the search and constraint spaces are chosen
as Krylov subspaces, i.e.,  $\vsS_n=\vsC_n=\vsK_n(\oiA,\vr_0)$. Furthermore, the
iterates $\vx_n$ minimize the $\oiA$-norm of the error, i.e.,
\begin{equation*}
  \nrm[\oiA]{\vx - \vx_n} =
  \min_{\vz\in\vx_0+\vsK_n(\oiA,\vr_0)}\nrm[\oiA]{\vx-\vz},
\end{equation*}
and $\vx_d$ is a solution of the linear system.

Because $\oiA$ is self-adjoint, the Lanczos algorithm (cf.\
section~\ref{sec:back:arnoldi}) can be applied with $\oiA$ and the initial
vector $\vr_0$. For $n<d=d(\oiA,\vr_0)$, the Lanczos algorithm generates a
$\vtV_{n+1}\in\vsH^{n+1}$ and a $\underline{\ofT}_n= \mat{\ofT_n \\
\delta_{n+1}\ve_n^\tp}\in\R^{n+1,n}$ that define a Lanczos relation for
$\vsK_n(\oiA,\vr_0)$, cf.\
definition~\ref{def:back:lan}.  In the $d$-th step, the generated
$\vtV_d\in\vsH^d$ and $\ofT_d\in\R^{d,d}$ define an invariant Lanczos
relation for $\vsK_d(\oiA,\vr_0)$.

For $n\leq d$, the Galerkin method with
$\vsS_n=\vsC_n=\vsK_n(\oiA,\vr_0)=\Span{\vtV_n}$ is well defined and by
theorem~\ref{thm:back:pg-well} the matrix $\ofT_n=\ip{\vtV_n}{\oiA\vtV_n}$ is
nonsingular and thus Hermitian and positive definite. Note that although $\oiA$
is allowed to be singular, the matrix $\ofT_n$ is nonsingular. By
corollary~\ref{cor:back:pg-repr}, the iterates $\vx_n$ are given by
\[
  \vx_n
  = \vx_0 + \vtV_n \ip{\vtV_n}{\oiA\vtV_n}^\inv\ip{\vtV_n}{\vr_0}
  = \vx_0 + \vtV_n \ofT_n^\inv \ve_1 \nrm{\vr_0}.
\]
A rather technical derivation, which is omitted here, then leads to the CG
algorithm~\ref{alg:back:cg}, cf.\ \cite[section 2.5.1]{LieS13} for the
full derivation.

\begin{algorithm}[ht]
  \begin{algorithmic}[1]
    \Require Self-adjoint and positive-semidefinite $\oiA\in\vsL(\vsH)$, right
    hand side $\vb\in\vsR(\oiA)$, initial guess $\vx_0\in\vsH$ and maximal
    number of iterations $n_{\max}\in\N$.
    \State $\vr_0 = \vb -\oiA\vx_0$, $\vp_0 = \vr_0$
    \For{$n=1,\ldots,n_{\max}$}
      \State $\alpha_{n-1} =
        \frac{\nrm{\vr_{n-1}}^2}{\ip{\vp_{n-1}}{\oiA\vp_{n-1}}}$
      \State $\vx_n = \vx_{n-1} + \alpha_{n-1}\vp_{n-1}$
      \State $\vr_n = \vr_{n-1} - \alpha_{n-1}\oiA\vp_{n-1}$
      \If{stopping criterion is reached}
        \State\Return $\vx_n$
      \EndIf
      \State $\omega_n = \frac{\nrm{\vr_n}^2}{\nrm{\vr_{n-1}}^2}$
      \State $\vp_n = \vr_n + \omega_n \vp_{n-1}$
    \EndFor
    \State\Return $\vx_{n_{\max}}$
  \end{algorithmic}
  \caption{CG algorithm.}
  \label{alg:back:cg}
\end{algorithm}

\subsection*{Convergence of CG}

The most important properties of the CG method (algorithm~\ref{alg:back:cg}) are
gathered in the following theorem.

\begin{thm}[Convergence of CG]
  \label{thm:back:cg}
  Let $\oiA\in\vsL(\vsH)$ be self-adjoint and positive semidefinite,
  $\vb\in\vsR(\oiA)$ and $\vx\in\vsH$ such that $\oiA\vx=\vb$. Furthermore, let
  $\vx_0\in\vsH$ be an initial guess with corresponding initial residual
  $\vr_0=\vb-\oiA\vx_0$ such that $d=d(\oiA,\vr_0)<\infty$.

  Then the CG method (see algorithm~\ref{alg:back:cg}) is well defined and the
  following holds:
  \begin{enumerate}
    \item \label{thm:back:cg:min}
      The error norm in iteration $n\leq d$ satisfies
      \[
        \nrm[\oiA]{\vx - \vx_n}
        = \min_{\vz\in\vx_0+\vsK_n(\oiA,\vr_0)}\nrm[\oiA]{\vx-\vz}
        = \min_{p\in\Polys_{n,0}} \nrm[\oiA]{p(\oiA)(\vx-\vx_0)}.
      \]
    \item The error in iteration $n\leq d$ is given by
      \[
        \vx-\vx_n = p_n^\CG(\oiA)(\vx-\vx_0)
      \]
      with $p_n^\CG\in\Polys_{n,0}$ and
      \[
        p_n^\CG(\lambda)
        = \prod_{i=1}^n\left(1-\frac{\lambda}{\theta_i^{(n)}}\right),
      \]
      where $\theta_1^{(n)},\ldots,\theta_n^{(n)}\in\R_+$ are the Ritz values of
      $\oiA$ with respect to $\vsK_n(\oiA,\vr_0)$, i.e., the eigenvalues of
      $\ofT_n=\ip{\vtV_n}{\oiA\vtV_n}$ (cf.\ section~\ref{sec:back:ritz}).
  \end{enumerate}
\end{thm}

\begin{proof}
  The well-definedness of the method and the first equality in item 1.\ follow
  from corollary~\ref{cor:back:krylov-well}. The last equality in item 1.\
  follows from the fact that for any $\vy\in\vsK_n(\oiA,\vr_0)$, there exists a
  $q\in\Polys_{n-1}$ such that $\vy=q(\oiA)\vr_0=q(\oiA)\oiA(\vx-\vx_0)$.  A
  proof of item 2.\ can be found in chapter 5.6 in \cite{LieS13}.
\end{proof}

Item~\ref{thm:back:cg:min} in theorem~\ref{thm:back:cg} states that the iterates
of the CG method minimize the error in the $\oiA$-norm in exact arithmetic.
This allows the construction of a priori bounds on the error in the $\oiA$-norm,
i.e., bounds that are based on certain properties of the operator $\oiA$.

In the finite-dimensional case, i.e., $N\DEF \dim\vsH<\infty$, a self-adjoint
operator $\oiA\in\vsL(\vsH)$ has an eigen-decomposition of the form
\[
  \oiA=\vtU\ofD\vtU^\adj,
\]
where $\vtU=[\vu_1,\ldots,\vu_N]\in\vsH^N$ is orthonormal, i.e.,
$\vtU^\adj\vtU=\ip{\vtU}{\vtU}=\ofI_N$ with $\vtU^\adj\vx\DEF\ip{\vtU}{\vx}$,
and $\ofD=\diag(\lambda_1,\ldots,\lambda_N)$ is a diagonal matrix with $\oiA$'s
eigenvalues. Let the eigenvalues be sorted and let $j+1$ be the index of the
first nonzero eigenvalue, i.e.,
$0=\lambda_1=\ldots=\lambda_j<\lambda_{j+1}\leq\ldots\leq\lambda_N$.
Furthermore, let $\vtU_2\DEF[\vu_{j+1},\ldots,\vu_N]$ and
$\ofD_2\DEF\diag(\lambda_{j+1},\ldots,\lambda_N)$. Then, in the setting of
theorem~\ref{thm:back:cg}, the $n$-th iterate of the CG method $\vx_n$ satisfies
\begin{align}
  \nrm[\oiA]{\vx-\vx_n}
  &= \min_{p\in\Polys_{n,0}} \nrm[\oiA]{p(\oiA)(\vx-\vx_0)}
  = \min_{p\in\Polys_{n,0}} \nrm{\oiA^{\frac{1}{2}} p(\oiA)(\vx-\vx_0)}
    \nonumber\\
  &= \min_{p\in\Polys_{n,0}} \nrm{p(\oiA)\oiA^{\frac{1}{2}}(\vx-\vx_0)}
  = \min_{p\in\Polys_{n,0}} \nrm{\vtU p(\ofD)\vtU^\adj \vtU
    \ofD^{\frac{1}{2}}\vtU^\adj(\vx-\vx_0)}
    \nonumber\\
  &= \min_{p\in\Polys_{n,0}} \nrm[2]{p(\ofD_2)
    \ofD_2^{\frac{1}{2}} \vtU_2^\adj(\vx-\vx_0)}
    \leq \min_{p\in\Polys_{n,0}}
    \nrm[2]{p(\ofD_2)}\nrm[2]{\ofD_2^{\frac{1}{2}}\vtU_2^\adj(\vx-\vx_0)}
    \label{eq:back:cg:ineq}\\
  &= \min_{p\in\Polys_{n,0}}
    \max_{\lambda\in\Lambda(\oiA)\setminus\{0\}}|p(\lambda)|
    \nrm[\oiA]{\vx-\vx_0}. \label{eq:back:cg:minmax-discrete}
\end{align}

The discrete set $\Lambda(\oiA)\setminus\{0\}$ in the min-max
problem~\eqref{eq:back:cg:minmax-discrete} can be replaced by the interval
$[\lambda_{j+1},\lambda_N]$ and the bound becomes
\begin{equation}
  \nrm[\oiA]{\vx-\vx_n}
  \leq \min_{p\in\Polys_{n,0}} \max_{\lambda\in[\lambda_{j+1},\lambda_N]}|p(\lambda)|
    \nrm[\oiA]{\vx-\vx_0}.
    \label{eq:back:cg:minmax-interval}
\end{equation}
The probably best known convergence bound for CG results
from~\eqref{eq:back:cg:minmax-interval} if the polynomial minimization is
replaced by an appropriately scaled and shifted Chebyshev polynomial. The
resulting bound makes use of the following definition of the
\emph{effective condition number} of an operator:

\begin{definition}[Effective condition number]
  \label{def:back:effcond}
  Let $\oiA\in\vsL(\vsH)$ be self-adjoint and let $\alpha(\oiA)\DEF
  \inf_{\lambda\in\Lambda(\oiA)\setminus\{0\}}|\lambda|>0$.  Then
  $\kappa_{\eff}(\oiA)\DEF \frac{\nrm{\oiA}}{\alpha(\oiA)}$ is the
  \emph{effective condition number} of $\oiA$.
\end{definition}

\begin{thm}[CG $\kappa$-bound]
  \label{thm:back:cg-kappa}
  Let the assumptions of theorem~\ref{thm:back:cg} hold and let $\oiA\neq 0$.
  Then
  \begin{equation}
    \nrm[\oiA]{\vx - \vx_n}
    \leq 2 \left(
    \frac{\sqrt{\kappa_{\eff}(\oiA)} - 1}
      {\sqrt{\kappa_{\eff}(\oiA)} + 1}
    \right)^n
    \nrm[\oiA]{\vx - \vx_0}.
    \label{eq:back:cg:kappa}
  \end{equation}
\end{thm}

\begin{proof}
  First note that the linear system $\oiA\vx=\vb$ with a possibly singular
  operator $\oiA$ can be reduced to the linear system
  $\oiB\vy=\vb$ where $\oiB\DEF\oiA|_{\vsN(\oiA)^\perp}$ is nonsingular.

  The proof for the finite-dimensional case can be found in many books on Krylov
  subspace methods, e.g., in~\cite{LieS13}. The infinite-dimensional case has
  been handled by Daniel in~\cite{Dan67}.
\end{proof}

Note that the influence of the solution $\vx$ and the initial approximation
$\vx_0$ has been separated from the polynomial minimization in the
inequality~\eqref{eq:back:cg:ineq} and all bounds derived from
inequality~\eqref{eq:back:cg:minmax-discrete}, e.g., the
$\kappa$-bound~\eqref{eq:back:cg:kappa}, may severely overestimate the error.

Other well-known approaches for convergence bounds try to capture the often
observed ``superlinear'' convergence behavior of the CG method. One such
approach is to choose $p$ in~\eqref{eq:back:cg:minmax-discrete} as a
\emph{composite polynomial} $p=q_l\cdot\tilde{p}$ where $q_l\in\Polys_{l,0}$ is
a fixed polynomial for $l\leq n$ and to carry out the minimization for
$\tilde{p}\in\Polys_{n-l,0}$, i.e.,
\[
  \frac{\nrm[\oiA]{\vx-\vx_n}}{\nrm[\oiA]{\vx-\vx_0}}
  \leq \min_{\tilde{p}\in\Polys_{n-l,0}}
    \max_{\lambda\in\{\lambda_{j+1},\dots,\lambda_N\}}
    |q_l(\lambda)\tilde{p}(\lambda)|.
\]
Picking $q_l(\lambda)=\prod_{i=N-l+1}^N
\left(1-\frac{\lambda}{\lambda_i}\right)$ yields $|q_l(\lambda_i)|\leq 1$ for
$i\in\{j+1,\dots,N-l\}$ and thus
\begin{align}
  \frac{\nrm[\oiA]{\vx-\vx_n}}{\nrm[\oiA]{\vx-\vx_0}}
  &\leq
    \max_{\lambda\in\{\lambda_{j+1},\dots,\lambda_{N-l}\}}|q_l(\lambda)|
    \min_{\tilde{p}\in\Polys_{n-l,0}}
    \max_{\lambda\in\{\lambda_{j+1},\dots,\lambda_{N-l}\}}
    |\tilde{p}(\lambda)|\notag\\
  &\leq
    \min_{\tilde{p}\in\Polys_{n-l,0}}
    \max_{\lambda\in\{\lambda_{j+1},\dots,\lambda_{N-l}\}}
    |\tilde{p}(\lambda)|
  \leq 2 \left(
    \frac{\sqrt{\kappa_l(\oiA)} - 1}
      {\sqrt{\kappa_l(\oiA)} + 1}
    \right)^{n-l},
    \label{eq:back:cg:kappa-comp}
\end{align}
where
$\kappa_l(\oiA)=\frac{\lambda_{N-l}}{\lambda_{j+1}}\leq\kappa_{\eff}(\oiA)$.
Bounds of this form have been proposed by Axelsson~\cite{Axe76},
Jennings~\cite{Jen77} and others. The bound~\eqref{eq:back:cg:kappa-comp}
predicts a faster convergence rate of the CG method (in infinite precision)
after $l$ iterations if there are $l$ eigenvalues at the upper end of $\oiA$'s
spectrum that are well-separated from the remaining spectrum. However, as
illustrated in the book by Liesen and Strako\v{s}~\cite{LieS13} and the article
by Gergelits and Strako\v{s}~\cite{GerS14}, bounds based on composite
polynomials are questionable in the presence of round-off errors.

\begin{algorithm}[ht]
  \begin{algorithmic}[1]
    \Require Self-adjoint and positive-semidefinite $\oiA\in\vsL(\vsH)$,
    self-adjoint and positive-definite preconditioner $\oiM\in\vsL(\vsH)$, right
    hand side $\vb\in\vsR(\oiA)$, initial guess $\vx_0\in\vsH$ and maximal
    number of iterations $n_{\max}\in\N$.
    \State $\vr_0 = \vb -\oiA\vx_0$, $\vp_0=\vz_0=\oiM\vr_0$
    \For{$n=1,\ldots,n_{\max}$}
      \State $\alpha_{n-1} =
      \frac{\ip{\vz_{n-1}}{\vr_{n-1}}}{\ip{\vp_{n-1}}{\oiA\vp_{n-1}}}$
      \State $\vx_n = \vx_{n-1} + \alpha_{n-1}\vp_{n-1}$
      \State $\vr_n = \vr_{n-1} - \alpha_{n-1}\oiA\vp_{n-1}$
      \If{stopping criterion is reached}
        \State\Return $\vx_n$
      \EndIf
      \State $\vz_k = \oiM\vr_n$
      \State $\omega_n = \frac{\ip{\vz_n}{\vr_n}}{\ip{\vz_{n-1}}{\vr_{n-1}}}$
      \State $\vp_n = \vz_n + \omega_n \vp_{n-1}$
    \EndFor
    \State\Return $\vx_{n_{\max}}$
  \end{algorithmic}
  \caption{Preconditioned CG algorithm, cf.~\cite{HesS52}. Implemented as
    \lstinline{krypy.linsys.Cg} in \cite{krypy}.}
  \label{alg:back:pcg}
\end{algorithm}

\subsection*{Preconditioned CG}

In order to speed up the computation in practical applications, CG is not
directly applied to the linear system $\oiA\vx=\vb$, but instead applied to
a \emph{preconditioned} linear system
\begin{equation}
  \oiM \oiA \vx = \oiM \vb,
  \label{eq:back:ls-prec}
\end{equation}
with the inner product $\ipdots[\oiM^\inv]$ defined by
$\ip[\oiM^\inv]{\vx}{\vy}=\ip{\vx}{\oiM^\inv\vy}$. Here, $\oiM\in\vsL(\vsH)$ is
a suitably chosen self-adjoint and positive-definite operator, the
\emph{preconditioner}. The actual choice of the preconditioner highly depends on
the problem that has to be solved.

Note that the CG method is in fact well defined when applied to the linear
system~\eqref{eq:back:ls-prec}, because
\[
  \ip[\oiM^\inv]{\vx}{\oiM\oiA\vy}
  = \ip{\vx}{\oiA\vy}
  = \ip{\oiA\vx}{\vy}
  = \ip[\oiM^\inv]{\oiM\oiA\vx}{\vy}
\]
and $\ip[\oiM^\inv]{\vx}{\oiM\oiA\vx}=\ip{\vx}{\oiA\vx}\geq 0$ hold for all
$\vx,\vy\in\vsH$. Thus the linear operator $\oiM\oiA$ is self-adjoint and
positive (semi-)definite with respect to $\ipdots[\oiM^\inv]$. If CG is applied
to the preconditioned linear system~\eqref{eq:back:ls-prec} with the inner
product $\ipdots[\oiM^\inv]$, then
\[
  \ip[\oiM\oiA]{\vx}{\vy}
  = \ip[\oiM^\inv]{\vx}{\oiM\oiA\vy}
  = \ip{\vx}{\oiA\vy}
\]
shows that the error is still minimized in the $\oiA$-norm, i.e., the iterates
$\vx_n$ satisfy
\begin{equation*}
  \nrm[\oiA]{\vx-\vx_n}
  = \min_{p\in\Polys_{n,0}}\nrm[\oiA]{p(\oiM\oiA)(\vx - \vx_0)}.
\end{equation*}
For the preconditioned CG method, the $\kappa$-bound~\eqref{eq:back:cg:kappa}
becomes
\begin{equation*}
  \nrm[\oiA]{\vx - \vx_n}
  \leq 2 \left(
  \frac{\sqrt{\kappa_{\eff}(\oiM\oiA)} - 1}
  {\sqrt{\kappa_{\eff}(\oiM\oiA)} + 1}
  \right)^n
  \nrm[\oiA]{\vx - \vx_0}.
\end{equation*}
Comparing this bound with its unpreconditioned
counterpart~\eqref{eq:back:cg:kappa}, the term \emph{preconditioning} seems
reasonable. However, it should be pointed out, that the $\kappa$-bound is not
descriptive in many cases and that the condition number is not enough to
estimate the convergence behavior of the CG method. The actual convergence
behavior of the CG method is often superlinear while the $\kappa$-bound only
predicts linear convergence, i.e., a constant factor for the reduction of the
error norm.

By storing an additional vector, the preconditioned CG method can be implemented
with only one application of $\oiA$ and one application of $\oiM$ per iteration,
see algorithm~\ref{alg:back:pcg}. A derivation of this algorithm can be found
in the book of Elman, Silvester and Wathen~\cite{ElmSW05}. An algorithm for the
CG method in a Hilbert space setting can also be found in the work of
G\"{u}nnel, Herzog and Sachs~\cite{GueHS14}.


\section{Minimal residual methods}
\label{sec:back:mr}

In 1975, Paige and Saunders~\cite{PaiS75} introduced the minimal residual method
(MINRES method) for the solution of the linear system~\eqref{eq:back:ls} with a
self-adjoint but possibly indefinite operator $\oiA$. The method is derived from
a Lanczos relation and the constructed iterates $\vx_n\in\vx_0+\vsK_n(\oiA,\vr_0)$
are chosen such that the residual norm is minimal. A decade later, Saad and
Schultz~\cite{SaaS86} proposed the generalized minimal residual method (GMRES
method) for a general (possibly non-self-adjoint) operator $\oiA$ by using an
Arnoldi relation instead of a Lanczos relation. For a self-adjoint operator
$\oiA$, GMRES is \emph{mathematically} equivalent to MINRES but the results of
both methods can differ significantly in the presence of round-off errors, see
section~\ref{sec:back:round-off}. The presentation in this section proceeds
in reverse chronological order by first introducing the GMRES method and
afterwards the special case MINRES.

\subsection{GMRES method}
\label{sec:back:mr:gmres}

In this subsection, the linear system~\eqref{eq:back:ls} is again assumed to be
consistent, i.e., $\vb\in\vsR(\oiA)$. The operator $\oiA\in\vsL(\vsH)$, the initial
guess $\vx_0\in\vsH$ and the corresponding initial residual
$\vr_0=\vb-\oiA\vx_0$ are assumed to fulfill $d=d(\oiA,\vr_0)\leq\infty$ and
\begin{equation}
  \vsK_d(\oiA,\vr_0)\cap\vsN(\oiA)=\{0\}.
  \label{eq:back:gmres-cond}
\end{equation}
The first condition holds true, e.g., if $\dim\vsH<\infty$. Unlike the
situation of the CG method in section~\ref{sec:back:cg}, the latter condition is
not always fulfilled. However, it is obviously fulfilled if $\oiA$ is
nonsingular. Condition~\eqref{eq:back:gmres-cond} also holds if
$\vsR(\oiA)\cap\vsN(\oiA)=\{0\}$ because $\vb\in\vsR(\oiA)$.
Condition~\eqref{eq:back:gmres-cond} is the subject of
further discussion in section~\ref{sec:rec:def:mr} in the context of the
deflated GMRES method.

With the above assumptions, item 2.\ in corollary~\ref{cor:back:krylov-well}
shows that the Petrov--Galerkin method with search space
$\vsS_n=\vsK_n(\oiA,\vr_0)$ and constraint space $\vsC_n=\oiA\vsS_n$ is well
defined. Furthermore, the iterates $\vx_n$ minimize the residual norm, i.e.
\begin{equation}
  \nrm{\vb-\oiA\vx_n} =
  \min_{\vz\in\vx_0+\vsK_n(\oiA,\vr_0)}\nrm{\vb-\oiA\vz},
  \label{eq:back:gmres-opt}
\end{equation}
and $\vx_d$ is a solution of the linear system~\eqref{eq:back:ls}.

For a general $\oiA\in\vsL(\vsH)$, the Arnoldi algorithm can be used with the
initial vector $\vr_0$. For $n<d$, a $\vtV_n+1\in\vsH^{n+1}$ and a
$\underline{\ofH}_n=\mat{\ofT_n \\ h_{n+1,n} \ve_n^\tp}\in\C^{n+1,n}$ are
generated that define an Arnoldi relation for $\vsK_n(\oiA,\vr_0)$. In the
$d$-th step, the generated $\vtV_d\in\vsH^d$ and $\vsH_d\in\C^{d,d}$ define an
invariant Arnoldi relation for $\vsK_d(\oiA,\vr_0)$.

The Petrov--Galerkin method with search space
$\vsS_n=\vsK_n(\oiA,\vr_0)$ and constraint space $\vsC_n=\oiA\vsS_n$ is well
defined for $n\leq d$ and the above assumptions. Because $\vx_n=\vx_0+\vtV_n
\vy_n$ for a $\vy_n\in\C^n$, the residual norm becomes for $n<d$:
\[
  \nrm{\vb-\oiA\vx_n}
  = \nrm{\vtV_{n+1}(\ve_1\nrm{\vr_0} - \underline{\ofH}_n\vy_n)}
  = \nrm[2]{\ve_1\nrm{\vr_0} - \underline{\ofH}_n\vy_n}.
\]
The optimality property~\eqref{eq:back:gmres-opt} then reads
\begin{equation}
  \nrm[2]{\ve_1\nrm{\vr_0} - \underline{\ofH}_n\vy_n}
  = \min_{\vz\in\C^n} \nrm[2]{\ve_1\nrm{\vr_0} - \underline{\ofH}_n\vz}.
  \label{eq:back:gmres-ls}
\end{equation}
In the GMRES algorithm, this least squares problem is solved by maintaining a
QR-factorization of $\underline{\ofH}_n=\ofQ_n\mat{\ofR_n\\0}$ with
$\ofQ_n^\htp\ofQ_n=\ofI_{n+1}$ and $\ofR_n\in\C^{n,n}$ upper triangular. The
factors $\ofQ_n$ and $\ofR_n$ can be updated from the factors $\ofQ_{n-1}$ and
$\ofR_{n-1}$ from the last Arnoldi step by using Givens rotations.
By theorem~\ref{thm:back:pg-well}, the matrix
\begin{align*}
  \ip{\oiA\vtV_n}{\oiA\vtV_n}
  &= \ip{\vtV_{n+1}\underline{\ofH}_n}{\vtV_{n+1}\underline{\ofH}_n}
  = \underline{\ofH}_n^\htp\underline{\ofH}_n
  = \mat{\ofR_n\\0}^\htp \ofQ_n^\htp\ofQ_n \mat{\ofR_n\\0}\\
  &= \ofR_n^\htp \ofR_n
\end{align*}
is nonsingular and thus also $\ofR_n$ is nonsingular. The residual norm
\[
  \nrm[2]{\ve_1\nrm{\vr_0} - \ofQ_n\mat{\ofR_n\\0}\vy_n}
  = \nrm[2]{\ofQ_n^\htp\ve_1\nrm{\vr_0} -\mat{\ofR_n\\0}\vy_n}
\]
is thus minimized by $\vy_n = \ofR^\inv \vu_n$, where
$[\vu_n,\eta_n]^\tp=\ofQ_n^\htp\ve_1\nrm{\vr_0}$ with $\vu_n\in\C^n$ and
$\eta_n\in\C$. The $n$-th GMRES approximation then is given
\[
  \vx_n=\vx_0 + \vtV_n\ofR_n^\inv\vu_n
\]
and the residual norm is
\[
  \nrm{\vb-\oiA\vx_n} = |\eta_n|.
\]

\begin{algorithm}[ht]
  \begin{algorithmic}[1]
    \Require $\oiA\in\vsL(\vsH)$, right hand side $\vb\in\vsR(\oiA)$ and
    initial guess $\vx_0\in\vsH$ such that
    condition~\eqref{eq:back:gmres-cond} is fulfilled. Maximal number of
    iterations $n_{\max}\in\N$.
    \State $\vr_0 = \vb -\oiA\vx_0$
    \For{$n=1,\ldots,n_{\max}$}
      \State Generate Arnoldi relation for $\vsK_n(\oiA,\vr_0)$: either
        \begin{itemize}
          \item $\vtV_{n+1}\in\vsH^{n+1}$ and $\underline{\ofH}_n\in\C^{n+1,n}$
            or
          \item $\vtV_n\in\vsH^n$ and $\underline{\ofH}_n\in\C^{n,n}$ if
            $\vsK_n(\oiA,\vr_0)$ is $\oiA$-invariant.
        \end{itemize}
        \label{alg:back:gmres:arnoldi}
      \If{$\vsK_n(\oiA,\vr_0)$ is $\oiA$-invariant}
        \State\Return $\vx_n=\vx_0+\vtV_n\ofH_n^\inv\ve_1\nrm{\vr_0}$
      \Else
        \State Compute QR factorization of
          $\underline{\ofH}_n=\ofQ_n\mat{\ofR_n\\0}$,
          $\ofQ_n\in\C^{n+1,n+1}$, $\ofR_n\in\C^{n,n}$.
        \label{alg:back:gmres:qr}
        \State $[\vu_n,\eta_n]^\tp = \ofQ_n^\htp\ve_1\nrm{\vr_0}$ with
        $\vu_n\in\C^n$, $\eta_n\in\C$.
        \State Form $\vx_n = \vx_0 + \ofR_n^\inv\vu_n$ if necessary (note that
          $\nrm{\vb-\oiA\vx_n}=|\eta_n|$).
        \If{stopping criterion is reached}
          \State\Return $\vx_n$
        \EndIf
      \EndIf
    \EndFor
    \State\Return $\vx_{n_{\max}}$
  \end{algorithmic}
  \caption{GMRES algorithm (generic version; cf.~\cite{SaaS86}). Implemented as
    \lstinline{krypy.linsys.Gmres} in \cite{krypy}.}
  \label{alg:back:gmres}
\end{algorithm}

A generic version of the GMRES algorithm is given in
algorithm~\ref{alg:back:gmres}. In line~\ref{alg:back:gmres:arnoldi} of this
algorithm, the Arnoldi relation can be constructed with the modified
Gram--Schmidt or Householder Arnoldi algorithms, cf.
section~\ref{sec:back:arnoldi}. The computation of the QR factorization in
line~\ref{alg:back:gmres:qr} can be computed iteratively by applying $n$ Givens
rotations in the $n$-th iteration, cf. \cite{SaaS86}.

\subsubsection*{Convergence of GMRES}

In this subsection, some well-known convergence properties of the GMRES method
are stated for the finite-dimensional case, i.e., $N\DEF\dim\vsH<\infty$.
Compared to the analysis of the convergence behavior of the CG method for a
self-adjoint and positive-definite operator $\oiA$, the analysis becomes more
intricate in the situation of the GMRES method with a general linear operator
$\oiA$. The most important properties of the GMRES method are gathered in the
following theorem.

\begin{thm}[Convergence of GMRES]
  \label{thm:back:gmres}
  Let $N\DEF\dim\vsH<\infty$ and let $\oiA\vx=\vb$ be a consistent linear system
  with $\oiA\in\vsL(\vsH)$ and $\vb\in\vsH$. Furthermore, let $\vx_0\in\vsH$ be
  an initial guess such that condition~\eqref{eq:back:gmres-cond} is fulfilled
  with $d=d(\oiA,\vr_0)$.

  Then the GMRES method (see algorithm~\ref{alg:back:gmres}) is well defined and
  the following holds:
  \begin{enumerate}
    \item \label{thm:back:gmres:min}
      The residual norm in iteration $n\leq d$ satisfies
      \begin{equation}
        \nrm{\vb - \oiA\vx_n}
        = \min_{\vz\in\vx_0+\vsK_n(\oiA,\vr_0)}\nrm{\vb-\oiA\vz}
        = \min_{p\in\Polys_{n,0}} \nrm{p(\oiA)\vr_0}.
        \label{eq:back:gmres:min}
      \end{equation}
    \item The residual in iteration $n\leq d$ is given by
      \[
        \vb - \oiA\vx_n = p_n^\MR(\oiA)\vr_0
      \]
      with $p_n^\MR\in\Polys_{n,0}$ and
      \[
        p_n^\MR(\lambda)
        = \prod_{i\in J_n}\left(1-\frac{\lambda}{\theta_i^{(n)}}\right),
      \]
      where $\theta_1^{(n)},\ldots,\theta_n^{(n)}\in\C\cup\{\infty\}$ are the harmonic Ritz values of
      $\oiA$ with respect to $\vsK_n(\oiA,\vr_0)$ and
      $J_n=\{i\in\{1,\ldots,n\}~|~\theta_i^{(n)}\neq\infty\}$ (cf.\
      section~\ref{sec:back:ritz}).
    \item The following statements are equivalent for an iteration $1\leq n\leq d$:
      \begin{enumerate}
        \item GMRES stagnates at iteration $n$, i.e., $\vx_n=\vx_{n-1}$.
          \label{thm:back:gmres:eq:stag}
        \item $\ofH_n=\ip{\vtV_n}{\oiA\vtV_n}$ is singular.
          \label{thm:back:gmres:eq:sing}
        \item There exists an infinite harmonic Ritz value of $\oiA$ with
          respect to the subspace $\vsK_n(\oiA,\vr_0)$, i.e.,
          $\infty\in\{\theta_1^{(n)},\ldots,\theta_n^{(n)}\}$.
          \label{thm:back:gmres:eq:ritz}
        \item The minimal residual polynomial $p_n^\MR\in\Polys_{n,0}$ does not
          have full degree, i.e., $\deg p_n^\MR < n$.
          \label{thm:back:gmres:eq:deg}
      \end{enumerate}
  \end{enumerate}
\end{thm}

\begin{proof}
  The well-definedness of the method and item 1.\ are analogous to
  theorem~\ref{thm:back:cg}. A proof of item 2.\ was given by Freund
  in~\cite{Fre92}. The equivalence of~\ref{thm:back:gmres:eq:stag}
  and~\ref{thm:back:gmres:eq:sing} was shown by Brown in~\cite{Bro91}. The
  equivalence of~\ref{thm:back:gmres:eq:sing} and~\ref{thm:back:gmres:eq:ritz}
  follows from the fact that the existence of an infinite harmonic Ritz value is
  equivalent to the existence of a zero eigenvalue of
  $(\underline{\ofH}_n^\htp\underline{\ofH}_n)^\inv\ofH_n^\htp$, cf.\
  section~\ref{sec:back:ritz}. The last equivalence in item 3.\ immediately
  follows from item 2.
\end{proof}

A meaningful a priori characterization of the convergence behavior of the GMRES
method for general linear operators is even harder than in the case of the CG
method.  The remaining part of this subsection gives a brief overview of common
approaches for describing the convergence behavior of the GMRES method.

\begin{thm}[GMRES spectral bound]
  \label{thm:back:gmres:spectral}
  Let $N\DEF\dim\vsH<\infty$ and let $\oiA\vx=\vb$ be a consistent linear system
  with $\oiA\in\vsL(\vsH)$ and $\vb\in\vsH$. Furthermore, let
  $\oiA=\vtS\ofJ\vtS^\inv$ be a Jordan decomposition with $\vtS\in\vsH^N$ and
  $\ofJ=\diag(\ofJ_1,\ldots,\ofJ_k)$, where
  $\ofJ_i=\ofJ_i(\lambda_i)\in\C^{N_i,N_i}$ are the Jordan blocks for
  $i\in\{1,\ldots,k\}$. The eigenvalues are assumed to be ordered such that
  $\lambda_1,\ldots,\lambda_l\neq 0$ and $\lambda_{l+1}=\ldots=\lambda_k=0$ and
  $m\DEF\max\{N_{l+1},\ldots,N_k\}$ denotes the index of the zero eigenvalue.
  Furthermore, assume that $\vx_0\in\vsH$ is an initial guess such that
  $0\neq\vr_0=\vb-\oiA\vx_0\in\vsR(\oiA^m)$.

  Then for $n\leq d$, the residuals $\vr_n$ of the GMRES method satisfy
  \begin{equation}
    \frac{\nrm{\vr_n}}{\nrm{\vr_0}} \leq \kappa(\vtS)
    \min_{p\in\Polys_{n,0}}\max_{i\in\{1,\ldots,l\}}\nrm[2]{p(\ofJ_i)}.
    \label{eq:back:gmres:spectral}
  \end{equation}
\end{thm}

\begin{proof}
  The proof for the case where $\oiA$ is nonsingular has been given, e.g., by
  Jia in~\cite[theorem~1]{Jia98}. Here, the proof is given with a minor
  modification that takes into account the possibility of a singular operator
  $\oiA$.  If $\vr_0\in\vsR(\oiA^m)$, then
  $\vr_0=\vtS\mat{\ofI_M&\\&0}\vtS^\inv\vr_0$, where $M=\sum_{i=1}^l N_i$. Thus
  the following holds for a $p\in\Polys_n$
  \[
    p(\oiA)\vr_0
    = \vtS p(\ofJ) \vtS^\inv \vr_0
    = \vtS \diag(p(\ofJ_1),\dots,p(\ofJ_l),0_{N-M}) \vtS^\inv\vr_0
  \]
  and with equation~\eqref{eq:back:gmres:min} therefore
  \[
    \nrm{\vr_n}
    = \min_{p\in\Polys_{n,0}}\nrm{p(\oiA)\vr_0}
    \leq \nrm{\vtS}\nrm{\vtS^\inv}
      \min_{p\in\Polys_{n,0}}\max_{i\in\{1,\ldots,l\}}\nrm[2]{p(\ofJ_i)}
      \nrm{\vr_0}.
  \]
\end{proof}

For a diagonalizable matrix $\oiA=\vtS \ofD\vtS^\inv\in\C^{N,N}$ with
$\ofD=\diag(\lambda_1,\ldots,\lambda_N)$,
equation~\eqref{eq:back:gmres:spectral} reads
\begin{equation}
  \frac{\nrm{\vr_n}}{\nrm{\vr_0}}
  \leq \kappa(\vtS)
  \min_{p\in\Polys_{n,0}}\max_{i\in\{1,\ldots,l\}}|p(\lambda_i)|,
  \label{eq:back:gmres:spectral-diag}
\end{equation}
where the eigenvalues are ordered and the index $l$ is defined as in
theorem~\ref{thm:back:gmres:spectral}. Because of this bound it is tempting to
say that the convergence behavior of the GMRES method is determined by the
eigenvalues of the operator $\oiA$. However, this would be as misleading as
calling the $\kappa$-bound~\eqref{eq:back:cg:kappa} a descriptive bound for the
CG method.  Note that the bound~\eqref{eq:back:gmres:spectral-diag} exhibits the
condition number of the eigenvector matrix $\vtS$ which may render the bound
useless in applications.

Greenbaum, Pt{\'a}k and Strako{\v{s}} showed in~\cite{GrePS96} that the spectrum
alone cannot describe the convergence behavior for the GMRES method for a
general linear operator $\oiA$. Their analysis shows that if any set
$L\subset\C\setminus\{0\}$ with $|L|\leq N$ and any $N$ numbers $\eta_0\geq
\eta_1 \geq \cdots \geq \eta_{N-1}> \eta_N=0$ are given, then there exists a
matrix $\oiA\in\C^{N,N}$ with $\Lambda(\oiA)=L$ and a right hand side
$\vb\in\vsH$ such that GMRES applied to $\oiA\vx=\vb$ with $\vx_0=0$ constructs
residuals $\vr_n$ that satisfy $\nrm{\vr_n}=\eta_n$ for $n\in\{0,\ldots,N\}$.
A parametrization of \emph{all} matrices $\oiA\in\C^{N,N}$ and right hand sides
$\vb\in\vsH$ with the above property was provided by Arioli, Pt{\'a}k and
Strako\v{s} in~\cite{AriPS98}.

Trefethen took a different path in~\cite{Tre90} by using the
\emph{$\epsilon$-pseudospectrum} $\Lambda_\epsilon(\oiA)$ (which was called the
\emph{set of $\epsilon$-approximate eigenvalues of $\oiA$} in~\cite{Tre90}).

\begin{definition}[Pseudospectrum]
  \label{def:back:pseudo}
  For $\dim\vsH<\infty$, $\oiA\in\vsL(\vsH)$ and $\epsilon>0$, the
  \emph{$\epsilon$-pseudo\-spectrum} of $\oiA$ is defined by
  \begin{align*}
    \Lambda_\epsilon(\oiA)
    &= \bigcup_{\substack{\oiE\in\vsL(\vsH)\\\nrm{\oiE}<\epsilon}}
      \Lambda(\oiA+\oiE) \\
    &= \left\{ \lambda\in
      \C~\left|~\nrm{(\lambda\oiI-\oiA)^\inv}>\epsilon^\inv\right.\right\} \\
    &= \left\{ \lambda\in
      \C~\left|~\nrm{(\lambda\oiI-\oiA)\vv}
      <\epsilon~\text{for a}~\vv\in\vsH\right.\right\},
  \end{align*}
  where $\nrm{(\lambda\oiI-\oiA)^\inv}\DEF\infty$ if the resolvent
  $(\lambda\oiI-\oiA)^\inv$ does not exist, i.e., if $\lambda\in\Lambda(\oiA)$.
\end{definition}

A detailed treatment of pseudospectra including a proof of the equivalence of
the above definitions can be found in the monograph of Trefethen and
Embree~\cite{TreE05}.  For $\epsilon>0$, the spectrum is contained in the
$\epsilon$-pseudospectrum, i.e.,
$\Lambda(\oiA)\subset\Lambda_\epsilon(\oiA)$. Note that the pseudospectrum is
defined as an open set in definition~\ref{def:back:pseudo}. If the boundary of
$\Lambda_\epsilon(\oiA)$ is denoted by $\partial\Lambda_\epsilon(\oiA)$ and
$p\in\Polys_{n}$ is a polynomial, then $p(\oiA)$ can be expressed as a Cauchy
integral along the curve $\partial\Lambda_\epsilon(\oiA)$:
\[
  p(\oiA)
  = \frac{1}{2\pi\iu}\int_{\partial\Lambda_\epsilon(\oiA)}
    p(\lambda) (\lambda\oiI-\oiA)^\inv d\lambda.
\]
This equation is used to prove the following theorem, see Nachtigal, Reddy and
Trefethen~\cite{NacRT92}.

\begin{thm}[GMRES pseudospectral bound]
  \label{thm:back:gmres:pseudo}
  Let the assumptions of theorem~\ref{thm:back:gmres:spectral} hold and let
  $\epsilon>0$.

  Then for $n\leq d$, the residuals of the GMRES method satisfy
  \[
    \frac{\nrm{\vr_n}}{\nrm{\vr_0}}
    \leq \frac{ |\partial\Lambda_\epsilon(\oiA_1)|}{2\pi\epsilon}
      \min_{p\in\Polys_{n,0}}
      \sup_{\lambda\in\Lambda_\epsilon(\oiA_1)}|p(\lambda)|,
  \]
  where $\oiA_1\DEF\oiA|_{\vsR(\oiA^m)}$, i.e.
  $\oiA_1
  =\restr{\vtS\diag(\ofJ_1,\ldots,\ofJ_l,0,\ldots,0)\vtS^\inv}{\vsR(\oiA^m)}$,
  and $|\partial\Lambda_\epsilon(\oiA_1)|$ denotes the curve's arc length.
\end{thm}

\begin{proof}
  If $\vr_0\in\vsR(\oiA^m)$, then
  \[
    p(\oiA)\vr_0
    = p(\oiA_1)\vr_0
    = \frac{1}{2\pi\iu}\int_{\partial\Lambda_\epsilon(\oiA_1)}
      p(\lambda)(\lambda\oiI-\oiA_1)^\inv d\lambda~\vr_0
  \]
  holds for any $p\in\Polys_n$. Estimating the norm yields
  \begin{align*}
    \nrm{p(\oiA)\vr_0}
    &\leq \frac{1}{2\pi} \int_{\partial\Lambda_\epsilon(\oiA_1)}
      |p(\lambda)| \nrm{(\lambda\oiI-\oiA_1)^\inv}d\lambda\nrm{\vr_0}
    = \frac{1}{2\pi\epsilon} \int_{\partial\Lambda_\epsilon(\oiA_1)}
      |p(\lambda)| d\lambda \nrm{\vr_0} \\
    &\leq \frac{|\partial\Lambda_\epsilon(\oiA_1)|}{2\pi\epsilon}
      \max_{\lambda\in\partial\Lambda_\epsilon(\oiA_1)}
      |p(\lambda)| \nrm{\vr_0}
    = \frac{|\partial\Lambda_\epsilon(\oiA_1)|}{2\pi\epsilon}
      \sup_{\lambda\in\Lambda_\epsilon(\oiA_1)}
      |p(\lambda)| \nrm{\vr_0},
  \end{align*}
  where the last equality is due to the fact that $p$ is holomorphic and its
  maximum is attained on the boundary of the pseudospectrum
  $\Lambda_\epsilon(\oiA_1)$. The proof is complete with the minimization
  property of GMRES, i.e., equation~\eqref{eq:back:gmres:min}.
\end{proof}

Theorem~\ref{thm:back:gmres:pseudo} does not answer the question of an
appropriate choice of $\epsilon$ and this question turns out to be tricky in
practice. On the one hand, $\epsilon$ should be chosen large to make the
factor $\frac{1}{\epsilon}$ small, but on the other hand it has to be chosen
small enough such that the pseudospectrum $\Lambda_\epsilon(\oiA_1)$ and its
boundary are not too large. In order to obtain a relevant bound, the
pseudospectrum $\Lambda_\epsilon(\oiA_1)$ has to exclude the origin because
$p(0)=1$ for $p\in\Polys_{n,0}$.

Note that theorem~\ref{thm:back:gmres:pseudo} is usually stated without taking
special care of the zero eigenvalues~\cite{Tre90,NacRT92,TreE05}. For a singular
operator $\oiA$, the naive way of taking the boundary of
$\Lambda_\epsilon(\oiA)$ as the curve for the Cauchy integral does not provide a
usable bound because the origin is included. The restriction of the operator to
the invariant subspace $\vsR(\oiA^m)$ that corresponds to the nonzero
eigenvalues can be used because $\vr_0$ is also contained in this subspace by
assumption. Note that $\vr_0\in\vsR(\oiA^m)$ if and only if a solution can be
found in a Krylov subspace, see proposition~\ref{prop:back:invariance}. Another
detail in theorem~\ref{thm:back:gmres:pseudo} should be made clear here because
it might be puzzling at first sight. If the linear operator $\oiA\in\vsL(\vsH)$
is singular, then the operator $\oiA_1$ in theorem~\ref{thm:back:gmres:pseudo}
has to be treated as the \emph{nonsingular} operator $\oiA_1\in\vsL(\vsG)$,
where $\vsG=\vsR(\oiA^m)$. Thus, the perturbations $\oiE$ in the first equation
of the definition of the pseudospectrum (cf.\ definition~\ref{def:back:pseudo})
also have to be elements of $\vsL(\vsG)$, i.e.,
\[
  \Lambda_\epsilon(\oiA_1)
  = \bigcup_{\substack{\oiE\in\vsL(\vsG)\\\nrm{\oiE}<\epsilon}}
    \Lambda(\oiA+\oiE).
\]

The pseudospectral approach reappears in the
analysis of perturbed Krylov subspace methods in section~\ref{sec:rec:per:kry}.

In~\cite{Sta94,Sta97}, Starke generalized and improved a convergence bound for
the GCR method of Elman~\cite{Elm82} for the GMRES method which is based on the
field of values:

\begin{definition}[Field of values]
  \label{def:back:fov}
  The \emph{field of values} of $\oiA\in\vsL(\vsH)$ is defined by
  \[
    W(\oiA)\DEF \{\ip{\vv}{\oiA\vv}~|~\vv\in\vsH,~\nrm{\vv}=1\}.
  \]
  The distance of $W(\oiA)$ to the origin is denoted by
  \[
    \nu(\oiA)\DEF\inf\left\{|w|~\big|~w\in W(\oiA)\right\}.
  \]
\end{definition}

The field of value bound of Starke in~\cite{Sta94,Sta97} is restricted to
matrices with positive-definite symmetric part but it has been shown by Eiermann
and Ernst~\cite{EieE01} that the bound actually holds for any nonsingular
matrix. Here, their result is stated for a possibly singular operator:

\begin{thm}[GMRES field of value bound]
  \label{thm:back:gmres:fov}
  Let the assumptions of theorem~\ref{thm:back:gmres:spectral} hold.

  Then for $n\leq d$, the residuals of the GMRES method satisfy
  \begin{equation}
    \frac{\nrm{\vr_n}}{\nrm{\vr_0}}
    \leq \left(1 - \nu(\oiA_1)\nu(\oiA_1^\inv)\right)^{\frac{n}{2}},
    \label{eq:back:gmres:fov}
  \end{equation}
  where $\oiA_1\DEF\oiA|_{\vsR(\oiA^m)}$ as in
  theorem~\ref{thm:back:gmres:pseudo}.
\end{thm}

\begin{proof}
  The proof is given in~\cite[theorem 6.1 and corollary 6.2]{EieE01} for a
  nonsingular operator $\oiA$. Analogous to the proof of
  theorem~\ref{thm:back:gmres:pseudo}, the result can be generalized to a
  singular operator because $\vr_0\in\vsR(\oiA^m)$.
\end{proof}

Note that the bound~\eqref{eq:back:gmres:fov} is useless if $0\in W(\oiA_1)$,
e.g., if $\oiA$ is indefinite.  Starke showed in~\cite{Sta97} and with Klawonn
in~\cite{KlaS99}, that the field of values can be used to obtain useful GMRES
convergence bounds for preconditioned non-symmetric elliptic problems and saddle
point problems. Benzi and Olshanskii~\cite{BenO11} were able to mathematically
justify the already observed convergence behavior of a preconditioned GMRES
method for the Navier--Stokes problem. Furthermore, Liesen and Tich\'{y} showed
in~\cite{LieT13} that the bound in theorem~\ref{thm:back:gmres:fov} not only
bounds the worst-case GMRES residual norm but also the ideal GMRES
approximation, i.e.,
\[
  \frac{\nrm{\vr_n}}{\nrm{\vr_0}}
  \leq\underbrace
    {\max_{0\neq\vv\in\vsH}\min_{p\in\Polys_{n,0}}
    \frac{\nrm{p(\oiA)\vv}}{\nrm{\vv}}}_{\text{worst-case GMRES}}
  \leq\underbrace
    {\min_{p\in\Polys_{n,0}}
    \nrm{p(\oiA)}}_{\text{ideal GMRES}}
  \leq \left(1 - \nu(\oiA)\nu(\oiA^\inv)\right)^{\frac{n}{2}}.
\]
While the worst-case GMRES bound is attainable for at least one right hand side
and initial guess, this does not need to be true for the ideal GMRES bound; see
also the discussion in section~5.7.3 in the book of Liesen and
Strako\v{s}~\cite{LieS13}. The term \emph{ideal GMRES} has been
introduced by Greenbaum and Trefethen in~\cite{GreT94}. More results on the
field of values in the context of Krylov subspace methods can be found
in~\cite{Eie93,Sta97,KlaS99,Ern00,EieE01,BenO11}.


\subsubsection*{Preconditioned GMRES}

Instead of applying GMRES to the linear system $\oiA\vx=\vb$, it is in practice
applied to the linear system
\begin{align}
  \oiM\oiA\vx&=\oiM\vb \label{eq:back:gmres:left}\\
  \text{or}\qquad \oiA\oiM\vy&=\vb\quad\text{with}\quad\vx=\oiM\vy,
  \label{eq:back:gmres:right}
\end{align}
where $\oiM\in\vsL(\vsH)$ is invertible and chosen such that convergence is
faster than when applied to the original linear system. Probably because the
$\kappa$-bound~\eqref{eq:back:cg:kappa} for the CG method is so wide-spread, the
term \emph{preconditioner} was established for the operator $\oiM$ also in the
case of the GMRES method for a general operator $\oiA$. Note that the term
\emph{preconditioner} is highly misleading because the behavior of the GMRES
method is usually not governed by the condition number of the operator.
However, because of the wide-spread use, $\oiM$ is also called a
preconditioner here and
equations~\eqref{eq:back:gmres:left}--\eqref{eq:back:gmres:right} are called
the left and right preconditioned linear system. Unlike the CG method, the
preconditioner can be any invertible operator in the GMRES method and the inner
product is not required to be adapted to the preconditioner.


\subsection{MINRES method}
\label{sec:back:mr:minres}

Let the operator $\oiA$ of the linear system~\eqref{eq:back:ls} be self-adjoint
in this subsection. In this case, the Arnoldi algorithm in
line~\ref{alg:back:gmres:arnoldi} of the GMRES algorithm~\ref{alg:back:gmres}
can be replaced by the Lanczos algorithm.  Paige and Saunders showed
in~\cite{PaiS75}, that not only the Lanczos algorithm benefits from a three-term
recurrence, but also that the least squares problem~\eqref{eq:back:gmres-ls} can
be solved by a three-term recurrence.  The resulting MINRES
algorithm~\ref{alg:back:minres} makes use of Givens rotations in order to
maintain a QR factorization of the Hermitian tridiagonal matrix that is produced
by the Lanczos algorithm, cf.\ Elman, Silvester and Wathen~\cite{ElmSW05} and
Fischer~\cite{Fis11}.

\begin{algorithm}[htb]
  \begin{algorithmic}[1]
    \Require Self-adjoint $\oiA\in\vsL(\vsH)$, right hand side $\vb\in\vsR(\oiA)$ and
    initial guess $\vx_0\in\vsH$. Maximal
    number of iterations $n_{\max}\in\N$.
    \State $\vv_0 = \vw_0 = \vw_1 = 0$
    \State $s_0=s_1=0$, $c_0=c_1=1$
    \State $\vr_0 = \vb -\oiA\vx_0$, $\eta_0 = \nrm{\vr_0}$, $\vv_1 = \frac{\vr_0}{\eta_0}$
    \For{$n=1,\ldots,n_{\max}$}
      \State $\vz \gets \oiA \vv_n$
      \State $\vz \gets \vz - \delta_n \vv_{n-1}$
      \State $\gamma_n = \ip{\vv_n}{\vz}$
      \State $\vz \gets \vz - \gamma_n \vv_n$
      \State $\delta_{n+1} = \nrm{\vz}$
      \State $\alpha_0 = c_n \gamma_n  - c_{n-1} s_n \delta_n$
      \State $\alpha_1 = \sqrt{\alpha_0^2 + \delta_{n+1}^2}$
      \State $\alpha_2 = s_n\gamma_n + c_{n-1} c_n \delta_n$
      \State $\alpha_3 = s_{n-1} \delta_n$
      \State $c_{n+1} = \alpha_0 / \alpha_1$
      \State $s_{n+1} = \delta_{n+1}/\alpha_1$
      \State $\vw_{n+1} = \frac{1}{\alpha_1}(\vv_n - \alpha_3 \vw_{n-1} -
        \alpha_2 \vw_n)$
      \State $\vx_n = \vx_{n-1} + c_{n+1} \eta_{n-1} \vw_{n+1}$
      \State $\eta_{n} = - s_{n+1} \eta_{n-1}$
      \If{stopping criterion is reached (note that
        $\nrm{\vb-\oiA\vx_n}=|\eta_n|$)}
        \State\Return $\vx_n$
      \EndIf
      \State $\vv_{n+1} = \frac{\vz}{\delta_{n+1}}$
    \EndFor
    \State\Return $\vx_{n_{\max}}$
  \end{algorithmic}
  \caption{MINRES algorithm (based on algorithm 2.4 in \cite{ElmSW05}, see also
  the preconditioned version in algorithm~\ref{alg:back:pminres})}
  \label{alg:back:minres}
\end{algorithm}

Because MINRES is mathematically equivalent to GMRES, all statements from
theorem~\ref{thm:back:gmres} and the convergence bounds for GMRES are
also valid for MINRES\@. Note that condition~\eqref{eq:back:gmres-cond} is
always fulfilled for a self-adjoint operator $\oiA$, see
section~\ref{sec:back:mr:gmres}. Furthermore, if $\oiA$ is self-adjoint and
$N\DEF\dim\vsH<\infty$, then a bound of the form of the
$\kappa$-bound~\eqref{eq:back:cg:kappa} for the CG method can be derived.

\pagebreak[4]

\begin{thm}
  \label{thm:back:minres:bound}
  Let $N\DEF\dim\vsH<\infty$ and let $\oiA\vx=\vb$ be a consistent linear system
  with a self-adjoint and indefinite operator $\oiA\in\vsL(\vsH)$ and
  $\vb\in\vsH$. Furthermore, let the eigenvalues of $\oiA$ be sorted such
  that $\lambda_1\leq\ldots\leq\lambda_s <0=\lambda_{s+1}=\ldots=\lambda_{t-1}
  <\lambda_t\leq\ldots\leq\lambda_N$ holds. Assume that an initial guess
  $\vx_0\in\vsH$ is given and the corresponding initial residual
  $\vr_0=\vb-\oiA\vx_0$ is of grade $d=d(\oiA,\vr_0)$.

  Then for $n\leq d$, the residuals $\vr_n$ of the MINRES/GMRES method satisfy
  \begin{equation}
    \frac{\nrm{\vr_n}}{\nrm{\vr_0}}
    \leq 2 \left(
    \frac{ \sqrt{|\lambda_1\lambda_N|} - \sqrt{|\lambda_s\lambda_t|}}
    { \sqrt{|\lambda_1\lambda_N|} + \sqrt{|\lambda_s\lambda_t|}}
    \right)^{\left[\frac{n}{2}\right]},
    \label{eq:back:minres:bound}
  \end{equation}
  where $\left[\frac{n}{2}\right]$ is the integer part of $\frac{n}{2}$.
\end{thm}

\begin{proof}
  The proof was given in the monograph of Greenbaum~\cite{Gre97} for a
  nonsingular operator. Analogous to the previous proofs for the GMRES method,
  it can be generalized to the singular case by using the restriction
  $\oiA|_{\vsR(\oiA)}$. Note that the index $m$ of the zero eigenvalue of $\oiA$
  is at the maximum 1 because $\oiA$ is self-adjoint. Therefore, the condition
  $\vb\in\vsR(\oiA)$ implies $\vr_0\in\vsR(\oiA)$ and guarantees that a solution
  can be found in a Krylov subspace.
\end{proof}

In order to speed up the convergence of the MINRES method, a preconditioner can
be applied. However, the preconditioner is subject to the same restrictions as
in the preconditioned CG method: it has to be self-adjoint and positive
definite and the inner product is also changed by the choice of the
preconditioner. A variant of the preconditioned MINRES algorithm is given in
algorithm~\ref{alg:back:pminres}, cf.~\cite{ElmSW05}.

\begin{algorithm}[htb]
  \begin{algorithmic}[1]
    \Require Self-adjoint $\oiA\in\vsL(\vsH)$, self-adjoint and
    positive-definite preconditioner $\oiM\in\vsL(\vsH)$, right hand side
    $\vb\in\vsR(\oiA)$ and initial guess $\vx_0\in\vsH$. Maximal number of
    iterations $n_{\max}\in\N$.
    \State $\vv_0 = \vw_0 = \vw_1 = 0$
    \State $s_0=s_1=0$, $c_0=c_1=1$
    \State $\vz \gets \vb -\oiA\vx_0$, $\vr_0=\oiM\vz$,
      $\eta_0 = \sqrt{\ip{\vz}{\vr_0}}$, $\vz_1 = \frac{\vz}{\eta_0}$,
      $\vv_1 = \frac{\vr_0}{\eta_0}$
    \For{$n=1,\ldots,n_{\max}$}
      \State $\vz \gets \oiA \vv_n$
      \State $\vz \gets \vz - \delta_n \vz_{n-1}$
      \State $\gamma_n = \ip{\vv_n}{\vz}$
      \State $\vz \gets \vz - \gamma_n \vz_n$
      \State $\vv \gets \oiM\vz$
      \State $\delta_{n+1} = \sqrt{\ip{\vv}{\vz}}$
      \State $\alpha_0 = c_n \gamma_n  - c_{n-1} s_n \delta_n$
      \State $\alpha_1 = \sqrt{\alpha_0^2 + \delta_{n+1}^2}$
      \State $\alpha_2 = s_n\gamma_n + c_{n-1} c_n \delta_n$
      \State $\alpha_3 = s_{n-1} \delta_n$
      \State $c_{n+1} = \alpha_0 / \alpha_1$
      \State $s_{n+1} = \delta_{n+1}/\alpha_1$
      \State $\vw_{n+1} = \frac{1}{\alpha_1}(\vv_n - \alpha_3 \vw_{n-1} -
        \alpha_2 \vw_n)$
      \State $\vx_n = \vx_{n-1} + c_{n+1} \eta_{n-1} \vw_{n+1}$
      \State $\eta_{n} = - s_{n+1} \eta_{n-1}$
      \If{stopping criterion is reached (note that
        $\nrm{\vb-\oiA\vx_n}=|\eta_n|$)}
        \State\Return $\vx_n$
      \EndIf
      \State $\vz_{n+1} = \frac{\vz}{\delta_{n+1}}$
      \State $\vv_{n+1} = \frac{\vv}{\delta_{n+1}}$
    \EndFor
    \State\Return $\vx_{n_{\max}}$
  \end{algorithmic}
  \caption{Preconditioned MINRES algorithm (based on algorithm~6.1 in
    \cite{ElmSW05}). Implemented as \lstinline{krypy.linsys.Minres} in
    \cite{krypy}.}
  \label{alg:back:pminres}
\end{algorithm}


\section{Round-off errors}
\label{sec:back:round-off}

Round-off errors unavoidably affect the results of all variants of the Arnoldi
and Lanczos algorithms and consequently affect all methods that rely on these
algorithms. Observable effects of round-off errors in Krylov subspace methods
for linear systems are, e.g., a limited maximal attainable accuracy way above
machine precision, a delay of convergence or even misconvergence. In general,
the round-off properties heavily depend on the actual algorithm that is used.
This section gives a brief overview on the difficulties that round-off errors
can pose and states some well-known results for $N\DEF\dim\vsH<\infty$ and the
Euclidean case, i.e.,  $\ipdots=\ipdots[2]$. Throughout this section, the unit
round-off is denoted by $\varepsilon$ and $C$ denotes a positive constant that
is independent of the operator $\oiA$, the initial vector $\vv$, the dimension
$N$ and the Arnoldi/Lanczos step $n$. Two questions regarding the
\emph{computed} Arnoldi or Lanczos basis $\vtV_{n+1}$ and Hessenberg matrix
$\underline{\ofH}$ are of major interest:
\begin{itemize}
  \item To what extent is the Arnoldi recurrence fulfilled, e.g., how large is the
    Arnoldi residual $\eta\DEF\nrm{\oiA\vtV_n - \vtV_{n+1}\underline{\ofH}}$?
  \item How far is $\vtV_n$ from orthonormality, e.g., how large is
    $\zeta\DEF\nrm[2]{\ofI_n - \ip{\vtV_n}{\vtV_n}}$?
\end{itemize}

Two variants of the Arnoldi algorithm are commonly used: the modified
Gram--Schmidt variant (cf.\ algorithm~\ref{alg:back:arn}) and the Householder
variant~\cite{Wal88,Saa03}. In the Euclidean case, i.e., $\ipdots=\ipdots[2]$,
both variants fulfill \cite[inequality~(2.3)]{DrkGRS95}
\[
  \eta\leq C n N^\frac{3}{2} \varepsilon \nrm{\oiA}.
\]

According to inequality~(2.5) in~\cite{DrkGRS95}, the loss of orthogonality in
the modified Gram--Schmidt Arnoldi algorithm can be bounded by
\[
  \zeta\leq C n^2 N \varepsilon \kappa\left([\vv_1,\oiA\vtV_n]\right)
\]
if $n N^2 \varepsilon \kappa\left([\vv_1,\oiA\vtV_n]\right)\ll 1$. Here,
$\kappa(\vtX)$ is the condition number of a full rank $\vtX\in\vsH^l$,
which is defined as the ratio of its largest singular value $\sigma_1(\vtX)$ and
smallest singular value $\sigma_l(\vtX)$, i.e.
$\kappa(\vtX)\DEF\frac{\sigma_1(\vtX)}{\sigma_l(\vtX)}$.

The loss of orthogonality in the Householder Arnoldi algorithm can be bounded
independently of the operator $\oiA$ (cf.\ inequality~2.4 in \cite{DrkGRS95}):
\[
  \zeta\leq C n^\frac{3}{2} N \varepsilon.
\]
The operator-independent orthogonality makes the Householder variant very
attractive. A forward error analysis of the GMRES algorithm with the Householder
Arnoldi algorithm can be found in the work of Arioli and Fassino~\cite{AriF96}.

For GMRES with modified Gram--Schmidt orthogonalization, it was
shown by Greenbaum, Rozlo{\v{z}}n{\'{\i}}k and Strako{\v{s}} in~\cite{GreRS97}
that a severe loss of orthogonality occurs only after the residual norm has
almost reached the maximum attainable accuracy. In \cite{PaiRS06}, Paige,
Rozlo{\v{z}}n{\'{\i}}k and Strako{\v{s}} showed that GMRES with modified
Gram--Schmidt orthogonalization computes a backward stable solution for linear
systems $\oiA\vx=\vb$ with ``sufficiently nonsingular $\oiA$''. These works
justified the widespread use of the modified Gram--Schmidt orthogonalization
in GMRES for a wide range of problems.

In cases where the modified Gram--Schmidt Arnoldi algorithm suffers from
severe loss of orthogonality and the Householder method is not
applicable, the orthogonalization can be iterated, i.e., the block of the
\emph{for}-loop in lines~\ref{alg:back:arn:for}--\ref{alg:back:arn:rof} of
algorithm~\ref{alg:back:arn} is repeated. Regarding the number of required
reorthogonalizations, Kahan coined the phrase ``twice is enough'', which was
made precise in later articles, see~\cite{GirL02,GirLR05}. It should be noted,
that one additional iteration of orthogonalization also cures the poor round-off
properties of the classical Gram--Schmidt variant if the initial set of vectors
are numerically nonsingular, see~\cite{GirLR05}. The classical Gram--Schmidt
variant is not considered here but can be beneficial in parallel algorithms. The
(iterated) Gram--Schmidt and Householder variants are implemented
in~\cite{krypy} as \lstinline{krypy.utils.Arnoldi}.

In the Lanczos algorithm, the sensitivity to round-off errors is even worse due
to the fact that the orthogonalization is only performed against the last two
Lanczos vectors. The consequences of round-off errors in the Lanczos algorithm
have already been observed and studied in early works on Krylov subspace
methods, e.g., by Lanczos~\cite{Lan50,Lan52}, Hestenes and Stiefel~\cite{HesS52}
and Wilkinson~\cite{Wil65}. By this time, in order to overcome the gradual loss
of orthogonality, a common strategy was to reorthogonalize a new Lanczos vector
against all previously computed Lanczos vectors, thus essentially carrying out a
full Gram--Schmidt orthogonalization. With regard to this strategy, Parlett
writes from the perspective of 1994 in~\cite{Par94}:
\begin{quote}
  It is not disrespectful to say that Lanczos himself, and J. H. Wilkinson, the
  leading expert in matrix computations from 1960--1984, both panicked at this
  phenomenon. Each of them insisted on doing what we now call full
  reorthogonalization.
\end{quote}
In the meantime, Paige showed in his PhD thesis~\cite{Pai71} from 1971, that the
loss of orthogonality of the Lanczos basis coincides with the convergence of
certain Ritz values and Ritz vectors (see section~\ref{sec:back:ritz} for the
definition of Ritz values and Ritz vectors). This observation led to more
efficient orthogonalization strategies. In 1979, Parlett and Scott~\cite{ParS79}
proposed the \emph{selective orthogonalization} which consists of the
orthogonalization of each new Lanczos vector against the last two Lanczos
vectors and all previously converged Ritz vectors and those that are about to
converge. In 1984, Simon~\cite{Sim84} introduced the \emph{partial
orthogonalization} where the orthogonalization is performed against the last two
and some of the previous Lanczos vectors, depending on a recurrence describing
the loss of orthogonality.

In~\cite{SleVM00} the impact of certain round-off errors on the relative
residual was analyzed for an unpreconditioned MINRES variant. An upper bound on
the difference between the exact arithmetic residual $\vr_n$ and the finite
precision residual
$\widehat{\vr}_n$ was given~\cite[formula (26)]{SleVM00}
\[
  \frac{\nrm{\vr_n - \widehat{\vr}_n}}{\nrm{\vb}}
  \leq \varepsilon \left( 3 \sqrt{3n} \kappa_2(\oiA)^2 + n \sqrt{n}
  \kappa_2(\oiA)\right).
\]
The corresponding bound for GMRES~\cite[formula (17)]{SleVM00} only involves a
factor of $\kappa_2(\oiA)$ instead of its square. The numerical results
in~\cite{SleVM00} also indicate that the maximal attainable accuracy of MINRES
is worse than the one of GMRES. Thus, if very high accuracy is required, the
GMRES algorithm should be used, preferably with Householder orthogonalization.

The analysis of round-off errors in Krylov subspace methods is a wide field of
research with an overwhelming amount of articles. The monograph of
Meurant~\cite{Meu06} provides an in-depth analysis of the Lanczos and CG
methods and their algorithmic realizations in exact and finite precision
arithmetic. A discussion of the finite precision properties of the CG and GMRES
algorithms and pointers to further literature can be found in the monograph of
Liesen and Strako\v{s}~\cite{LieS13}.

In the next chapter, projections are used extensively in the context of
deflated Krylov subspace methods. For orthogonal projections, the implementation
can be realized with (iterated) Gram--Schmidt variants whose properties with
respect to round-off errors are well understood. In 2011, Stewart~\cite{Ste11}
complemented the literature with a thorough round-off error analysis of oblique
projections. For the Euclidean inner product on $\vsH=\C^N$ and two
$n$-dimensional subspaces $\vsV,\vsW\subseteq\vsH$, the analysis in~\cite{Ste11}
suggests the XQRY-form of the projection $\oiP=\oiP_{\vsV,\vsW^\perp}$, where
the application to a vector $\vz\in\C^N$ is carried out by a right-to-left
evaluation of
\begin{equation}
  \ofV\ofR^\inv\ofQ^\htp\ofW^\htp\vz.
  \label{eq:back:round:proj}
\end{equation}
Here, $\ofV,\ofW\in\C^{N,n}$ are such that $\vsV=\Span{\ofV}$,
$\vsW=\Span{\ofW}$, $\ofV^\htp\ofV=\ofW^\htp\ofW=\ofI_n$ and
$\ofQ\ofR=\ofW^\htp\ofV$ is a QR-decomposition, i.e., $\ofQ,\ofR\in\C^{n,n}$
with $\ofQ^\htp\ofQ=\ofI_n$ and $\ofR$ upper triangular. If $\tilde{\oiP}\vz$
denotes the computed application of the projection~\eqref{eq:back:round:proj} in
finite precision, then~\cite[corollary~5.2]{Ste11} shows that
\[
  \frac{\nrm[2]{\tilde{\oiP}\vz-\oiP\vz}}{\nrm[2]{\oiP\vz}}
  \leq \left(1 + \nrm[2]{\oiP} +
    \frac{\nrm[2]{\oiP}\nrm[2]{\vz}}{\nrm[2]{\oiP\vz}}\right)
    C\varepsilon + O(\varepsilon^2).
\]
As Stewart points out, the appearance of $\nrm[2]{\oiP}$ is disturbing but on
the other hand it is not surprising since
$\nrm[2]{\oiP}=\frac{1}{\cos\theta_{\max}(\vsV,\vsW)}$ can be seen as a
measure for the departure from orthogonality (or conditioning) of the range
$\vsV$ and null space $\vsW^\perp$ of $\oiP$. Another interesting result
from~\cite{Ste11} is that care has to be taken when applying the complementary
projection $\oiP_{\vsW^\perp,\vsV}=\id-\oiP_{\vsV,\vsW^\perp}$.
In~\cite[section~7]{Ste11}, it is shown that the rule of thumb ``twice is
enough'' also holds for oblique projections, i.e., that the application of
$\oiP_{\vsW^\perp,\vsV}=\id-\oiP_{\vsV,\vsW^\perp}$ to a vector $\vz$ should be
carried out twice with the XQRY-form of $\oiP_{\vsV,\vsW^\perp}$ in
equation~\eqref{eq:back:round:proj}. The proposed algorithms from~\cite{Ste11}
are implemented in~\cite{krypy} as \lstinline{krypy.utils.Projection}.

Also the computation of small angles between subspaces turns out to be affected
severely by round-off errors. The naive algorithm via the $\arccos$ of the
singular values of $\ip{\vtV}{\vtW}$ for orthonormal bases $\vtV\in\vsH^n$ and
$\vtW\in\vsH^m$ is not able to accurately compute angles smaller than
$\sqrt{\varepsilon}$. In~\cite{KnyA02}, Knyazev and Argentati showed how small
angles can be computed accurately. Their algorithm is implemented
in~\cite{krypy} as \lstinline{krypy.utils.angles}.


\chapter{Recycling for sequences of linear systems}
\label{ch:rec}

The goal of this thesis is to explore possibilities to improve the convergence
behavior of Krylov subspace methods in situations where a sequence of linear
systems has to be solved. Let
\begin{equation}
  \oiA^{(i)}\vx^{(i)}=\vb^{(i)},\quad i\in\{1,\dots,M\},
  \label{eq:rec:seq}
\end{equation}
be a sequence of consistent linear systems with linear operators
$\oiA^{(i)}\in\vsL(\vsH)$ and right hand sides $\vb^{(i)}\in\vsH$. It is assumed
that the linear systems can only be solved subsequently and not all at once in
parallel.  This is, e.g., the case in many practical applications where the
operator $\oiA^{(i)}$ and the right
hand side $\vb^{(i)}$ depend on the solution $\vx^{(i-1)}$ of the last linear
system.  Often, the operators and right hand sides in the sequence are not
random but subsequent linear operators and right hand sides are ``close'' to
each other, i.e., $\oiA^{(i-1)}\approx\oiA^{(i)}$ and
$\vb^{(i-1)}\approx\vb^{(i)}$. Each application has its own meaning of the
symbol ``$\approx$'' and the influence of differences between subsequent
operators and right hand sides is treated in more detail in
sections~\ref{sec:rec:per} and \ref{sec:rec:sel} of this chapter.

Such a sequence of linear systems arises in a wide range of applications, e.g.,
in
\begin{itemize}
  \item implicit time-stepping schemes for time-dependent partial differential
    equations, cf.~\cite{Meu01,Ber04,BirDMT08},
  \item optimization algorithms, where a problem has to be solved for a range of
    predefined or iteratively computed parameters,
    cf.~\cite{WanSP07,MehS11},
  \item nonlinear equations, where several iterations of Newton's method have to
    be performed, cf.~\cite{Kel03,Deu11,GauS13,pynosh} and chapter~\ref{ch:nls}.
\end{itemize}

The basic idea of \emph{recycling} in the context of Krylov subspace methods for
sequences of linear systems is to re-use information that has been computed in
the solution process of the linear systems $1,\dots,i-1$ in order to speed up
the solution process for the $i$-th linear system.

An effective recycling strategy for solving the sequence~\eqref{eq:rec:seq}
addresses the following two questions:

\begin{enumerate}
  \item How can external data be incorporated into a Krylov subspace method in
    order to influence its convergence behavior?
    \label{rec:how}

  \item Which data from previously solved
    linear systems results in the best overall performance when it is
    incorporated into a Krylov subspace method for the next linear system?
    \label{rec:which}
\end{enumerate}

An overview of some well-known approaches that address the
first question is provided in section~\ref{sec:rec:strat} for the CG, GMRES
and MINRES methods, see sections~\ref{sec:back:cg} and \ref{sec:back:mr} for basic
properties and algorithms of these methods. In section~\ref{sec:rec:def},
deflated Krylov subspace methods are discussed and analyzed for the purpose of
incorporating external data. Furthermore, the close relationship between
deflated and augmented Krylov subspace methods is analyzed.

The second question is dealt with in section~\ref{sec:rec:sel} which relies on
the perturbation results from section~\ref{sec:rec:per}.

Since this chapter deals with practical mathematical strategies for actual
computations, it is assumed that $\vsH$ is finite-dimensional, i.e.,
$N\DEF\dim\vsH<\infty$.


\section{Strategies}
\label{sec:rec:strat}

Of course, the question of how to accelerate Krylov subspace methods with
external data is not new. This section aims at giving a brief overview of
strategies that have been proposed and used in the literature.

In the context of sequences of linear systems, the Krylov subspaces that were
constructed for previous linear systems potentially contain valuable information
for the solution of the next linear system. A key question thus is: how can
subspaces be used to influence the convergence behavior of a Krylov
subspace method? This section omits the notational overhead of sequences and it
is assumed that a single consistent linear system
\begin{equation}
  \oiA\vx=\vb
  \label{eq:rec:ls}
\end{equation}
is given with a linear operator $\oiA\in\vsL(\vsH)$ and right hand side
$\vb\in\vsH$.

In order to further simplify the notation in this chapter, the following
projections are defined for a given $\oiA\in\vsL(\vsH)$ and $\vtX\in\vsH^n$ such
that $\vsX=\Span{\vtX}$ and $\oiA\vsX$ are $n$-dimensional.
\begin{enumerate}
  \item If $\theta_{\max}(\vsX,\oiA\vsX)<\frac{\pi}{2}$, then define
    \begin{align}
      \oiP_\vsX^\CG &\DEF \oiP_{\vsX,(\oiA^\adj\vsX)^\perp},
      \qquad \text{i.e.}\quad
      \oiP_\vsX^\CG\vx = \vtX\ip{\vtX}{\oiA\vtX}^\inv\ip{\vtX}{\oiA\vx},
      \label{eq:proj-cg}\\
      \text{and}\quad
      \oiQ_\vsX^\CG &\DEF \oiP_{\vsX^\perp,\oiA\vsX},
      \qquad \text{i.e.}\quad
      \oiQ_\vsX^\CG\vx = \vx - \oiA\vtX\ip{\vtX}{\oiA\vtX}^\inv\ip{\vtX}{\vx}.
      \label{eq:proj-cg-res}
    \end{align}
    If $\oiA$ is self-adjoint and positive semidefinite, then
    $\oiP_\vsX^\CG$ is the orthogonal projection on $\vsX$ with respect to
    the (possibly semidefinite) inner product $\ipdots[\oiA]$.
  \item For a general $\oiA$
    \begin{equation}
      \oiP_\vsX^\MR \DEF \oiP_{\vsX,(\oiA^\adj\oiA\vsX)^\perp},
      \qquad\text{i.e.}\quad
      \oiP_\vsX^\MR\vx =
      \vtX\ip{\oiA\vtX}{\oiA\vtX}^\inv
      \ip{\oiA\vtX}{\oiA\vx}
      \label{eq:proj-mr}
    \end{equation}
    is the orthogonal projection on $\vsX$ with respect to the (possibly
    semidefinite) inner product $\ipdots[\oiA^\adj\oiA]$.
    Furthermore, the following projection is defined:
    \begin{align}
      \oiQ_\vsX^\MR \DEF \oiP_{(\oiA\vsX)^\perp},
      \qquad \text{i.e.}\quad
      \oiQ_\vsX^\MR\vx = \vx - \oiA\vtX\ip{\oiA\vtX}{\oiA\vtX}^\inv\ip{\oiA\vtX}{\vx}.
      \label{eq:proj-mr-res}
    \end{align}
\end{enumerate}
The complementary projections are denoted by
$\oiP_{\vsX^\perp}^\CG=\id - \oiP_\vsX^\CG$ and
$\oiP_{\vsX^\perp}^\MR=\id - \oiP_\vsX^\MR$.

\begin{lemma}
  \label{lem:proj-commute}
  If the above projections are well defined then the following statements hold:
  \begin{enumerate}
    \item $\oiA\oiP_{\vsX^\perp}^\CG = \oiQ_\vsX^\CG \oiA$.
    \item $\oiA\oiP_{\vsX^\perp}^\MR = \oiQ_\vsX^\MR \oiA$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  See lemma~\ref{lem:projcommute}.
\end{proof}

For $1\leq n\leq d(\oiA,\vr_0)$, the projections $\oiP_\vsX^\CG$ and $\oiP_\vsX^\MR$ can
be used to represent the iterates $\vx_n^\CG$ of the CG method and the iterates
$\vx_n^\MR$ of the GMRES and MINRES methods, cf.\
sections~\ref{sec:back:galerkin}, \ref{sec:back:cg} and \ref{sec:back:mr}:
\begin{align*}
  \vx_n^\CG &= \vx_0 + \oiP_{\vsK_n(\oiA,\vr_0)}^\CG (\vx - \vx_0)\\
  \text{and}\qquad
  \vx_n^\MR &= \vx_0 + \oiP_{\vsK_n(\oiA,\vr_0)}^\MR (\vx - \vx_0).
\end{align*}
The corresponding residuals are
\begin{align*}
  \vr_n^\CG
    &= \vb -\oiA\vx_n^\CG
    = \oiA\left(\id - \oiP_{\vsK_n(\oiA,\vr_0)}^\CG\right) (\vx-\vx_0)
    = \oiQ_{\vsK_n(\oiA,\vr_0)}^\CG \vr_0 \\
  \text{and}\qquad
  \vr_n^\MR
    &= \vb -\oiA\vx_n^\MR
    = \oiA\left(\id - \oiP_{\vsK_n(\oiA,\vr_0)}^\MR\right) (\vx-\vx_0)
    = \oiQ_{\vsK_n(\oiA,\vr_0)}^\MR \vr_0.
\end{align*}

The following subsections give a brief overview on well-known techniques that
allow to incorporate external data in Krylov subspace methods by adapting the
initial guess, modifying the preconditioner or augmenting the search space.
Section~\ref{sec:rec:def} and subsequent sections concentrate on
deflated Krylov subspace methods.

\subsection{Initial guess}
\label{sec:rec:strat:ini}
The most obvious approach is to construct a ``good'' initial guess
$\vx_0$. Given an $m$-dimensional subspace $\vsU$ that potentially contains
a good approximation of an exact solution $\vx$, the following two
approaches appear to be sensible in the light of the minimization properties of
the CG, GMRES and MINRES methods:
\begin{enumerate}
  \item Choose $\vx_0^\CG\in\vsU$ such that the $\oiA$-norm of the error is
    minimized if $\oiA$ is self-adjoint and positive (semi-)definite:
    \begin{equation}
      \nrm[\oiA]{\vx-\vx_0^\CG}
      = \min_{\vu\in\vsU} \nrm[\oiA]{\vx-\vu}.
      \label{eq:rec:strat:ini:cg}
    \end{equation}
  \item Choose $\vx_0^\MR\in\vsU$ such that the residual norm is minimized:
    \begin{equation}
      \nrm{\vb-\oiA\vx_0^\MR}
      = \min_{\vu\in\vsU} \nrm{\vb-\oiA\vu}
      = \min_{\vz\in\oiA\vsU} \nrm{\vb-\vz}.
      \label{eq:rec:strat:ini:mr}
    \end{equation}
\end{enumerate}

The first option matches the minimization property of the CG method while the
latter one fits the minimization property of the GMRES and MINRES methods.  If
$\vtU\in\vsH^m$ is given with $\vsU=\Span{\vtU}$ and $\vsU\cap\vsN(\oiA)=\{0\}$,
then the optimal initial guesses with respect to $\vsU$
in~\eqref{eq:rec:strat:ini:cg} and \eqref{eq:rec:strat:ini:mr} can be
represented with the help of corollary~\ref{cor:back:pg-repr} by
\begin{align}
  \vx_0^\CG
    &= \oiP_\vsU^\CG\vx
    = \vtU \ip{\vtU}{\oiA\vtU}^\inv\ip{\vtU}{\vb}
  \label{eq:rec:ini-cg} \\
  \text{and}\qquad
  \vx_0^\MR
    &= \oiP_\vsU^\MR\vx
    = \vtU\ip{\oiA\vtU}{\oiA\vtU}^\inv\ip{\oiA\vtU}{\vb}.
  \label{eq:rec:ini-mr}
\end{align}

The initial guess $\vx_0^\CG$ has been used by Fischer in~\cite{Fis98} for a
sequence of linear systems with a fixed self-adjoint and positive-definite
operator $\oiA^{(1)}=\ldots=\oiA^{(M)}$. In~\cite{Fis98}, the used recycle data
are approximate solutions of the previously solved linear systems.

If a nonzero initial guess $\vx_0\in\vsH$ is given, then the computation of the
initial guess $\vx_0^\MR$ with $\vtU=\vx_0$ is known as the \emph{Heged\"{u}s
trick}~\cite{LieS13}.

If an $m$-dimensional subspace $\vsU=\Span{\vu_1,\ldots,\vu_m}$ and a nonzero
initial guess $\vx_0\notin\vsU$ is given, then the optimal initial
guesses~\eqref{eq:rec:ini-cg}--\eqref{eq:rec:ini-mr} with respect to $\vsU$ can
be constructed with $\widehat{\vtU}\DEF[\vx_0,\vu_1,\ldots,\vu_m]$ instead of
$\vtU$.

In order to implement the computation of $\vx_0^\CG$, a QR decomposition of
$\vtU=\vtQ\ofR$ with $\vtQ\in\vsH^m$, $\ip[\oiA]{\vtQ}{\vtQ}=\ofI_m$ and upper
triangular and invertible $\ofR\in\C^{m,m}$ can be computed. A straightforward
computation shows that the initial guess~\eqref{eq:rec:ini-cg} can then be
obtained by
\[
  \vx_0^\CG = \vtQ\ip{\vtQ}{\vb}.
\]
The computational cost of this implementation is the cost of orthogonalizing
$\vtU$ in the inner product $\ipdots[\oiA]$ (which includes $m$ applications of the
operator $\oiA$), and $m$ inner products and vector updates.

For the computation of $\vx_0^\MR$, a QR decomposition of $\oiA\vtU=\vtQ\ofR$
with $\vtQ\in\vsH^m$, $\ip{\vtQ}{\vtQ}=\ofI_m$ and upper triangular and
invertible $\ofR\in\C^{m,m}$ can be performed such that the optimal initial
guess with respect to $\vsU$ becomes
\[
  \vx_0^\MR = \vtU \ofR^\inv \ip{\vtQ}{\vb}.
\]
The computational cost for this implementation is the cost of $m$ applications
of the operator $\oiA$, the cost of the QR decomposition of $\oiA\vtU$, the cost
of $m$ inner products and vector updates and the application of $\ofR^\inv$
(backward substitution).

For $m\ll N$, the complexity for the computation of $\vx_0^\CG$ and $\vx_0^\MR$
is thus dominated by the $m$ applications of the operator $\oiA$ in practice.
Note that if GMRES is used with a singular operator $\oiA$, the solvability
condition $\vr_0=\vb-\oiA\vx_0^\MR\in\vsR(\oiA^m)$ (with $m$ being the index of
the zero eigenvalue of $\oiA$) may be violated by using the initial
guesses~\eqref{eq:rec:ini-cg}--\eqref{eq:rec:ini-mr}, cf.\
theorem~\ref{thm:back:gmres} and proposition~\ref{prop:back:invariance}.

The use of a nonzero initial guess can reduce the initial residual in some
cases where a problem-related initial guess is at hand. However, it will not
change the overall convergence behavior of the Krylov subspace method in general
-- unless the initial guess is of a very special form, e.g., such that the
corresponding initial residual has only a few nonzero entries in a
representation in terms of an eigenvector basis.

In most cases, however, the adaption of the initial guess alone does not change
the qualitative convergence behavior. In practice, the adaption is roughly as
costly as $m$ iterations of CG, MINRES or GMRES. Therefore, it should only be
used if there is evidence that it reduces the number of iterations to reach a
given tolerance by $m$ or more.

\subsection{Preconditioning}
\label{sec:rec:strat:prec}
Another strategy concerns the use of preconditioners. In practice, Krylov
subspace methods are usually only feasible in combination with a preconditioner.
The choice of the preconditioner is highly problem-dependent and thus there is
no general recipe for the incorporation of external data into preconditioners.

If the operators in a sequence of linear systems are explicitly given as a
matrices, one strategy is to use a (possibly incomplete) factorization of the
matrix of one linear system in the sequence as a preconditioner for subsequent
linear systems. When the performance of the Krylov subspace method degrades due
to the accumulated changes in the matrices, a new factorization can be computed.

Knoll and Keyes~\cite{KnoK04} used this approach in the context of Newton-Krylov
methods (or inexact Newton methods), i.e., Newton's method where the linear
systems with the Jacobian operator are solved up to a specified tolerance with a
Krylov subspace method.

A similar strategy was used by Mehrmann and Schr\"{o}der in~\cite{MehS11} for a
sequence of large scale linear systems in an industrial application. The linear
systems represent a frequency response problem stemming from the optimization
of acoustic fields and are of the form
\begin{equation}
  (-\omega^2 \ofM + \iu \omega \ofD +
  \ofK_\R^{(\omega)}+\iu\ofK_\C)\vx^{(\omega)}=\vb^{(\omega)}
  \label{eq:rec:freq}
\end{equation}
with real symmetric matrices $\ofM,\ofK_\R^{(\omega)},\ofK_\C\in\R^{N,N}$ and a
complex symmetric matrix $\ofD\in\C^{N,N}$. The parameter $\omega$ represents
the frequency and the linear system~\eqref{eq:rec:freq} has to be solved for the
frequency range $\omega\in\{1,2,\ldots,10^3\}$. For small values of $\omega$,
the real symmetric matrix
\[
  \ofP\DEF-\omega^2\ofM + \ofK_\R^{(\omega)}
\]
is factorized with the MUMPS software package~\cite{mumps} into
$\ofP=\ofL\ofD\ofL^\tp$ and used as a preconditioner for a GMRES variant.
A once computed factorization is reused as a preconditioner for subsequent
linear systems until the performance of the Krylov subspace method deteriorates.
The factorization is then recomputed for the matrix of the current linear
system. For large frequencies close to $10^3$, Mehrmann and Schr\"{o}der
observed that the used Krylov subspace method hardly converged. They thus
switched from the Krylov subspace method to the direct solver MUMPS which is
used to perform a $\ofL\ofD\ofL^\tp$ factorization of the entire complex
symmetric matrix in equation~\eqref{eq:rec:freq} for these frequencies.

For specific classes of preconditioners, update formulas for the preconditioner
are known.
Meurant showed in~\cite{Meu01} how a Cholesky-like factorization of the matrix
$\epsilon\ofI+\ofA$ with $\epsilon\in\R_+$ can be obtained as an update of an
already computed incomplete Cholesky factorization of a symmetric M-matrix
$\ofA$. The proposed update schemes have been applied to a finite difference
discretization of the heat equation in~\cite{Meu01}.
Benzi and Bertaccini~\cite{BenB03} showed how an update can be performed for the
perturbed matrix $\epsilon\ofI+\ofA$ with a general symmetric and
positive-definite matrix $\oiA$. In~\cite{Ber04}, Bertaccini allows the
perturbed matrix
to take the form $\ofD+\ofA$, where $\ofD\in\C^{N,N}$ is a diagonal matrix and
$\ofA\in\C^{N,N}$ is Hermitian and positive semidefinite.
In a series of articles, Duintjer Tebbens and
T\r{u}ma~\cite{DuiT07,DuiT08,DuiT10} and their coauthors Birken and
Meister~\cite{BirDMT08} generalized the approaches from~\cite{BenB03,Ber04} and
showed how (incomplete) LU factorizations can be updated. In these works, no
assumptions are made on the structure of the perturbation. In~\cite{DuiT10}, the
updates for an incomplete LU factorization can be used in a matrix-free setting,
i.e., when the matrix $\ofA$ is not explicitly formed but only the matrix-vector
multiplication can be evaluated.
Note that these updating schemes do not carry over data from the solution
process from one linear system to the next but try to construct a preconditioner
update by only analyzing the changes in the matrix.

In the case of geometric multigrid~\cite{Hac85} or algebraic
multigrid~\cite{RugS83} preconditioners, data can often be reused easily in a
sequence of linear systems. If the linear systems result from the discretization
of a partial differential equation and the geometry is fixed throughout the
sequence, then the restriction and prolongation operators in the multigrid
method remain constant. Similarly, the sparsity pattern of the matrix does not
change in the sequence in many applications of algebraic multigrid
preconditioners and thus the costly initial setup of the restriction and
prolongation operators only has to be carried out once for the first linear
system.

All of the above strategies aim at reducing the costs for the construction or
application of a preconditioner. However, they do not incorporate external data
into the solution process and do not benefit from the fact that previous
linear systems have already been solved. Nevertheless, it should be emphasized
strongly, that the application of a problem-dependent preconditioner is crucial
in almost all applications and the approaches in subsequent sections should be
seen as complementary to preconditioning. An overview of preconditioning
techniques can be found in the survey article of Benzi~\cite{Ben02} and the
books of Saad~\cite{Saa03} and Greenbaum~\cite{Gre97}.


\subsection{Augmentation}
\label{sec:rec:strat:aug}
In augmented Krylov subspace methods, the search space is enlarged by an
$m$-di\-men\-sio\-nal subspace $\vsU$. With a Krylov subspace method that
satisfies an optimality condition like CG, GMRES or MINRES, the underlying
minimization can be carried out over the sum of the Krylov subspace and the
augmentation subspace $\vsU$. Ideally, the augmentation subspace $\vsU$ contains
information about the problem that is only slowly revealed in the Krylov
subspace itself, e.g., eigenvectors corresponding to eigenvalues of small
magnitude.  This approach was used by Morgan~\cite{Mor95} and also by Chapman
and Saad~\cite{ChaS97} in order to overcome recurring convergence slowdowns
after restarting the GMRES method. For $n<d(\oiA,\vr_0)$, the method described
by Morgan computes the minimal residual approximation with respect to an initial
guess $\vx_0$ and the subspace $\vsK_n(\oiA,\vr_0)+\vsU$, i.e., the approximate
solution $\vx_n^\MR\in\vx_0+\vsK_n(\oiA,\vr_0)+\vsU$ that satisfies
\begin{equation*}
  \nrm{\vb-\oiA\vx_n^\MR} = \min_{\vz\in\vx_0+\vsK_n(\oiA,\vr_0)+\vsU}
  \nrm{\vb-\oiA\vz}.
\end{equation*}
In the restarted GMRES setup in~\cite{Mor95} and \cite{ChaS97}, the subspace
$\vsU$ is chosen as the span of a few harmonic Ritz vectors which correspond to
the harmonic Ritz values of smallest magnitude. In~\cite{Mor00}, Morgan showed
how Ritz vectors can be included implicitly as an augmentation space upon
restarts.

If a nonzero initial guess $\vx_0$ is used with augmentation, the initial guess
can also be included in the augmentation subspace, i.e., the augmentation space
$\widehat{\vsU}=\vsU+\Span{\vx_0}$ can be used instead of $\vsU$. Without loss
of generality, it is now assumed that the initial guess is already included in
$\vsU$. Then the optimal initial guess $\vx_0^\MR$ with minimal residual
$\vr_0^\MR$ with respect to $\vsU$ can be used, see
subsection~\ref{sec:rec:strat:ini}. For $0\leq n\leq d(\oiA,\vr_0^\MR)$, the
iterates $\vx_n^\MR$ then can be represented by
\[
  \vx_n^\MR = \oiP_{\vsK_n(\oiA,\vr_0^\MR) + \vsU}^\MR \vx
\]
and satisfy the optimality property
\begin{equation}
  \nrm{\vb - \oiA\vx_n^\MR} =
    \min_{\vz\in\vsK_n(\oiA,\vr_0^\MR)+\vsU}\nrm{\vb-\oiA\vz}.
    \label{eq:rec:strat:aug:mr}
\end{equation}

For self-adjoint and positive-semidefinite operators $\oiA$, an analogous
strategy can be carried out for the CG method by minimizing the $\oiA$-norm of
the error over the augmented Krylov subspace. For $0\leq n\leq
d(\oiA,\vr_0^\CG)$, the iterates $\vx_n^\CG$ are given by
\[
  \vx_n^\CG = \oiP_{\vsK_n(\oiA,\vr_0^\CG) + \vsU}^\CG \vx
\]
and satisfy the optimality property
\begin{equation}
  \nrm[\oiA]{\vx-\vx_n^\CG} =
    \min_{\vz\in\vsK_n(\oiA,\vr_0^\CG)+\vsU}\nrm[\oiA]{\vx-\vz}.
  \label{eq:rec:strat:aug:cg} \\
\end{equation}

Note that the minimization properties in
equations~\eqref{eq:rec:strat:aug:cg}--\eqref{eq:rec:strat:aug:mr} coincide for
$n=0$ with the respective minimization properties for the initial guess in
equations~\eqref{eq:rec:strat:ini:cg}--\eqref{eq:rec:strat:ini:mr}.  However,
the minimization in
equations~\eqref{eq:rec:strat:ini:cg}--\eqref{eq:rec:strat:ini:mr} is only used
to obtain an initial guess with minimal residual before applying the GMRES or
MINRES method and the subspace $\vsU$ is ignored in the actual GMRES or MINRES
method. In contrast, the subspace $\vsU$ is explicitly included in the
minimization in
equations~\eqref{eq:rec:strat:aug:cg}--\eqref{eq:rec:strat:aug:mr} while the
Krylov subspace method proceeds. Thus, the above augmentation approach can be
seen as a straightforward extension of the adaption of the initial guess to the
full solution process in a Krylov subspace method.

Because an augmented Krylov subspace method of the above type
just includes the subspace $\vsU$ in the minimization, it is obvious that
the errors in the $\oiA$-norm or residual norms are bounded by their
non-augmented counterparts, i.e.
\begin{align*}
  \nrm[\oiA]{\vx-\vx_n^\CG}
    &\leq \min_{\vz\in\vsK_n(\oiA,\vr_0^\CG)} \nrm[\oiA]{\vx-\vz} \\
  \text{and}\qquad
    \nrm{\vb-\oiA\vx_n^\MR}
    &\leq \min_{\vz\in\vsK_n(\oiA,\vr_0^\MR)} \nrm{\vb-\oiA\vz}.
\end{align*}

It is important to note that there is no guarantee that the sum
$\vsK_n(\oiA,\vr_0)+\vsU$ is a direct sum. In finite precision, the sum will
often be direct but the minimal angle $\theta_{\min}(\vsK_n(\oiA,\vr_0),\vsU)$
may become small and thus care has to be taken if separate bases for
$\vsK_n(\oiA,\vr_0)$ and $\vsU$ are used.

Erhel and Guyomarc'h~\cite{ErhG97,ErhG00} introduced an augmented variant of the
CG algorithm for sequences of linear systems where the operators
$\oiA^{(1)}=\ldots=\oiA^{(M)}$ are constant and only the right hand sides
change. In their method, the subspace $\vsU$ is the full Krylov subspace that
was constructed in the solution process of the previous linear system. The
method minimizes the $\oiA$-norm of the error over the subspace
$\vsK_n(\oiP_{\vsU^\perp,\oiA\vsU} \oiA, \vr_0) + \vsU$, i.e., the Krylov
subspace is built with the projected operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$.
This also makes this method a \emph{deflated} method which is the subject of the
next subsection.

Saad~\cite{Saa97} complemented the literature with convergence estimates of
augmented minimal residual Krylov subspace methods in the case of nearly
$\oiA$-invariant augmentation spaces $\vsU$. Let
\[
  \oiA=[\vtS_1,\vtS_2]\mat{\ofJ_1 & \\ & \ofJ_2} [\vtS_1,\vtS_2]^\inv
\]
be a partitioned Jordan decomposition and let
$\vsS_1=\Span{\vtS_1},\vsS_2=\Span{\vtS_2}$. Then Saad showed
in~\cite[corollary~3.6]{Saa97} that the iterates $\vx_n^\MR$ from the augmented
minimal residual method, see equation~\eqref{eq:rec:strat:aug:mr}, satisfy
\[
  \nrm{\vb-\oiA\vx_n^\MR}
  \leq
  \frac{\nrm{\bar{\vr}_n}}{\sin\left(\frac{\theta_{\min}(\vsS_1,\vsS_2)}{2}\right)},
\]
where $\bar{\vr}_n$ is the residual of $n$ steps GMRES applied to the linear
system
\[
  \oiA\delta
  = \theta_{\max}(\oiA\vsU,\vsS_1) \oiP_{\vsS_1,\vsS_2}\vr_0^\MR
  + \oiP_{\vsS_2,\vsS_1}\vr_0^\MR
\]
with initial guess zero. The analysis suggests that the convergence behavior of
the augmented minimal residual method is similar to GMRES applied to the linear
system
\begin{equation}
  \oiA\delta = \oiP_{\vsS_2,\vsS_1}\vr_0^\MR
  \label{eq:rec:strat:aug:defl}
\end{equation}
if the sine of the maximal angle $\sin\theta_{\max}(\oiA\vsU,\vsS_1)$ between
$\oiA\vsU$ and an invariant subspace $\vsS_1$ is small. The right hand side in
equation~\eqref{eq:rec:strat:aug:defl} has no components in the invariant
subspace $\vsS_1$ but only in $\vsS_2$ which may significantly improve the
convergence behavior compared to the original right hand side. However, as it
is shown in section~\ref{sec:rec:per:kry}, Krylov subspace methods are very
sensitive to perturbations of the operator or the right hand side without
further assumptions. Krylov subspace methods with perturbations are
discussed in detail in the context of deflation in
sections~\ref{sec:rec:per:kry} and \ref{sec:rec:sel:kry}.

Recently, Wakam and Erhel~\cite{WakE13} proposed a restarted GMRES variant that
uses an augmented non-orthogonal basis in order to reduce the communication time
in the context of parallel computations on distributed memory machines.

In~\cite{EieES00}, Eiermann, Ernst and Schneider provided a clear abstract
framework for analyzing augmented Krylov subspace methods in the setting of
restarted minimal residual methods.

\section{Deflation}
\label{sec:rec:def}

In \emph{deflated} Krylov subspace methods, the operator of the linear system is
explicitly modified in order to improve the convergence behavior. This is,
e.g., achieved by multiplying the linear system with a projection $\oiP$ and
then applying a Krylov subspace method to the projected linear system
\begin{equation}
  \oiP\oiA\vx_\star = \oiP\vb.
  \label{eq:rec:def:ls}
\end{equation}
The projected operator $\oiP\oiA$ exhibits the eigenvalue zero with multiplicity
at least $m=\dim\vsN(\oiP)$. Of course, the projection $\oiP$ has to fulfill
certain criteria in order to obtain a well-defined Krylov subspace method
because $\oiP\oiA$ is singular if $\oiP$ is not the identity projection, i.e.,
if $m>0$.
Furthermore, a solution $\vx_\star$ of the deflated linear
system~\eqref{eq:rec:def:ls} is not necessarily a solution of the original linear
system~\eqref{eq:rec:ls} and some ``correction'' scheme is required in order to
recover a solution of \eqref{eq:rec:ls} from $\vx_\star$.

The next subsection further motivates deflated Krylov subspace methods and gives
a brief overview of the literature.


\subsection{Motivation}
\label{sec:rec:def:mot}

A popular choice in the literature are projections that are based on
approximations of invariant subspaces. In order to motivate this idea, let
\[
  \oiA
  = [\vtS_1,\vtS_2]\mat{\ofJ_1 & \\ & \ofJ_2} [\vtS_1,\vtS_2]^\inv
\]
be a partitioned Jordan decomposition and
$\vsS_1=\Span{\vtS_1},\vsS_2=\Span{\vtS_2}$. Then the projection
$\oiP_{\vsS_2,\vsS_1}$ is well defined and a Jordan
decomposition of the projected operator $\oiP_{\vsS_2,\vsS_1}
\oiA$ is
\begin{equation*}
  \oiP_{\vsS_2,\vsS_1} \oiA
  = [\vtS_1,\vtS_2]\mat{0 & \\ & \ofJ_2} [\vtS_1,\vtS_2]^\inv.
\end{equation*}
If a Krylov subspace method is applied to the deflated linear
system~\eqref{eq:rec:def:ls} with $\oiP=\oiP_{\vsS_2,\vsS_1}$, then the
invariant subspace $\vsS_1$ and its associated spectrum $\Lambda(\ofJ_1)$ are
``hidden'' from the method and the impact on the convergence behavior can be
substantial. Also the spectral convergence bounds for CG, GMRES and
MINRES may improve significantly due to the fact that only the subset
$\Lambda(\ofJ_2)\subseteq\Lambda(\oiA)$ has to be taken into account, see
theorems~\ref{thm:back:cg-kappa}, \ref{thm:back:gmres:spectral} and
\ref{thm:back:minres:bound}. An approximate solution $\vx_n$ of the deflated
linear system~\ref{eq:rec:def:ls} can be turned into an approximate
solution of the original linear system~\eqref{eq:rec:ls} by either using an
adapted initial guess or by a simple correction scheme. The details are
addressed in section~\ref{sec:rec:def:well}.

In practice, only an approximation $\vsU$ to an invariant subspace $\vsS_1$ is
at hand. Still $m=\dim\vsN(\oiP)$ eigenvalues are sent to zero but the remaining
part of the spectrum will differ from $\Lambda(\ofJ_2)$ in general. However, it
is to be expected that small perturbations of an invariant subspace lead to
small perturbations in the remaining spectrum if the minimal angle
$\theta_{\min}(\vsS_1,\vsS_2)$ is not too small, i.e., if the invariant
subspaces $\vsS_1$ and $\vsS_2$ are well-conditioned. The effects of perturbed
subspaces on projections and spectral properties of projected operators are
analyzed in sections~\ref{sec:rec:per:proj} and \ref{sec:rec:per:def}.

Furthermore, it is shown in
section~\ref{sec:rec:def:aug}, that there is a close relationship between deflated and
augmented Krylov subspace methods. Because deflated methods essentially
replace the operator with a projected operator, the impact on the convergence
behavior can be studied by analyzing Krylov subspace methods for singular linear
systems. In the remaining part of this subsection, a brief overview of deflation
strategies in the literature is given. A comprehensive presentation with a broad
overview on the literature can be found in the survey article by Simoncini and
Szyld~\cite{SimS07}.  Subsequent sections present details for the deflated CG,
MINRES and GMRES methods along with new results that emerge from the analysis in
this thesis.

The first deflated Krylov subspace methods have been introduced in 1987 by
Nicolaides~\cite{Nic87} and in 1988 by Dost\'{a}l~\cite{Dos88}. Both
showed how the convergence behavior can be improved by using a deflated CG
method for symmetric and positive-definite operators $\oiA$. Since these early
works, the idea of deflation and augmentation has evolved and several authors
applied them to a wide range of Krylov subspace methods.
Saad,
Yeung, Erhel and Guyomarc'h~\cite{SaaYEG00} presented a deflated CG algorithm
that is mathematically equivalent to the one that Nicolaides introduced
in~\cite{Nic87}. The authors of~\cite{SaaYEG00} also seem to have made the first
link between augmented and deflated Krylov subspace methods by showing that the
deflated CG algorithm is a generalization of the augmented CG algorithm of Erhel
and Guyomarc'h~\cite{ErhG97} to arbitrary augmentation spaces.

The determination of a deflation subspace varies across the literature but there
is a strong focus on using eigenvector approximations, e.g., Ritz or harmonic
Ritz vectors. However, other choices have been studied by several authors.
In the context of discretized elliptic partial differential equations, already
the early work of Nicolaides~\cite{Nic87} considers the use of deflation
subspaces that are based on piecewise constant interpolation from a set of
subdomains. Mansfield~\cite{Man90} showed that Schur
complement-type domain decomposition methods can be seen as a series of deflated
methods. Similar to the work of Nicolaides~\cite{Nic87}, Mansfield~\cite{Man91}
used a deflation subspace that is based on a coarse grid interpolation and
combined this approach with a damped Jacobi smoother as a preconditioner that is
related to the two-grid method. Nabben and Vuik pointed out similarities between
the deflated CG method and domain decomposition methods
in~\cite{NabV04,NabV06,NabV08} and extended the comparison to multigrid methods
with Tang and Erlangga in~\cite{TanNVE09} and with Tang and MacLachlan
in~\cite{TanMNV10}. In~\cite{KahR12}, Kahl and Rittich presented convergence
estimates for the deflated CG method with arbitrary deflation subspaces based on
the effective condition number and techniques from algebraic multigrid methods.

For non-self-adjoint operators, de Sturler introduced the GCRO method
in~\cite{Stu96} which is a nested Krylov subspace method involving an outer and
an inner iteration. The outer method is the GCR method~\cite{Elm82,EisES83} and
in each iteration of GCR several steps of GMRES are applied to the projected
linear system~\eqref{eq:rec:def:ls} with $\oiP=\oiP_{(\oiA\vsU)^\perp}$,
where the subspace $\vsU$ is determined by the outer GCR iteration.
In~\cite{Stu99}, de Sturler enhanced the GCRO method with an optimal truncation
variant for restarts (GCROT). Furthermore, the GCRO method was generalized to
an arbitrary subspace $\vsU$ by Kilmer and de Sturler in~\cite{KilS06}.

The eigenvalue translation preconditioner by Kharchenko and
Yeremin~\cite{KhaY95} also aims at moving some eigenvalues but instead of moving
them to zero they are moved to a vicinity of $1$. However, the technique does
not fit into the setting of a deflated linear system~\ref{eq:rec:def:ls} but
rather is a classical preconditioner. The same holds for the approach that was
taken by Erhel, Burrage and Pohl in~\cite{ErhBP96} where the presented
preconditioner moves some eigenvalues to the spectral radius $\rho(\oiA)$ of the
operator $\oiA$. For the CG method, Nabben and Vuik~\cite{NabV06} showed that a
pure deflation approach yields iterates with smaller or equal $\oiA$-norm of the
error compared to a deflation-based preconditioner that shifts eigenvalues to
$1$.  Deflation-based preconditioners have also been used and analyzed by
Erlangga and Nabben~\cite{ErlN08,ErlN08a} and Tang, Nabben, Vuik and
Erlangga~\cite{TanNVE09}.


\subsection{Projections and corrections}
\label{sec:rec:def:well}

As mentioned in the previous section~\ref{sec:rec:def:mot}, $\oiA$-invariant
subspaces are usually not known in practice. Thus, analogous to the strategies
in section~\ref{sec:rec:strat}, it is now just assumed that an $m$-dimensional
subspace $\vsU\subseteq\vsH$ is given in order to construct a projection $\oiP$
for the deflated linear system~\eqref{eq:rec:def:ls}. Furthermore, let
$\vtU\in\vsH^m$ be given such that $\vsU=\Span{\vtU}$. In principle, one
could apply CG, GMRES or MINRES to the projected linear
system~\eqref{eq:rec:def:ls} with any projection, e.g., $\oiP=\oiP_{\vsU^\perp}$.
However, care has to be taken such that the following conditions are met:
\begin{enumerate}
  \item The Krylov subspace method is well defined when it is applied to the
    projected linear system~\eqref{eq:rec:def:ls} which implies that it
    terminates with a solution $\vx_\star$ of~\eqref{eq:rec:def:ls}, see
    definition~\ref{def:back:krylov-well}.
  \item A solution $\vx$ of the original linear system~\eqref{eq:rec:ls} can be
    recovered from $\vx_\star$.
\end{enumerate}

This subsection discusses existing approaches from the literature and studies
when they satisfy the above conditions.

Two projections are dominant in the literature for deflated Krylov subspace
methods, i.e., Krylov subspace methods that are applied to the projected linear
system~\eqref{eq:rec:def:ls}:
\begin{enumerate}
  \item $\oiP=\oiQ_\vsU^\CG$, cf.\ equation~\eqref{eq:proj-cg-res}, is primarily
    used with the CG method, e.g.,
    in~\cite{Nic87,Dos88,Kol98,SaaYEG00,FraV01,NabV04,NabV06,NabV08,TanNVE09,TanMNV10}.
    The projection $\oiQ_\vsU^\CG$ is well defined if the linear operator $\oiA$
    is self-adjoint and positive semidefinite but may cease to exist for
    indefinite or non-self-adjoint linear operators $\oiA$.
  \item $\oiP=\oiQ_\vsU^\MR$, cf.\ equation~\eqref{eq:proj-mr-res}, appears in
    the context of the GMRES, MINRES and GCRO methods, e.g.,
    in~\cite{Stu96,KilS06,ParSMJM06,WanSP07,SooSX13}.
\end{enumerate}

The use of the two projections $\oiQ_\vsU^\CG$ and $\oiQ_\vsU^\MR$ can be
motivated as follows. Consider the projections $\oiP_\vsU^\CG$ and $\oiP_\vsU^\MR$ that
already showed up in the computation of an optimal initial guess with respect to
a subspace, cf.\ section~\ref{sec:rec:strat:ini}, and in augmented methods,
cf.\ section~\ref{sec:rec:strat:aug}. Both projections can be used to decompose
a solution $\vx$ of the original linear system~\eqref{eq:rec:ls} into two parts:
\begin{align*}
  \vx &= \oiP_\vsU^\CG\vx + \oiP_{\vsU^\perp}^\CG\vx \\
      &= \oiP_\vsU^\MR\vx + \oiP_{\vsU^\perp}^\MR\vx.
\end{align*}
The first terms of both decompositions are the optimal initial guesses
$\vx_0^\CG$ and $\vx_0^\MR$ with respect to the subspace $\vsU$ that have been
derived in section~\ref{sec:rec:strat:ini} and can be computed from the right
hand side
$\vb$:
\begin{align*}
  \oiP_\vsU^\CG\vx &= \vx_0^\CG = \vtU\ip{\vtU}{\oiA\vtU}^\inv\ip{\vtU}{\vb}\\
  \text{and}\qquad
  \oiP_\vsU^\MR\vx &= \vx_0^\MR =
    \vtU\ip{\oiA\vtU}{\oiA\vtU}^\inv\ip{\oiA\vtU}{\vb}.
\end{align*}
For the second terms, let $\vx_\star^\CG,\vx_\star^\MR\in\vsH$ such that
$\oiP_{\vsU^\perp}^\CG \vx_\star^\CG = \oiP_{\vsU^\perp}^\CG\vx$ and
$\oiP_{\vsU^\perp}^\MR \vx_\star^\MR = \oiP_{\vsU^\perp}^\MR\vx$. Inserting the
representations of the solution
\begin{align}
  \begin{aligned}
    \vx
      &= \oiP_\vsU^\CG\vx + \oiP_{\vsU^\perp}^\CG\vx_\star^\CG \\
      &= \oiP_\vsU^\MR\vx + \oiP_{\vsU^\perp}^\MR\vx_\star^\MR
  \end{aligned}
  \label{eq:rec:def:decomp}
\end{align}
into the original linear system~\eqref{eq:rec:ls} yields with
lemma~\ref{lem:proj-commute} the following equivalences:
\begin{align*}
  \oiA\vx=\vb
    &&\LLRA&& \oiA \oiP_{\vsU^\perp}^\CG\vx_\star^\CG = \oiA(\id - \oiP_\vsU^\CG)\vx
    &&\LLRA&& \oiQ_\vsU^\CG\oiA\vx_\star^\CG = \oiQ_\vsU^\CG\vb \\
  \text{and}\qquad
  \oiA\vx=\vb
    &&\LLRA&& \oiA \oiP_{\vsU^\perp}^\MR\vx_\star^\MR = \oiA(\id - \oiP_\vsU^\MR)\vx
    &&\LLRA&& \oiQ_\vsU^\MR\oiA\vx_\star^\MR = \oiQ_\vsU^\MR\vb.
\end{align*}
Thus, a solution $\vx$ of the original linear system~\eqref{eq:rec:ls} can be
obtained in two steps:
\begin{enumerate}
  \item Obtain a solution $\vx_\star$ of the projected linear
    system~\eqref{eq:rec:def:ls} with a projection
    $\oiP\in\{\oiQ_\vsU^\CG,\oiQ_\vsU^\MR\}$.
  \item Update the obtained solution $\vx_\star$ with the corresponding
    correction from equation~\eqref{eq:rec:def:decomp}.
\end{enumerate}
In the literature, the correction in step 2 is often carried out in the
beginning by adapting the initial guess. Both approaches are known to be
equivalent and thus only make an algorithmic difference, see
section~\ref{sec:rec:def:imp}.

The remaining question is: under which conditions are the CG, GMRES and MINRES
methods well defined when they are applied to the projected linear
system~\eqref{eq:rec:def:ls} with $\oiP\in\{\oiQ_\vsU^\CG,\oiQ_\vsU^\MR\}$.


\subsection{CG method}

The following theorem gathers well-known results about the CG method for the
deflated linear system~\ref{eq:rec:def:ls} with $\oiP=\oiQ_\vsU^\CG$.

\begin{thm}[Deflated CG]
  \label{thm:rec:def:cg}
  Let $\oiA\vx=\vb$ be a consistent linear system with a self-adjoint and
  positive-semidefinite operator $\oiA\in\vsL(\vsH)$ and right
  hand side $\vb\in\vsH$. Let $\vtU\in\vsH^m$ be given such that
  $\vsU=\Span{\vtU}$ and $\oiA\vsU$ are $m$-dimensional.

  Then the following holds:
  \begin{enumerate}
    \item The projection $\oiQ_\vsU^\CG$ is well defined.
    \item For all initial guesses $\vx_0$, the CG method applied to the linear
      system
      \begin{equation}
        \widehat{\oiA} \widehat{\vx} = \widehat{\vb}
        \label{eq:rec:def:cg-ls}
      \end{equation}
      with $\widehat{\oiA}\DEF \oiQ_\vsU^\CG\oiA$ and
      $\widehat{\vb}=\oiQ_\vsU^\CG\vb$ is well defined.
    \item If $\widehat{\vx}_n$ is the constructed iterate in the $n$-th step in
      2., then the corrected iterate
      \begin{equation}
        \vx_n\DEF \oiP_{\vsU^\perp}^\CG \widehat{\vx}_n
          + \vtU\ip{\vtU}{\oiA\vtU}^\inv\ip{\vtU}{\vb}
        \label{eq:rec:def:cg-cor}
      \end{equation}
      satisfies
      \begin{align*}
        \nrm[\oiA]{\vx - \vx_n} &= \nrm[\widehat{\oiA}]{\vx - \widehat{\vx}_n}
        \\
        \text{and}\qquad
          \vr_n
            = \vb-\oiA\vx_n
            &= \widehat{\vb}-\widehat{\oiA}\widehat{\vx}_n
            = \widehat{\vr}_n,
      \end{align*}
      where $\vx\in\vsH$ is a solution of $\oiA\vx=\vb$.
  \end{enumerate}
\end{thm}

\begin{proof}
  \begin{enumerate}
    \item The projection $\oiQ_\vsU^\CG$ is well defined if and only if
      $\ip{\vtU}{\oiA\vtU}$ is nonsingular, cf.
      theorem~\ref{thm:back:proj_sing}. If $\oiA$ is nonsingular the statement
      is trivial because $\ip{\vtU}{\oiA\vtU}=\ip[\oiA]{\vtU}{\vtU}$. If $\oiA$
      is singular, the statement can be shown analogously to the first part of
      the proof of theorem~\ref{lem:back:well-opt}.
    \item The projected linear operator $\widehat{\oiA}$ is self-adjoint because
      by lemma~\ref{lem:proj-commute}:
      \begin{equation}
        \widehat{\oiA}^\adj
        = (\oiQ_\vsU^\CG \oiA)^\adj
        = (\oiP_{\vsU^\perp,\oiA\vsU}\oiA)^\adj
        = \oiA\oiP_{(\oiA\vsU)^\perp,\vsU}
        = \oiA\oiP_{\vsU^\perp}^\CG
        = \oiQ_\vsU^\CG\oiA
        = \widehat{\oiA}.
        \label{eq:rec:def:cg-selfadj}
      \end{equation}
      Furthermore, the operator $\widehat{\oiA}$ is positive semidefinite
      because for all $\vv\in\vsH$:
      \begin{align*}
        \ip{\vv}{\widehat{\oiA}\vv}
        &= \ip{\vv}{\oiQ_\vsU^\CG\oiA\vv}
        = \ip{\vv}{\oiP_{\vsU^\perp,\oiA\vsU}\oiA\vv}
        = \ip{\vv}{\oiP_{\vsU^\perp,\oiA\vsU}
          \oiA\oiP_{(\oiA\vsU)^\perp,\vsU}\vv} \\
        &= \ip{\oiP_{(\oiA\vsU)^\perp,\vsU}\vv}
            {\oiA\oiP_{(\oiA\vsU)^\perp,\vsU}\vv}
        = \nrm[\oiA]{\oiP_{(\oiA\vsU)^\perp,\vsU}\vv}^2
        \geq 0.
      \end{align*}
      Since the projected linear system~\eqref{eq:rec:def:cg-ls} results from
      the original linear system by applying the projection $\oiQ_\vsU^\CG$, it
      is consistent, i.e., $\widehat{\vb}\in\vsR(\widehat{\oiA})$. The
      well-definedness thus follows from theorem~\ref{thm:back:cg}.
    \item The error norm equality follows from
      \begin{align*}
        \nrm[\oiA]{\vx-\vx_n}^2
        &= \nrm[\oiA]{\vx-\oiP_\vsU^\CG\vx
          - \oiP_{\vsU^\perp}^\CG \widehat{\vx}_n}^2
        = \nrm[\oiA]{\oiP_{\vsU^\perp}^\CG (\vx-\widehat{\vx}_n)}^2
        = \nrm[\oiA]{\oiP_{(\oiA\vsU)^\perp,\vsU} (\vx-\widehat{\vx}_n)}^2\\
        &= \ip{\oiP_{(\oiA\vsU)^\perp,\vsU} (\vx-\widehat{\vx}_n)}
          {\oiA\oiP_{(\oiA\vsU)^\perp,\vsU} (\vx-\widehat{\vx}_n)}
        = \ip{\vx-\widehat{\vx}_n}{\oiQ_\vsU^\CG\oiA(\vx-\widehat{\vx}_n)} \\
        &= \nrm[\widehat{\oiA}]{\vx-\widehat{\vx}_n}^2
      \end{align*}
      and the residual equality follows from
      \begin{align*}
        \vb - \oiA\vx_n
        = \vb - \oiA\vtU\ip{\vtU}{\oiA\vtU}^\inv\ip{\vtU}{\vb}
          - \oiA\oiP_{\vsU^\perp}^\CG\widehat{\vx}_n
        = \oiQ_\vsU^\CG\vb - \oiQ_\vsU^\CG\oiA\widehat{\vx}_n
        = \widehat{\vb} - \widehat{\oiA}\widehat{\vx}_n.
      \end{align*}
  \end{enumerate}
\end{proof}

The conditions under which the deflated CG method is well defined and the basic
properties in the above theorem are well-known in the literature. This is not
unconditionally true for all aspects of the deflated GMRES and MINRES methods
which are covered in the next section. Details concerning the implementation of
the deflated CG method are discussed together with GMRES and MINRES in
section~\ref{sec:rec:def:imp}. New statements about the spectral properties of
the deflated operator $\widehat{\oiA}$ as well as convergence bounds and
selection strategies for deflation vectors in the setting of a sequence of
linear systems are provided in sections~\ref{sec:rec:per} and \ref{sec:rec:sel}.


\subsection{GMRES method and breakdowns}
\label{sec:rec:def:mr}

The question of the conditions under which GMRES is well defined for a singular
but consistent linear system $\oiA\vx=\vb$ has been answered in parts in
section~\ref{sec:back:mr:gmres} and theorem~\ref{thm:back:gmres}. There, only a
particular initial guess was assumed to be given such that
equation~\eqref{eq:back:gmres-cond} holds:
\[
  \vsK_d(\oiA,\vr_0)\cap\vsN(\oiA)=\{0\}.
\]
Here, the question is: under which conditions is the GMRES method well defined
for every initial guess $\vx_0$. As already noticed in
section~\ref{sec:back:mr:gmres}, a sufficient condition is
\begin{equation}
  \vsR(\oiA)\cap\vsN(\oiA)=\{0\}
  \label{eq:rec:def:mr:cond}
\end{equation}
because $\vsK_d(\oiA,\vr_0)\subseteq\vsR(\oiA)$ holds for a consistent linear
system, i.e., if $\vb\in\vsR(\oiA)$. This has also been shown by Brown and
Walker in~\cite[theorem~2.6]{BroW97}. In~\cite[theorem~5.1]{GauGLN13}, the
author, Gutknecht, Liesen and Nabben extended the result of
Brown and Walker~\cite[theorem~2.6]{BroW97} by showing that
condition~\eqref{eq:rec:def:mr:cond} is also a necessary condition.

\begin{thm}
  \label{thm:rec:def:mr:cond}
  Let $\oiA\vx=\vb$ be a consistent linear system with $\oiA\in\vsL(\vsH)$
  (possibly singular) and $\vb\in\vsH$. Then the following two conditions are
  equivalent:
  \begin{enumerate}
    \item For every initial guess $\vx_0\in\vsH$, the GMRES method applied to
      the linear system $\oiA\vx=\vb$ is well defined.
    \item $\vsR(\oiA)\cap\vsN(\oiA)=\{0\}$.
  \end{enumerate}
\end{thm}

\begin{proof}
  It has been shown in~\cite[theorem~2.6]{BroW97} and in
  section~\ref{sec:back:mr:gmres}, that condition 2 implies condition 1. The
  reverse is proved by contradiction. If $0\neq\vv\in\vsN(\oiA)\cap\vsR(\oiA)$,
  then an initial guess can be constructed such that GMRES does not terminate
  with the solution. Because $\vv\in\vsR(\oiA)$, there exists a nonzero
  $\vw\in\vsH$ such that $\vv=\oiA\vw$. Then the initial guess
  $\vx_0\DEF\vx-\vw$ yields $\vr_0=\vb-\oiA\vx_0=\vb-\oiA\vx +
  \oiA\vw=\oiA\vw=\vv$. But then $\oiA\vr_0=0$ because $\vv\in\vsN(\oiA)$,
  such that the GMRES method terminates at the first iteration with the
  approximation $\vx_0$ which has a nonzero residual $\vr_0=\vv$.
\end{proof}

If a Krylov subspace method is not well defined, i.e., it terminates without
finding the solution, this situation is also referred to as a \emph{breakdown}
of the method. The above proof of theorem~\ref{thm:rec:def:mr:cond} leads to the
following characterization of all initial guesses that lead to a breakdown of
GMRES at the first iteration (see \cite[corollary~5.2]{GauGLN13}).

\begin{cor}
  \label{cor:rec:def:x0break}
  Let $\oiA\vx=\vb$ be a consistent linear system with $\oiA\in\vsL(\vsH)$
  (possibly singular), right hand side $\vb\in\vsH$ and a solution $\vx\in\vsH$.
  Then the GMRES method breaks down at the first iteration for all initial
  guesses
  \[
    \vx_0\in\vsX_0\DEF\{\vx-\vw~|~\oiA\vw\in\vsN(\oiA)\setminus\{0\}\}.
  \]
\end{cor}

As already mentioned in section~\ref{sec:back:mr:minres}, a self-adjoint
operator $\oiA$ always fulfills condition~\eqref{eq:rec:def:mr:cond}. However,
care has to be taken in the non-self-adjoint case.

Consider a nonsingular operator $\oiA$ and the deflated linear
system~\eqref{eq:rec:def:ls} with $\oiP=\oiQ_\vsU^\MR$, i.e,
\begin{equation}
  \widehat{\oiA}\widehat{\vx} = \widehat{\vb},
  \label{eq:rec:def:mr:ls}
\end{equation}
where $\widehat{\oiA}\DEF\oiQ_\vsU^\MR\oiA$ and
$\widehat{\vb}\DEF\oiQ_\vsU^\MR\vb$. The well-definedness of
the GMRES method applied to the deflated linear system~\ref{eq:rec:def:mr:ls}
can now be answered with theorem~\ref{thm:rec:def:mr:cond} (see
also~\cite[corollary~5.3]{GauGLN13}).

\begin{cor}
  \label{cor:rec:def:mr:cond-def}
  Let $\oiA\vx=\vb$ with a nonsingular operator $\oiA$ and $\vb\in\vsH$.
  For a subspace $\vsU\subseteq\vsH$, the following statements are equivalent:
  \begin{enumerate}
    \item For every initial guess $\vx_0$, the GMRES method applied to the
      linear system~\eqref{eq:rec:def:mr:ls} is well defined.
    \item $\vsU\cap(\oiA\vsU)^\perp=\{0\}$.
    \item $\vsH = \vsU \oplus (\oiA\vsU)^\perp$.
  \end{enumerate}
  In particular, the above conditions are fulfilled if $\vsU$ is an
  $\oiA$-invariant subspace, i.e., if $\oiA\vsU=\vsU$.
\end{cor}

\begin{proof}
  The equivalence of condition 2 in theorem~\ref{thm:rec:def:mr:cond} with
  condition 2 of corollary~\ref{cor:rec:def:mr:cond-def} becomes apparent by
  calculating the range and null space of the operator $\widehat{\oiA}$:
  \begin{align*}
    \vsR(\widehat{\oiA})
      &= \vsR(\oiP_{(\oiA\vsU)^\perp}\oiA)
      = \vsR(\oiP_{(\oiA\vsU)^\perp})
      = (\oiA\vsU)^\perp \\
    \text{and}\qquad
    \vsN(\widehat{\oiA})
      &= \vsN(\oiA\oiP_{(\oiA^\adj\oiA\vsU)^\perp,\vsU})
      = \vsN(\oiP_{(\oiA^\adj\oiA\vsU)^\perp,\vsU})
      = \vsU.
  \end{align*}

  For the equivalence of condition 2 and 3, let $\vtU\in\vsH^m$ with
  $m=\dim\vsU$. Then condition 2 is equivalent to
  \[
    0 \neq \ip{\vtU\vu}{\oiA\vtU} = \vu^\htp \ip{\vtU}{\oiA\vtU}
  \]
  for all nonzero $\vu\in\C^m$. This is equivalent to the nonsingularity of
  $\ip{\vtU}{\oiA\vtU}$ which is in turn equivalent to
  $\vsH=\vsU\oplus(\oiA\vsU)^\perp$ by theorem~\ref{thm:back:proj_sing}.
\end{proof}

To illustrate the possibility of breakdowns of the GMRES method, two examples
from \cite{GauGLN13} are given below.

\begin{ex}
  \label{ex:rec:def:break1}
  Consider the linear system $\oiA\vx=\vb$ with
  \[
    \oiA=\mat{0&1\\1&0},
      \qquad \vx=\mat{0\\1},
      \qquad \vb=\mat{1\\0}
  \]
  and the Euclidean inner product $\ipdots[2]$. Let the deflation basis be
  chosen as $\vtU=\mat{1,0}^\tp$, i.e., the deflation space is $\vsU=\Span{\vtU}$.
  Then
  \[
    \oiQ_\vsU^\MR
      = \oiP_{(\oiA\vsU)^\perp}
      = \oiP_{\Span{\vx}^\perp}
      = \mat{1&0\\0&0},
    \qquad \widehat{\oiA} = \oiQ_\vsU^\MR\oiA = \mat{0&1\\0&0}
    \quad\text{and}\quad \widehat{\vb}=\oiQ_\vsU^\MR\vb = \mat{1\\0}.
  \]
  If $\vx_0=0$, then $\widehat{\vr}_0=\widehat{\vb}$ and
  $\widehat{\oiA}\widehat{\vr}_0=0$ and
  the GMRES method applied to the deflated linear
  system~\eqref{eq:rec:def:mr:ls} terminates at the first iteration with the
  approximation $\vx_0$. Since $\widehat{\oiA}\vx_0\neq\widehat{\vb}$, this is
  a breakdown of GMRES. Furthermore, the correction~\eqref{eq:rec:def:decomp}
  does not recover the solution of the original linear system $\oiA\vx=\vb$ from
  $\vx_0$ because
  \[
    \oiP_\vsU^\MR\vx + \oiP_{\vsU^\perp}^\MR\vx_0
    = \vtU\ip[2]{\oiA\vtU}{\oiA\vtU}^\inv\ip[2]{\oiA\vtU}{\vb}
    = \vtU\ip[2]{\oiA\vtU}{\oiA\vtU}^\inv\ip[2]{\mat{0\\1}}{\mat{1\\0}}
    = 0
    \neq \vx.
  \]
\end{ex}

In corollary~\ref{cor:rec:def:mr:cond-def}, it was stated, that breakdowns
cannot occur in the GMRES method applied to the deflated linear
system~\eqref{eq:rec:def:mr:ls} if $\vsU$ is $\oiA$-invariant. However, the
following example shows that care has also to be taken with approximate
$\oiA$-invariant subspaces if $\oiA$ is non-normal.

\begin{ex}
  \label{ex:rec:def:break2}
  Let $\alpha>0$ be a small positive number. Then $\vv=\mat{0,1,\alpha}^\tp$ is
  an eigenvector of the matrix
  \[
    \oiA = \mat{0 &1 &-\alpha^\inv\\
                1 &0 & \alpha^\inv\\
                0 &0 & 1}
  \]
  corresponding to the eigenvalue 1. Instead of $\vv$, the perturbed vector
  $\vtU=\mat{0,1,0}^\tp$ is chosen as a basis for the deflation space
  $\vsU=\Span{\vtU}$. Then
  \[
    \oiQ_\vsU^\MR
      = \oiP_{\Span{\ve_1}^\perp}
      = \mat{0&0&0\\0&1&0\\0&0&1}
    \quad\text{and}\quad
    \oiQ_\vsU^\MR\oiA
      = \mat{0 &0 & 0\\
             1 &0 & \alpha^\inv\\
             0 &0 & 1}
   \]
   and for $\vx,\vb\in\C^3$ with $\oiA\vx=\vb$, the GMRES method breaks down
   in the first step for all
   $\vx_0\in\{\vx+\beta\mat{1,0,0}^\tp~|~\beta\neq 0\}$. Note that
   $\nrm[2]{\vtU-\vv}=\alpha$ can be chosen arbitrarily small.
\end{ex}

The preceding example shows that measuring the departure of $\vsU$ from being
$\oiA$-invariant by the norm of the vector differences is inappropriate. A
more appropriate measure is the maximum angle $\theta_{\max}(\vsU,\oiA\vsU)$
between the subspaces $\vsU$ and $\oiA\vsU$. Note that the
condition~$\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$ is equivalent to the
conditions in corollary~\ref{cor:rec:def:mr:cond-def} (see
theorem~\ref{thm:back:proj_sing}) and thus also to the well-definedness of the
GMRES method applied to the deflated linear system~\eqref{eq:rec:def:mr:ls},
cf.\ theorem~\ref{thm:rec:def:mr:cond}. The following theorem gathers the
observations and states further properties of the deflated GMRES method.

\begin{thm}[Deflated GMRES, variant 1]
  \label{thm:rec:def:gmres1}
  Let $\oiA\vx=\vb$ be a linear system with a nonsingular operator
  $\oiA\in\vsL(\vsH)$ and right hand side $\vb\in\vsH$.
  Furthermore, let $\vtU\in\vsH^m$ be given such that
  $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$ for $\vsU=\Span{\vtU}$.

  Then the following holds:
  \begin{enumerate}
    \item For all initial guesses $\vx_0$, the GMRES method applied to the
      linear system
      \begin{equation}
        \widehat{\oiA}\widehat{\vx}=\widehat{\vb}
        \label{eq:rec:def:mr-ls}
      \end{equation}
      with $\widehat{\oiA}=\oiQ_\vsU^\MR\oiA$ and
      $\widehat{\vb}=\oiQ_\vsU^\MR\vb$ is well defined.
    \item If $\widehat{\vx}_n$ is the constructed iterate in the $n$-th step in
      1., then the corrected iterate
      \begin{equation}
        \vx_n\DEF \oiP_{\vsU^\perp}^\MR \widehat{\vx}_n
          + \vtU\ip{\oiA\vtU}{\oiA\vtU}^\inv\ip{\oiA\vtU}{\vb}
        \label{eq:rec:def:mr-cor}
      \end{equation}
      satisfies
      \begin{equation}
        \vr_n
        = \vb - \oiA\vx_n
        = \widehat{\vb} - \widehat{\oiA}\widehat{\vx}_n
        = \widehat{\vr}_n.
        \label{eq:rec:def:mr-res}
      \end{equation}
  \end{enumerate}
\end{thm}

\begin{proof}
  Item 1. is just a summary of theorem~\ref{thm:rec:def:mr:cond},
  corollary~\ref{cor:rec:def:mr:cond-def} and theorem~\ref{thm:back:proj_sing}.

  The residual equality in item 2. follows with lemma~\ref{lem:proj-commute} by
  noticing that
  \[
    \vb-\oiA\vx_n
    = \vb - \oiA\vtU\ip{\oiA\vtU}{\oiA\vtU}^\inv\ip{\oiA\vtU}{\vb}
      - \oiA\oiP_{\vtU^\perp}^\MR \widehat{\vx}_n
    = \oiQ_\vsU^\MR\vb - \oiQ_\vsU^\MR\oiA\widehat{\vx}_n
    = \widehat{\vb}-\widehat{\oiA}\widehat{\vx}_n.
  \]
\end{proof}

The key observation, that $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$ is a
necessary and sufficient condition for GMRES to be well defined for all initial
guesses, opens up another possibility for a deflated GMRES method. The condition
$\theta_{\max}(\vsU,\oiA\vsU)$ is also necessary and sufficient for the
existence of the projection $\oiQ_\vsU^\CG=\oiP_{\vsU^\perp,\oiA\vsU}$ which is
used in the deflated CG method, see equation~\eqref{eq:proj-cg}. As it turns
out, the GMRES method is also well defined when it is applied to the
deflated linear system~\eqref{eq:rec:def:ls} with $\oiP=\oiQ_\vsU^\CG$.

\begin{thm}[Deflated GMRES, variant 2]
  \label{thm:rec:def:gmres2}
  Let the assumptions of theorem~\ref{thm:rec:def:gmres1} hold. Then the
  following holds:
  \begin{enumerate}
    \item The projection $\oiQ_\vsU^\CG$ is well defined.
    \item For all initial guesses $\vx_0$, the GMRES method applied to the
      linear system
      \begin{equation}
        \widehat{\oiA}\widehat{\vx} = \widehat{\vb}
        \label{eq:rec:def:mr2-ls}
      \end{equation}
      with $\widehat{\oiA}\DEF \oiQ_\vsU^\CG\oiA$ and
      $\widehat{\vb}=\oiQ_\vsU^\CG\vb$ is well defined.
    \item The corrected iterates
      \begin{equation*}
        \vx_n\DEF \oiP_{\vsU^\perp}^\CG \widehat{\vx}_n
          + \vtU\ip{\vtU}{\oiA\vtU}^\inv\ip{\vtU}{\vb}
      \end{equation*}
      satisfy
      \begin{equation*}
          \vr_n
            = \vb-\oiA\vx_n
            = \widehat{\vb}-\widehat{\oiA}\widehat{\vx}_n
            = \widehat{\vr}_n.
      \end{equation*}
  \end{enumerate}
\end{thm}

\begin{proof}
  \begin{enumerate}
    \item The well-definedness of the projection directly follows from the
      assumption $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$, cf.\
      theorem~\ref{thm:complement}.
    \item The well-definedness of GMRES follows from
      theorem~\ref{thm:rec:def:mr:cond} by calculating the range and null space
      of the deflated operator:
      \begin{align*}
        \vsR(\widehat{\oiA})
          &= \vsR(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)
          = \vsR(\oiP_{\vsU^\perp,\oiA\vsU})
          = \vsU^\perp \\
        \text{and}\qquad
        \vsN(\widehat{\oiA})
          &= \vsN(\oiA\oiP_{(\oiA\vsU)^\perp,\vsU})
          = \vsN(\oiP_{(\oiA\vsU)^\perp,\vsU})
          = \vsU.
      \end{align*}
    \item The proof of the residual equality is analogous to the proof for the
      CG method in theorem~\ref{thm:rec:def:cg}.
  \end{enumerate}
\end{proof}

The second variant of the deflated GMRES variant is rarely used in the
literature. Erlangga and Nabben~\cite{ErlN08} used a more general projection
$\oiP$ for the deflated system~\eqref{eq:rec:def:ls} which is defined by
$\oiP\vx = \vx - \oiA\vtZ\ip{\vtY}{\oiA\vtZ}^\inv\ip{\vtY}{\vx}$. For
$\vtZ=\vtY$, the method coincides with the second deflated GMRES variant in
theorem~\ref{thm:rec:def:gmres2}. Yeung, Tang and Vuik~\cite{YeuTV10} also used
this deflated GMRES variant and presented an analysis for the case where $\vsU$
is an exact $\oiA$-invariant subspace.


\subsection{MINRES method and breakdowns}
\label{sec:rec:def:minres}

In cases where the original operator $\oiA$ is self-adjoint but possibly
indefinite, the MINRES method is attractive, see
section~\ref{sec:back:mr:minres}. Since GMRES is mathematically equivalent to
MINRES for a self-adjoint operator, all results for GMRES
carry over to MINRES if the deflated operator is self-adjoint.
However, as outlined in this subsection, also a non-self-adjoint
deflated operator is feasible due to the special structure of the involved
Krylov subspace.

Kilmer and de Sturler~\cite{KilS06} applied the MINRES method to the deflated
linear system~\eqref{eq:rec:def:mr-ls}, i.e., with the projected operator
$\widehat{\oiA}=\oiQ_\vsU^\MR\oiA$. In this case, the operator $\widehat{\oiA}$
is in general \emph{not} self-adjoint, even if $\oiA$ is self-adjoint. However,
as pointed out in~\cite[footnote on p. 2153]{KilS06}, it turns out that
\begin{equation*}
  \vsK_n(\oiQ_\vsU^\MR\oiA,\oiQ_\vsU^\MR\vv)
  = \vsK_n(\oiQ_\vsU^\MR\oiA\oiQ_\vsU^\MR,\oiQ_\vsU^\MR\vv)
\end{equation*}
holds for every vector $\vv\in\vsH$. Now the operator
$\oiQ_\vsU^\MR\oiA\oiQ_\vsU^\MR$ is self-adjoint because both $\oiA$ and
$\oiQ_\vsU^\MR$ are self-adjoint. Thus the Krylov subspace that is constructed
in the MINRES method when it is applied to the linear
system~\eqref{eq:rec:def:mr-ls} is implicitly generated by a self-adjoint
operator and the MINRES method is well defined for this linear system under the
same condition as the GMRES method, i.e.,
$\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$.

\begin{cor}[Deflated MINRES variants]
  \label{cor:rec:def:minres}
  Let the assumptions of theorem~\ref{thm:rec:def:gmres1} hold and let $\oiA$ be
  self-adjoint.

  Then the statements in theorems~\ref{thm:rec:def:gmres1} and
  \ref{thm:rec:def:gmres2} hold if GMRES is replaced by MINRES.
\end{cor}

\begin{proof}
  That the MINRES method is well defined when it is applied to the deflated
  linear system~\eqref{eq:rec:def:mr-ls} has been pointed out in the discussion
  preceding this theorem. For the second variant, analogous to the deflated CG
  method, it can be shown that the deflated operator
  $\widehat{\oiA}=\oiQ_\vsU^\CG\oiA$ is self-adjoint, see
  equation~\eqref{eq:rec:def:cg-selfadj}.
\end{proof}

Because the matrix in example~\ref{ex:rec:def:break1} is Hermitian, this example
also serves as an example of a breakdown of the first deflated MINRES variant if
the condition $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$ is violated.

In~\cite{OlsS10}, Olshanskii and Simoncini used an augmented and deflated MINRES
method where deflation is achieved with the second deflated MINRES variant,
i.e., the MINRES method applied to the deflated linear
system~\eqref{eq:rec:def:mr2-ls}. However, they included an additional explicit
augmentation in the MINRES algorithm. The author and Schl\"{o}mer
used the second deflated MINRES variant in~\cite{GauS13} in order to solve a
sequence of linear systems that stems from nonlinear Schr\"{o}dinger
equations. This application is discussed in detail in chapter~\ref{ch:nls}.


\subsection{Equivalence of deflation and augmentation}
\label{sec:rec:def:aug}

In the literature, many deflated Krylov subspace methods also incorporate some
sort of explicit augmentation. One example is the deflated and augmented version
of the CG algorithm by Saad, Yeung, Erhel and Guyomarc'h~\cite{SaaYEG00} where
an approximate solution $\vx_n$ is sought such that the Galerkin conditions
\begin{align*}
  \vx_n &\in \vx_0 + \vsK_n(\widehat{\oiA},\widehat{\vr}_0) + \vsU \\
  \vr_n &= \vb - \oiA\vx_n \perp \vsK_n(\widehat{\oiA},\widehat{\vr}_0) +\vsU
\end{align*}
hold with $\widehat{\oiA}=\oiQ_\vsU^\CG\oiA$ and
$\widehat{\vr}_0=\oiQ_\vsU^\CG(\vb-\oiA\vx_0)$.
However,
they showed that the proposed deflated and augmented method is mathematically
equivalent to the plain CG method applied to the deflated linear
system~\eqref{eq:rec:def:cg-ls} \emph{without} augmentation but with an adapted
initial guess, cf.\ section~\ref{sec:rec:def:imp}. A natural question is whether
also other deflated methods implicitly augment the search space.

In~\cite{GauGLN13}, the author, Gutknecht, Liesen and Nabben showed that a
\PetrovAndOrGalerkin{} method can in general be augmented by either
explicitly enlarging the search space as in in
equations~\eqref{eq:rec:strat:aug:mr}--\eqref{eq:rec:strat:aug:cg}, or
implicitly by applying an appropriate projection to the residuals and correcting
the thus computed approximate solutions.

The result is given here in its original form~\cite[theorem 3.2]{GauGLN13}
before it is recast in a more accessible form for the two special cases of the
deflated CG method (see theorem~\ref{thm:rec:def:cg}) and the first variant of
the deflated GMRES and MINRES method (see theorem~\ref{thm:rec:def:gmres1} and
corollary~\ref{cor:rec:def:minres}).

\begin{thm}
  \label{thm:rec:eq:aug-def}
  Let $\oiA\vx=\vb$ be a consistent linear system with linear operator
  $\oiA\in\vsL(\vsH)$ and right hand side and initial guess $\vb,\vx_0\in\vsH$.
  Furthermore, let $\oiB\in\vsL(\vsH)$ and $\vtU\in\vsH^m$ be such that
  $\ip{\oiB\vtU}{\oiA\vtU}$ is nonsingular and let
  $\widehat{\oiA}\in\vsL(\vsH)$, $\widehat{\vv}\in\vsH$ and $n\leq
  d(\widehat{\oiA},\widehat{\vv})$.

  Then, with $\vsU=\Span{\vtU}$, the following two pairs of conditions
  \begin{align}
    \left.
      \begin{aligned}
        \vx_n &\in \vx_0 + \vsK_n(\widehat{\oiA},\widehat{\vv}) + \vsU,\\
        \vr_n &= \vb - \oiA\vx_n
          \perp \oiB\vsK_n(\widehat{\oiA},\widehat{\vv}) + \oiB\vsU
      \end{aligned}
      \qquad
    \right\}
    \label{eq:rec:eq:explicit}
    \intertext{and}
    \left.
      \begin{aligned}
        \widehat{\vx}_n &\in \vx_0 + \vsK_n(\widehat{\oiA},\widehat{\vv}),\\
        \widehat{\vr}_n &= \oiP_{(\oiB\vsU)^\perp,\oiA\vsU}(\vb -
        \oiA\widehat{\vx}_n) \perp \oiB \vsK_n(\widehat{\oiA},\widehat{\vv})
      \end{aligned}
      \qquad
    \right\}
    \label{eq:rec:eq:implicit}
  \end{align}
  are equivalent in the sense that
  \begin{equation}
    \vx_n = \oiP_{(\oiA^\adj\oiB\vsU)^\perp,\vsU} \widehat{\vx}_n
      + \vtU\ip{\oiB\vtU}{\oiA\vtU}^\inv\ip{\oiB\vtU}{\vb}
    \qquad\text{and}\qquad
    \vr_n=\widehat{\vr}_n.
    \label{eq:rec:eq:cor}
  \end{equation}
\end{thm}

\begin{proof}
  The proof starts with the first pair of conditions~\eqref{eq:rec:eq:explicit}
  and shows the equivalence to the second pair~\eqref{eq:rec:eq:implicit}.
  Let the approximate solution be
  \begin{equation}
    \vx_n=\vx_0 + \vtV_n\vv_n + \vtU\vu_n
    \label{eq:rec:eq:solrepr}
  \end{equation}
  for a $\vtV_n\in\vsH^n$ with
  $\Span{\vtV_n}=\vsK_n(\widehat{\oiA},\widehat{\vv})$, $\vv_n\in\C^n$ and
  $\vu_n\in\C^m$. In order to satisfy the residual constraint
  in~\eqref{eq:rec:eq:explicit}, the residual $\vr_n=\vb-\oiA\vx_n$ must be
  orthogonal to both $\oiB\vsK_n(\widehat{\oiA},\widehat{\vv})$ and $\oiB\vsU$.
  Thus the residual constraint is equivalent to the pair of orthogonality
  conditions
  \begin{equation}
    \vr_n \perp \oiB\vsK_n(\widehat{\oiA},\widehat{\vv})
    \quad\text{and}\quad
    \vr_n \perp \oiB\vsU.
    \label{eq:rec:def:eq:twoconds}
  \end{equation}
  With $\vr_0=\vb-\oiA\vx_0$, the second condition is equivalent to
  \[
    0
    = \ip{\oiB\vtU}{\vr_n}
    = \ip{\oiB\vtU}{\vr_0 - \oiA\vtV_n\vv_n - \oiA\vtU\vu_n}
    = \ip{\oiB\vtU}{\vr_0 - \oiA\vtV_n\vv_n} - \ip{\oiB\vtU}{\oiA\vtU}\vu_n.
  \]
  By assumption, $\ip{\oiB\vtU}{\oiA\vtU}$ is nonsingular and thus
  \[
    \vu_n = \ip{\oiB\vtU}{\oiA\vtU}^\inv\ip{\oiB\vtU}{\vr_0 - \oiA\vtV_n\vv_n}.
  \]
  Substituting this into equation~\eqref{eq:rec:eq:solrepr} yields
  \begin{align}
    \vx_n &= \vx_0 + \vtV_n\vv_n
      + \vtU\ip{\oiB\vtU}{\oiA\vtU}^\inv
        \ip{\oiB\vtU}{\vr_0 - \oiA\vtV_n\vv_n} \notag \\
      &= \vx_0 + \vtV_n\vv_n
        - \vtU\ip{\oiB\vtU}{\oiA\vtU}^\inv\ip{\oiB\vtU}{\oiA(\vx_0+\vtV_n\vv_n)}
        + \vtU\ip{\oiB\vtU}{\oiA\vtU}^\inv\ip{\oiB\vtU}{\vb} \notag \\
      &= \oiP_{(\oiA^\adj\oiA\vsU)^\perp,\vsU} (\vx_0 + \vtV_n\vv_n)
        + \vtU\ip{\oiB\vtU}{\oiA\vtU}^\inv\ip{\oiB\vtU}{\vb}
        \label{eq:rec:def:cor1} \\
    \text{and}\qquad
    \vr_n &= \vb - \oiA\vtU\ip{\oiB\vtU}{\oiA\vtU}^\inv\ip{\oiB\vtU}{\vb}
      - \oiA\oiP_{(\oiA^\adj\oiB\vsU)^\perp,\vsU} (\vx_0 + \vtV_n\vv_n)
      \notag \\
      &= \oiP_{(\oiB\vsU)^\perp,\oiA\vsU}(\vb - \oiA(\vx_0 + \vtV_n\vv_n)).
        \label{eq:rec:def:rhat}
  \end{align}
  After defining
  \[
    \widehat{\vx}_n\DEF\vx_0 + \vtV_n\vv_n \in \vx_0
      + \vsK_n(\widehat{\oiA},\widehat{\vv})
  \]
  the first orthogonality condition in~\eqref{eq:rec:def:eq:twoconds} can be
  imposed on the residual~\eqref{eq:rec:def:rhat}:
  \[
    \oiP_{(\oiB\vsU)^\perp,\oiA\vsU}(\vb - \oiA \widehat{\vx}_n)
    \perp \oiB\vsK_n(\widehat{\oiA},\widehat{\vv}).
  \]
  The last two conditions now yield the second pair of
  conditions~\eqref{eq:rec:eq:implicit} and the
  correction~\eqref{eq:rec:eq:cor} follows from equation~\eqref{eq:rec:def:cor1}.
\end{proof}

For $\oiB=\id$, $\widehat{\oiA}=\oiQ_\vsU^\CG\oiA$ and
$\widehat{\vv}=\oiQ_\vsU^\CG(\vb-\oiA\vx_0)$, the second pair of
conditions~\eqref{eq:rec:eq:implicit} represents the Galerkin conditions for the
deflated CG method in theorem~\ref{thm:rec:def:cg} and the
correction~\eqref{eq:rec:eq:cor} is the corresponding
correction~\eqref{eq:rec:def:cg-cor}. This observation is registered in the
following corollary.

\begin{cor}
  \label{cor:rec:def:cg-aug}
  With the assumptions and notation from theorem~\ref{thm:rec:def:cg} and a
  solution $\vx\in\vsH$ of $\oiA\vx=\vb$, the corrected iterates $\vx_n$
  generated by the deflated CG method satisfy
  \[
    \vx_n = \vx_0 + \oiP_{\vsK_n(\widehat{\oiA},\widehat{\vr}_0)+\vsU}^\CG
      (\vx - \vx_0)
  \]
  or, equivalently,
  \[
    \nrm[\oiA]{\vx - \vx_n}
    = \min_{\vz\in\vx_0 + \vsK_n(\widehat{\oiA},\widehat{\vr}_0)+\vsU}
      \nrm[\oiA]{\vx - \vz}.
  \]
\end{cor}

As mentioned before, the fact that the deflated CG method implicitly performs
augmentation was already recognized by Saad, Yeung, Erhel and
Guyomarc'h~\cite{SaaYEG00}.

An analogous result can be stated for the deflated GMRES and MINRES methods. For
$\oiB=\oiA$, the orthogonality constraint in~\eqref{eq:rec:eq:implicit} is
equivalent to
\[
  \oiQ_\vsU^\MR(\vb-\oiA\widehat{\vx}_n)
    \perp \oiQ_\vsU^\MR\oiB\vsK_n(\widehat{\oiA},\widehat{\vv})
\]
because $\oiQ_\vsU^\MR = \oiP_{(\oiA\vsU)^\perp}$ is an orthogonal projection
and thus self-adjoint.  With $\widehat{\oiA}=\oiQ_\vsU^\MR\oiA$ and
$\widehat{\vv}=\oiQ_\vsU^\MR(\vb-\oiA\vx_0)$, the
conditions~\eqref{eq:rec:eq:implicit} thus represent the Petrov--Galerkin
conditions for the first variant of the deflated GMRES method in
theorem~\ref{thm:rec:def:gmres1} and the correction~\eqref{eq:rec:eq:cor} equals
the correction~\eqref{eq:rec:def:mr-cor}. The following corollary gathers
the made observations for the GMRES method.

\begin{cor}
  \label{cor:rec:def:mr-aug}
  With the assumptions and notation from theorem~\ref{thm:rec:def:gmres1} and a
  solution $\vx\in\vsH$ of $\oiA\vx=\vb$, the corrected iterates $\vx_n$
  generated by the first variant of the deflated GMRES method satisfy
  \begin{equation}
    \vx_n = \vx_0 + \oiP_{\vsK_n(\widehat{\oiA},\widehat{\vr}_0)+\vsU}^\MR
      (\vx - \vx_0)
    \label{eq:rec:def:mr-aug:x}
  \end{equation}
  or, equivalently,
  \[
    \nrm{\vb - \oiA\vx_n}
    = \min_{\vz\in\vx_0 + \vsK_n(\widehat{\oiA},\widehat{\vr}_0)+\vsU}
      \nrm{\vb - \oiA\vz}.
  \]
  The statement also holds for the first variant of the deflated MINRES method
  in the case of a self-adjoint operator $\oiA$.
\end{cor}

In~\cite{Stu96}, de Sturler introduced the GCRO method which is a nested
Krylov subspace method with an outer GCR method~\cite{Elm82,EisES83} and an
inner GMRES method. Inside each iteration of the outer GCR method, a special
subspace $\vsU$ is determined which is then used as a deflation space in the
deflated GMRES method (first variant). In the setting of GCRO, the implicit
augmentation of the deflated GMRES method has already been shown
in~\cite[theorem~2.2]{Stu96}. That the augmentation is also carried out in
general has been shown in~\cite{GauGLN13} thanks to the equivalence
theorem~\ref{thm:rec:eq:aug-def}.

Wang, de Sturler and Paulino introduced the RMINRES method~\cite{WanSP07} in
order to solve sequences of linear systems with self-adjoint operators.
Essentially, the RMINRES method consists of two main parts that can be
analyzed separately: an augmented and deflated MINRES method which is based on
the GCRO method and an extraction procedure for harmonic Ritz vectors. Here, the
augmented and deflated MINRES method is called the \emph{solver part} of
the RMINRES method. The solver part of the RMINRES method is presented as a
rather intricate algorithm~\cite[algorithm~2]{WanSP07} and essentially seeks to
find the approximate solution $\vx_n$ with minimal residual norm from an
augmented and deflated Krylov subspace, i.e., the approximate
solution~\eqref{eq:rec:def:mr-aug:x}. Corollary~\ref{cor:rec:def:mr-aug} states
that this can be achieved by simply applying the standard
MINRES algorithm to a deflated linear system, cf.\ the first variant of deflated
MINRES in corollary~\ref{cor:rec:def:minres}.

\begin{rmk}
  Note that the second variant of the deflated GMRES and MINRES method (see
  theorem~\ref{thm:rec:def:gmres1} and corollary~\ref{cor:rec:def:minres}) does
  not fit into the equivalence theorem because the orthogonality condition then
  reads
  \[
    \oiP_{\vsU^\perp,\oiA\vsU}(\vb-\oiA\widehat{\vx}_n)
      \perp \oiP_{\vsU^\perp,\oiA\vsU}\oiA \vsK_n(\widehat{\oiA},\widehat{\vv}),
  \]
  which is in general incompatible with the orthogonality constraint in the
  second pair of conditions~\eqref{eq:rec:eq:implicit}.
\end{rmk}


\subsection{Implementation}
\label{sec:rec:def:imp}

In the previous subsection, it was shown that augmentation is implicitly
achieved by the deflated CG method and the first variant of the deflated GMRES
and MINRES methods. In these methods, the standard CG, GMRES or MINRES algorithms
are applied to a deflated linear system and the approximate solutions are
corrected afterwards. Regarding the implementation, such a pure deflated Krylov
subspace method has clear advantages because the algorithm of the underlying
Krylov subspace method (e.g., CG, GMRES or MINRES) do not have to be modified
and one can draw on the most robust already existing algorithms and their
implementations. In terms of programming, deflation can thus be seen as a
wrapper around Krylov subspace methods.

For all deflated methods that were discussed above, it is also
possible to start with a corrected initial guess and use a projection as a right
``preconditioner'' which makes the post-correction superfluous. The difference
is only of algorithmic nature and is described briefly for the deflated
methods discussed in this thesis. First, the concept of right preconditioning is
recapitulated.

For a linear system $\oiA\vx=\vb$ and an invertible linear operator
$\oiM\in\vsL(\vsH)$, the right preconditioned system $\oiA\oiM\vy=\vb$ can be
solved for $\vy$ and then the original solution can be obtained from
$\vx=\oiM\vy$. Instead of $\vx_0$, the initial guess $\vy_0\DEF\oiM^\inv\vx_0$
is used implicitly and the initial residual
$\vr_0=\vb-\oiA\oiM\vy_0=\vb-\oiA\vx_0$ equals the residual of the
unpreconditioned system. For the CG, GMRES and MINRES methods, iterates of the
form
\[
  \vy_n=\vy_0+\vz_n
  \quad\text{with}\quad
  \vz_n\in\vsK_n(\oiA\oiM,\vr_0)
\]
and $\vx_n\DEF \oiM\vy_n = \vx_0 + \oiM\vz_n$ are constructed such that
$\nrm[\oiA]{\vx-\vx_n}$ or $\nrm{\vb-\oiA\vx_n}$ are minimal. For
well-definedness, the preconditioned operator $\oiA\oiM$ has to be self-adjoint
and positive definite for the CG method and self-adjoint for the MINRES method.
Note that $\vy_0$ is not needed and will never be computed explicitly.

Assume that two projections $\oiQ,\oiP\in\vsL(\vsH)$ are given such that
$\oiQ\oiA=\oiA\oiP$ and let CG, GMRES or MINRES be well defined when applied to
the projected linear system $\widehat{\oiA}\widehat{\vx}=\widehat{\vb}$ with
$\widehat{\oiA}=\oiQ\oiA$ and $\widehat{\vb}=\oiQ\vb$. The cases that are of
interest here are:
\begin{enumerate}
  \item $\oiQ=\oiQ_\vsU^\CG$ and $\oiP=\oiP_{\vsU^\perp}^\CG$ (with the CG,
    GMRES or MINRES method).
  \item $\oiQ=\oiQ_\vsU^\MR$ and $\oiP=\oiP_{\vsU^\perp}^\MR$ (with the GMRES or
    MINRES method).
\end{enumerate}
For an initial guess $\vx_0$ with corresponding initial residual
$\widehat{\vr}_0=\widehat{\vb}-\widehat{\oiA}\vx_0$ and $n\leq
d(\widehat{\oiA},\widehat{\vr}_0)$, iterates
$\widehat{\vx}_n\in\vx_0+\widehat{\vsK}_n$ are constructed where
$\widehat{\vsK}_n\DEF\vsK_n(\widehat{\oiA},\widehat{\vr}_0)$.
If $\vtV_n\in\vsH^n$ with $\Span{\vtV_n}=\widehat{\vsK}_n$, then the iterates
satisfy
\[
  \widehat{\vx}_n=\vx_0 + \vtV_n\ip{\oiB\vtV_n}{\widehat{\oiA}\vtV_n}^\inv
  \ip{\oiB\vtV_n}{\widehat{\oiA}(\vx-\vx_0)}
\]
with $\oiB=\id$ for the CG method and $\oiB=\widehat{\oiA}$ for the GMRES and
MINRES methods. The iterates then have to be corrected according to
\[
  \vx_n = \oiP\widehat{\vx}_n + (\id-\oiP)\vx.
\]

Now a closer look is taken at right preconditioning with $\oiM=\oiP$ which
differs from the above description because $\oiP$ is singular in general.
However, even if the right preconditioned system
\begin{equation}
  \oiA\oiP\tilde{\vy} = \vb
  \label{eq:rec:def:imp:right}
\end{equation}
is not consistent (i.e., $\vb\notin\vsR(\oiA\oiP)$), the right preconditioning
strategy can be used to solve the original linear system if the initial guess
\begin{equation}
  \tilde{\vx}_0 \DEF \oiP\vx_0 + (\id-\oiP)\vx
  \label{eq:rec:def:imp:x0}
\end{equation}
is used. The key issues are that $\oiA\oiP=\widehat{\oiA}$ and that the initial
residual for the right preconditioned system is computed as
\[
  \tilde{\vr}_0
  = \vb-\oiA\tilde{\vx}_0
  = \vb - \oiA\oiP\vx_0 - \oiA(\id-\oiP)\vx
  = \vb - \oiQ\oiA\vx_0 - (\id - \oiQ)\oiA\vx
  = \oiQ(\vb- \oiA\vx_0)
  = \widehat{\vb} - \widehat{\oiA}\vx_0.
\]
Thus, the Krylov subspace that is generated by the CG, GMRES or MINRES method
when it is applied to the right preconditioned
system~\eqref{eq:rec:def:imp:right} with the initial
guess~\eqref{eq:rec:def:imp:x0} actually equals the Krylov subspace
$\widehat{\vsK}_n$ which is constructed in the corresponding method when
it is applied to the linear system $\widehat{\oiA}\widehat{\vx}=\widehat{\vb}$
with initial guess $\vx_0$. The methods thus are also well defined and the
generated iterates are given by
\begin{align*}
  \tilde{\vx}_n
  &= \tilde{\vx}_0 + \oiP \vtV_n\ip{\oiB\vtV_n}{\widehat{\oiA}\vtV_n}^\inv
    \ip{\oiB\vtV_n}{\widehat{\oiA}(\vx-\tilde{\vx}_0)} \\
  &= \tilde{\vx}_0 + \oiP \vtV_n\ip{\oiB\vtV_n}{\widehat{\oiA}\vtV_n}^\inv
    \ip{\oiB\vtV_n}{\oiA\oiP(\vx-\oiP\vx_0 - (\id-\oiP)\vx)} \\
  &= \tilde{\vx}_0 + \oiP \vtV_n\ip{\oiB\vtV_n}{\widehat{\oiA}\vtV_n}^\inv
    \ip{\oiB\vtV_n}{\oiA\oiP(\vx-\vx_0)} \\
  &= \tilde{\vx}_0 + \oiP \vtV_n\ip{\oiB\vtV_n}{\widehat{\oiA}\vtV_n}^\inv
    \ip{\oiB\vtV_n}{\widehat{\oiA}(\vx-\vx_0)}\\
  &= \oiP\vx_0 + (\id-\oiP)\vx
    + \oiP \vtV_n\ip{\oiB\vtV_n}{\widehat{\oiA}\vtV_n}^\inv
    \ip{\oiB\vtV_n}{\widehat{\oiA}(\vx-\vx_0)} \\
  &= \oiP\left(\vx_0
    + \vtV_n\ip{\oiB\vtV_n}{\widehat{\oiA}\vtV_n}^\inv
    \ip{\oiB\vtV_n}{\widehat{\oiA}(\vx-\vx_0)}\right) + (\id-\oiP)\vx \\
  &= \oiP\widehat{\vx}_n + (\id-\oiP)\vx.
\end{align*}

\begin{table}
  \centering
  \begin{tabular}{l>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}cc}
    \toprule
    Method & \multicolumn{2}{c}{Projections}
      & Initial guess & Final iterate\\
    \cmidrule(r){2-3}
      & $\oiP_l$ & $\oiP_r$ & $\overline{\vx}_0$ & $\vx_n$ \\
    \midrule
      \multirow{2}{*}{CG (theorem~\ref{thm:rec:def:cg})}
        & $\oiQ_\vsU^\CG$  & -- & $\vx_0$ & $c^\CG(\overline{\vx}_n)$ \\
        & -- & $\oiP_{\vsU^\perp}^\CG$ & $c^\CG(\vx_0)$ & $\overline{\vx}_n$ \\
    \midrule
      GMRES/MINRES variant 1
        & $\oiQ_\vsU^\MR$  & -- & $\vx_0$ & $c^\MR(\overline{\vx}_n)$ \\
      (theorem~\ref{thm:rec:def:gmres1}, corollary~\ref{cor:rec:def:minres})
        & -- & $\oiP_{\vsU^\perp}^\MR$ & $c^\MR(\vx_0)$ & $\overline{\vx}_n$ \\
    \midrule
      GMRES/MINRES variant 2
        & $\oiQ_\vsU^\CG$  & -- & $\vx_0$ & $c^\CG(\overline{\vx}_n)$ \\
      (theorem~\ref{thm:rec:def:gmres2}, corollary~\ref{cor:rec:def:minres})
        & -- & $\oiP_{\vsU^\perp}^\CG$ & $c^\CG(\vx_0)$ & $\overline{\vx}_n$ \\
    \bottomrule
  \end{tabular}
  \caption{Overview of well-defined deflated Krylov subspace methods, see
    corollary~\ref{cor:rec:def:imp:overview}. Implementations can be found
    in~\cite{krypy} under
    \lstinline{krypy.deflation.Deflated\{Cg,Minres,Gmres\}}.}
  \label{tab:rec:def:imp:overview}
\end{table}

The equivalent methods are gathered in table~\ref{tab:rec:def:imp:overview} and
the following corollary.

\begin{cor}[Overview of well-defined deflated Krylov subspace methods]
  \label{cor:rec:def:imp:overview}
  Let a linear system $\oiA\vx=\vb$ with a nonsingular operator $\oiA$ and
  right hand side and initial guess $\vb,\vx_0\in\vsH$ be
  given. Furthermore, let a deflation
  space basis $\vtU\in\vsH^m$ be given such that $\vsU=\Span{\vtU}$ is
  $m$-dimensional with $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$ and
  let $c^\CG,c^\MR:\vsH\lra\vsH$ be defined by
  \begin{align*}
    c^\CG(\vz) &= \oiP_{\vsU^\perp}^\CG \vz + \vtU\ip{\vtU}{\oiA\vtU}^\inv
      \ip{\vtU}{\vb} \\
      \text{and}\qquad
    c^\MR(\vz) &= \oiP_{\vsU^\perp}^\MR \vz + \vtU\ip{\oiA\vtU}{\oiA\vtU}^\inv
      \ip{\oiA\vtU}{\vb}.
  \end{align*}

  Considering table~\ref{tab:rec:def:imp:overview}, the listed Krylov subspace
  methods are well defined when they are applied to the deflated linear system
  \[
    \oiP_l\oiA\oiP_r\vy=\oiP_l\vb
  \]
  with initial guess $\overline{\vx}_0$ and yield iterates $\overline{\vx}_n$
  from which the final approximation $\vx_n$ can be obtained. The operator
  $\oiA$ has to be self-adjoint and positive definite for the CG method and
  self-adjoint for the MINRES method. The right preconditioner $\oiP_r$ is
  treated as described in section~\ref{sec:rec:def:imp}.
\end{cor}

\begin{algorithm}[htbp]
  \begin{algorithmic}[1]
    \Require For $i\in\{1,\ldots,M\}$, the following is assumed to be given
      for solving the sequence of linear systems~\eqref{eq:rec:seq}:
    \begin{itemize}[noitemsep]
      \item $\oiA^{(i)}\in\vsL(\vsH)$
        \Comment operator
      \item $\vb^{(i)},\vx_0^{(i)}\in\vsH$
        \Comment right hand side and initial guess
      \item $\vtZ^{(i)}\in\vsH^{l^{(i)}}$ for $l^{(i)}\in\N$
        \Comment auxiliary deflation vectors (may be empty)
    \end{itemize}
    \State $\vtW^{(1)} = [~] \in\vsH^0$
      \Comment no deflation vectors can be determined in the first step
    \For{$i=1,\ldots,M$}
      \If{$i>1$}
        \State \algmulti[2]{Choose
          $\vtW^{(i)}\in\left(\vsK^{(i-1)}+\Span{\vtU^{(i-1)}}\right)^{k^{(i)}}$
          for a $k^{(i)}\in\N$.

          \begin{tabularx}{\linewidth}{|X}
            Only information from the previous linear system is used, e.g., the
            Arnoldi relation for the Krylov subspace $\vsK^{(i-1)}$,
            $\vtU^{(i-1)}$ and characterizations of the differences
            $\oiA^{(i)}-\oiA^{(i-1)}$, $\vb^{(i)}-\vb^{(i-1)}$ and
            $\vx_0^{(i)}-\vx_0^{(i-1)}$.

            The selection of such vectors is discussed in
            section~\ref{sec:rec:sel}.
          \end{tabularx}
          }
          \label{alg:rec:def:seq:sel}
      \EndIf
      \State $\vtU^{(i)}=[\vtW^{(i)},\vtZ^{(i)}]$
        \Comment new deflation subspace
      \If{$\ip{\vtU^{(i)}}{\oiA^{(i)}\vtU^{(i)}}$ is singular}
        \Comment solvability condition
          $\theta(\vsU,\oiA^{(i)}\vsU)<\frac{\pi}{2}$
          \label{alg:rec:def:seq:safe}
        \State \algmulti[2]{$\vtU^{(i)} \gets \vtU^{(i)}\ofX$

          \begin{tabularx}{\linewidth}{|X}
            Choose $\ofX\in\C^{k^{(i)}+l^{(i)},m}$ such that
            $\ip{\vtU^{(i)}}{\oiA^{(i)}\vtU^{(i)}}$ is nonsingular.
          \end{tabularx}
        }
      \EndIf
      \State \algmulti[1]{Solve deflated linear system to given tolerance with
        one of the methods in table~\ref{tab:rec:def:imp:overview} and the
        deflation space $\vsU=\Span{\vtU^{(i)}}$.

        \begin{tabularx}{\linewidth}{|X}
          The underlying Krylov subspace method is applied to
          $\oiP_l\oiA^{(i)}\vx^{(i)}=\oiP_l\vb^{(i)}$ or
          $\oiA^{(i)}\oiP_r\vy^{(i)}=\vb^{(i)}$ with the appropriate correction
          of the initial guess or approximate solution. In the course of the
          method, an Arnoldi relation for the Krylov subspace
          $\vsK^{(i)}=\vsK_{n^{(i)}}(\oiP_l\oiA^{(i)},\oiP_l\vr_0^{(i)})$ is
          constructed with $\vr_0^{(i)}=\vb^{(i)}-\oiA^{(i)}\vx_0^{(i)}$.
        \end{tabularx}
        }
    \EndFor
  \end{algorithmic}
  \caption{Prototype of recycling Krylov subspace methods for sequences of
    linear systems. Implemented as
    \lstinline{krypy.recycling.Recycling\{Cg,Minres,Gmres\}} in \cite{krypy}.}
  \label{alg:rec:def:seq}
\end{algorithm}

In a brief algorithmic interlude, the situation of a sequence of linear
systems~\eqref{eq:rec:seq}
is considered here again. In some cases, good candidates for deflation subspaces
are known for the problem at hand (see chapter~\ref{ch:nls}). However, in
general, the goal is to determine and improve suitable deflation subspaces on
the fly while solving the linear systems one after another. A prototype
algorithm for the solution of the sequence of linear systems is given in
algorithm~\ref{alg:rec:def:seq}. In each iteration, the deflation vectors
$\vtU^{(i)}$ are chosen as the auxiliary vectors $\vtZ^{(i)}$ and a set of
vectors from the previous Krylov subspace $\vsK^{(i-1)}$ and the span of the
previous deflation vectors $\vtU^{(i-1)}$.  The algorithm deliberately leaves
open the question of how to exactly choose the deflation subspace for the next
linear system in line~\ref{alg:rec:def:seq:sel}. Selection strategies for the
deflation subspace are the subject of section~\ref{sec:rec:sel}.
Algorithm~\ref{alg:rec:def:seq} can be seen as a straightforward building block
for the strategies that are about to be outlined later in this thesis.
Algorithms along the lines of algorithm~\ref{alg:rec:def:seq} have been used
without the safety check in line~\ref{alg:rec:def:seq:safe}, e.g., by Kilmer and
de Sturler~\cite{KilS06}, Parks et al.~\cite{ParSMJM06} and the author and
Schl\"{o}mer~\cite{GauS13}.


\section{Perturbation theory}
\label{sec:rec:per}

The goal of this subsection is to investigate the convergence behavior of
the deflated Krylov subspace methods that have been presented in
section~\ref{sec:rec:def}. For a given deflation subspace $\vsU\subseteq\vsH$
with $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$, all of the presented deflated
methods essentially consist of the application of the CG, GMRES or MINRES method
to a projected linear system
\[
  \oiP\oiA\widehat{\vx} = \oiP\vb
\]
with $\oiP\in\{\oiP_{\vsU^\perp,\oiA\vsU},\oiP_{(\oiA\vsU)^\perp}\}$. For a
self-adjoint and positive-definite operator $\oiA$ and the projection
$\oiP=\oiP_{\vsU^\perp,\oiA\vsU}$, it was shown by Vuik, Nabben and
Tang~\cite{VuiNT06} that the effective condition number $\kappa_\eff(\oiP\oiA)$
(see definition~\ref{def:back:effcond}) satisfies\footnote
 {It was stated in~\cite{VuiNT06,TanNVE09} that $\kappa_\eff(\oiP\oiA) <
 \kappa(\oiA)$.  The example $\oiA=\ofI_2$ and $\vsU=\Span{\ve_1}$, where
 $\kappa_\eff(\oiP\oiA)=\kappa(\oiA)=1$, shows that the strict inequality does
 not hold in general. However, inequality~\eqref{eq:rec:per:effcond} is true
 which can be seen easily from corollary~\ref{cor:rec:per:def:spectrum} and the
 Cauchy interlacing property.
}
\begin{equation}
  \kappa_\eff(\oiP\oiA) \leq \kappa(\oiA)
  \label{eq:rec:per:effcond}
\end{equation}
for all deflation spaces $\vsU\neq\vsH$. This implies that the asymptotic
$\kappa$-bound for the CG method (cf.\ theorem~\ref{thm:back:cg-kappa}) cannot
grow for any deflation space $\vsU$. The effective condition number
$\kappa_\eff(\oiP\oiA)$ can be quantified easily if $\vsU$ is $\oiA$-invariant.
Assume that the spectrum of $\oiA$ is
$\Lambda(\oiA)=\{\lambda_1,\ldots,\lambda_N\}$ and that $\vsU$ is an
$m$-dimensional invariant subspace associated with the eigenvalues
$\{\lambda_1,\ldots,\lambda_m\}$. Then the spectrum of $\oiP\oiA$ is
\[
  \Lambda(\oiP\oiA) = \{0,\lambda_{m+1},\ldots,\lambda_N\}
\]
and the effective condition number of $\oiP\oiA$ thus is
\[
  \kappa_\eff(\oiP\oiA) = \frac{\max_{i\in\{m+1,\ldots,N\}}\lambda_i}
    {\min_{i\in\{m+1,\ldots,N\}}\lambda_i}.
\]
Of course, an exact $\oiA$-invariant subspace is not available in practice but
only approximations, e.g., the span of Ritz or harmonic Ritz vectors, cf.\
section~\ref{sec:back:ritz}. In the deflation and augmentation
literature~\cite{EieES00,SaaYEG00}, it is sometimes argued that an almost
$\oiA$-invariant subspace will not result in substantial differences of the
spectrum or the convergence behavior.

To the knowledge of the author, no meaningful quantifications of the impact of
deflation space perturbations on the spectrum of the deflated operator or the
convergence behavior of deflated methods have been established in the
literature. When studying perturbations for deflated Krylov subspace methods,
several topics that are interesting and partly unexplored themselves lie along
the way. These topics include perturbation theory for projections, deflated
operators and Krylov subspace methods. The theory of this section is also
of importance in section~\ref{sec:rec:sel} which deals with the determination of
deflation spaces in the setting of a sequence of linear systems.


\subsection{Projections}
\label{sec:rec:per:proj}

This subsection complements the results on projections and angles between
subspaces from sections~\ref{sec:back:proj} and \ref{sec:back:ang} with a
new perturbation result for projections. In this subsection, the Hilbert space
$\vsH$ may be infinite-dimensional. Given two projections
$\oiP,\oiQ\in\vsL(\vsH)$, the question is: how can the norm
\[
  \nrm{\oiP-\oiQ}
\]
be characterized? For special cases, this problem has been addressed in the
literature and a brief overview is given here. Since a projection is uniquely
defined by its range and null space, it is assumed that four closed and nonzero
subspaces $\vsV,\vsW,\vsX,\vsY\subseteq\vsH$ are given such that
$\vsV\oplus\vsW=\vsX\oplus\vsY=\vsH$. Then with $\oiP=\oiP_{\vsV,\vsW}$ and
$\oiQ=\oiP_{\vsX,\vsY}$ the task is to express or bound
\begin{equation}
  \nrm{\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY}}
  \label{eq:rec:per:proj:diff}
\end{equation}
in terms of angles between the involved subspaces. Note that this subsection
makes heavy use of the statements on projections in section~\ref{sec:back:proj}
and on angles and gaps between subspaces in section~\ref{sec:back:ang}.

If both projections are orthogonal, i.e., $\vsW=\vsV^\perp$ and
$\vsY=\vsX^\perp$, then the norm of the difference is by
definition~\ref{def:maxangle} the sine of the maximal canonical angle between
$\vsV$ and $\vsX$:
\[
  \nrm{\oiP_\vsV - \oiP_\vsX}
  = \sin \theta_{\max}(\vsV,\vsX)
  = \Theta(\vsV,\vsX).
\]

With $\vsW=\vsY$, a more interesting situation was considered by
Berkson~\cite{Ber63} in the more general setting of a Banach space. In the
situation of a Hilbert space and with the notation used in this thesis, he
showed in~\cite[theorem~5.2]{Ber63} that
\begin{equation}
  \nrm{\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsW}}
  \leq \nrm{\oiP_{\vsV,\vsW}} \frac{\mu}{1-\mu}
  \quad\text{with}\quad
  \mu = \nrm{\oiP_{\vsV,\vsW}} \Theta(\vsV,\vsX)
  \label{eq:rec:per:proj:Ber}
\end{equation}
holds if $\mu<1$. The terms in the right hand side of
inequality~\eqref{eq:rec:per:proj:Ber} can be expressed in terms of angles
between the involved subspaces by using the identities from lemma~\ref{lem:proj}
and definition~\ref{def:maxangle}:
\begin{equation*}
  \nrm{\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsW}}
  \leq \frac{1}{\cos\theta_{\max}(\vsV,\vsW^\perp)} \frac{\mu}{1-\mu}
  \quad\text{with}\quad
  \mu =  \frac{\sin\theta_{\max}(\vsV,\vsX)}{\cos\theta_{\max}(\vsV,\vsW^\perp)}.
\end{equation*}

Dirr, Rako{\v{c}}evi{\'c} and Wimmer sharpened the bound
in~\cite[theorem~3.1]{DirRW05} by showing that
\begin{equation}
  \nrm{\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsW}}
  \leq \nrm{\oiP_{\vsV,\vsW}} \frac{\eta}{1-\eta}
  \quad\text{with}\quad
  \eta = \nrm{\oiP_{\vsX^\perp}\oiP_{\vsV,\vsW}}
  \label{eq:rec:per:proj:DirRW}
\end{equation}
holds if $\eta<1$. In general, $\eta$ cannot be directly represented in terms of
angles and is often not available in practice. However, $\eta$ can be bounded by
\[
  \eta
  = \nrm{\oiP_{\vsX^\perp}\oiP_{\vsV,\vsW}}
  = \nrm{\oiP_{\vsX^\perp}\oiP_\vsV\oiP_{\vsV,\vsW}}
  \leq \nrm{\oiP_{\vsX^\perp}\oiP_\vsV}\nrm{\oiP_{\vsV,\vsW}}
  = \nrm{\oiP_{\vsV,\vsW}}\Theta(\vsV,\vsX)
  = \mu
\]
which leads to the bound~\eqref{eq:rec:per:proj:Ber} of Berkson. This relation
between the bounds has already been established in~\cite{DirRW05}. Though
sharper, the bound~\eqref{eq:rec:per:proj:DirRW} is not considered in the
following for the above reason.

In this subsection, a bound for the general problem~\eqref{eq:rec:per:proj:diff}
with four independent subspaces $\vsV,\vsW,\vsX,\vsY\subseteq\vsH$ satisfying
$\vsV\oplus\vsW=\vsX\oplus\vsY=\vsH$ is presented. The bound appears to be new
and it is shown that the new bound is sharper than Berkson's
bound~\eqref{eq:rec:per:proj:Ber} in the special case $\vsW=\vsY$. As a
preparation for the proof of the bound in the upcoming
theorem~\ref{thm:rec:per:proj:diff}, the following lemma is stated.

\begin{lemma}
  \label{lem:rec:per:proj:prod}
  Let $\vsV,\vsW,\vsX,\vsY\subseteq\vsH$ be closed and nonzero subspaces with
  $\vsV\oplus\vsW=\vsX\oplus\vsY=\vsH$.  Then
  \begin{align*}
    \nrm{\oiP_{\vsV,\vsW} \oiP_{\vsX,\vsY}}
    \leq \frac{\cos \theta_{\min}(\vsW^\perp,\vsX)}
      {\cos \theta_{\max}(\vsV,\vsW^\perp)
      \cos \theta_{\max}(\vsX,\vsY^\perp) }.
  \end{align*}
\end{lemma}

\begin{proof}
  The lemma follows from statements~\ref{lem:proj:orth}, \ref{lem:proj:orth2}
  and \ref{lem:proj:norm} of lemma~\ref{lem:proj} and
  statement~\ref{lem:orthproj:prod} of lemma~\ref{lem:orthproj}:
  \begin{align*}
    \nrm{\oiP_{\vsV,\vsW} \oiP_{\vsX,\vsY}}
    &= \nrm{\oiP_{\vsV,\vsW} \oiP_{\vsW^\perp} \oiP_\vsX
      \oiP_{\vsX,\vsY}}
    \leq \nrm{\oiP_{\vsV,\vsW}} \nrm{\oiP_{\vsW^\perp} \oiP_\vsX}
      \nrm{\oiP_{\vsX,\vsY}} \\
    &= \frac{\cos \theta_{\min}(\vsW^\perp,\vsX)}
      {\cos \theta_{\max}(\vsV,\vsW^\perp)
      \cos \theta_{\max}(\vsX,\vsY^\perp) }.
  \end{align*}
\end{proof}

The bound in lemma~\ref{lem:rec:per:proj:prod} is sharp. For an arbitrary
Hilbert space $\vsH$ with $\dim\vsH\geq 2$ (for $\dim\vsH<2$ one of the involved
subspaces has to be zero) this can be seen by choosing four vectors
$\vv,\vw,\vx,\vy\in\vsH$ of norm $1$ with $\ip{\vv}{\vw}\neq 0$ and
$\ip{\vx}{\vy}\neq 0$. Let the four subspaces be defined by $\vsV=\Span{\vv}$,
$\vsW=\Span{\vw}^\perp$, $\vsX=\Span{\vx}$ and $\vsY=\Span{\vy}^\perp$.
With $\vz\in\vsH$ and theorem~\ref{thm:back:proj_sing}, the projections can then
be represented by
\[
  \oiP_{\vsV,\vsW}\vz=\vv \frac{\ip{\vw}{\vz}}{\ip{\vw}{\vv}}
  \qquad\text{and}\qquad
  \oiP_{\vsX,\vsY}\vz=\vx \frac{\ip{\vy}{\vz}}{\ip{\vy}{\vx}}.
\]
Then
\begin{align*}
  \nrm{\oiP_{\vsV,\vsW}\oiP_{\vsX,\vsY}}
  &= \nrm{\oiP_{\vsV,\vsW}\oiP_{\vsX,\vsY} \vy}
  = \frac{1}{\left|\ip{\vy}{\vx}\right| }
    \nrm{\oiP_{\vsV,\vsW} \vx}
  =
    \frac{\left|\ip{\vw}{\vx}\right|}
    {\left|\ip{\vw}{\vv}\right| \left|\ip{\vy}{\vx}\right|} \\
  &= \frac{\cos\theta_{\min}(\vsW^\perp,\vsX)}
    {\cos \theta_{\min}(\vsV,\vsW^\perp)\cos \theta_{\min}(\vsX,\vsY^\perp)}.
\end{align*}
At this point, statement~\ref{lem:orthproj:onedim} of lemma~\ref{lem:orthproj}
(because $\dim\vsW^\perp=1$) and statement~\ref{lem:proj:mintomax} of
lemma~\ref{lem:proj} (because $\vsH=\vsV\oplus\vsW$) can be applied in order to
obtain
\[
  \cos \theta_{\min} (\vsV,\vsW^\perp)
  = \sin \theta_{\min} (\vsV,\vsW)
  = \cos \theta_{\max} (\vsV,\vsW^\perp).
\]
An analogous argument shows that $\cos \theta_{\min} (\vsX,\vsY^\perp) =
\cos \theta_{\max} (\vsX,\vsY^\perp)$ and the bound is attained because
\[
  \nrm{\oiP_{\vsV,\vsW}\oiP_{\vsX,\vsY}}
  = \frac{\cos\theta_{\min}(\vsW^\perp,\vsX)}
    {\cos \theta_{\max}(\vsV,\vsW^\perp)
    \cos \theta_{\max}(\vsX,\vsY^\perp)
  }.
\]

Now this section's main result can be shown. In order to serve the applications
in later sections, the theorem bounds
$\nrm{\oiL(\oiP_{\vsV,\vsW}-\oiP_{\vsX,\vsY})}$, where $\oiL\in\vsL(\vsH)$ is an
arbitrary linear operator. Obviously, a bound for the original problem
$\nrm{\oiP_{\vsV,\vsW}-\oiP_{\vsX,\vsY}}$ results with $\oiL=\id$.

\begin{thm}
  \label{thm:rec:per:proj:diff}
  Let $\vsV,\vsW,\vsX,\vsY\subseteq\vsH$ be closed and nonzero subspaces with
  $\vsV\oplus\vsW=\vsX\oplus\vsY=\vsH$ and let $\oiL\in\vsL(\vsH)$. Then
  \begin{align*}
    \nrm{\oiL(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})}
    \leq \frac{\nrm{\restr{\oiL}{\vsV}} \cos \theta_{\min} (\vsW^\perp,\vsY)
      +  \nrm{\restr{\oiL}{\vsW}} \cos \theta_{\min} (\vsV^\perp,\vsX)}
      {\cos \theta_{\max}(\vsV,\vsW^\perp)
      \cos \theta_{\max}(\vsX,\vsY^\perp)}.
  \end{align*}
\end{thm}

\begin{proof}
  The theorem is proved by decomposing the identity operator as
  $\id = \oiP_{\vsV,\vsW}+\oiP_{\vsW,\vsV}$:
  \begin{align*}
    \nrm{\oiL (\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})}
    =& \nrm{\oiL \oiP_{\vsV,\vsW}(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})
      + \oiL \oiP_{\vsW,\vsV}(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY}) } \\
    =& \nrm{\oiL \oiP_{\vsV,\vsW}(\id - \oiP_{\vsX,\vsY})
      - \oiL \oiP_{\vsW,\vsV} \oiP_{\vsX,\vsY} }\\
    &= \nrm{\oiL \oiP_{\vsV,\vsW}\oiP_{\vsY,\vsX}
      - \oiL \oiP_{\vsW,\vsV} \oiP_{\vsX,\vsY} } \\
    \leq& \nrm{\oiL \oiP_{\vsV,\vsW}\oiP_{\vsY,\vsX}}
      + \nrm{\oiL \oiP_{\vsW,\vsV} \oiP_{\vsX,\vsY} } \\
    =& \nrm{\oiL \oiP_\vsV \oiP_{\vsV,\vsW}\oiP_{\vsY,\vsX}}
      + \nrm{\oiL \oiP_\vsW \oiP_{\vsW,\vsV} \oiP_{\vsX,\vsY} } \\
    \leq& \nrm{\oiL \oiP_\vsV} \nrm{\oiP_{\vsV,\vsW}\oiP_{\vsY,\vsX}}
      + \nrm{\oiL \oiP_\vsW} \nrm{\oiP_{\vsW,\vsV} \oiP_{\vsX,\vsY} } \\
    =& \nrm{\restr{\oiL}{\vsV} } \nrm{\oiP_{\vsV,\vsW}\oiP_{\vsY,\vsX}}
      + \nrm{\restr{\oiL}{\vsW}} \nrm{\oiP_{\vsW,\vsV} \oiP_{\vsX,\vsY} }
      \\
    \leq&
      \frac{\nrm{\restr{\oiL}{\vsV}} \cos \theta_{\min} (\vsW^\perp,\vsY)}
      {\cos \theta_{\max} (\vsV,\vsW^\perp)
      \cos \theta_{\max} (\vsY,\vsX^\perp) } \\
    &+
      \frac{\nrm{\restr{\oiL}{\vsW}} \cos \theta_{\min} (\vsV^\perp,\vsX)}
      {\cos \theta_{\max} (\vsW,\vsV^\perp)
      \cos \theta_{\max} (\vsX,\vsY^\perp) } \\
    =&
      \frac{\nrm{\restr{\oiL}{\vsV}} \cos \theta_{\min} (\vsW^\perp,\vsY)
      +  \nrm{\restr{\oiL}{\vsW}} \cos \theta_{\min} (\vsV^\perp,\vsX)}
      {\cos \theta_{\max} (\vsV,\vsW^\perp)
      \cos \theta_{\max} (\vsX,\vsY^\perp)}.
  \end{align*}
\end{proof}

The bound in theorem~\ref{thm:rec:per:proj:diff} is attained for special choices
of the subspaces where some of the terms disappear. However, it is shown that
the bound can be attained up to a factor for a rather technical construction of
subspaces.

Let $\oiL\in\vsL(\vsH)$ have a bounded inverse and let
$\vsZ_1,\vsZ_2,\vsZ_3\subseteq\vsH$ be closed subspaces such that
$\vsH=\vsZ_1\oplus\vsZ_2\oplus\vsZ_3$, $\dim\vsZ_3 = 2$ and
$\vsZ_i\perp\vsZ_j$ for $i,j\in\{1,2,3\}$ and $i\neq j$. Furthermore, let
$\vv,\vw,\vx,\vy\in\vsZ_3$ such that
$\vsZ_3=\Span\vv\oplus\Span\vx=\Span\vw\oplus\Span\vy$ and
$\ip{\vv}{\vw},\ip{\vx}{\vy}\neq 0$. Then the following subspaces are defined:
\begin{align}
  \begin{aligned}
    \vsV&\DEF\vsZ_1\oplus\Span\vv, & \vsW&\DEF(\vsZ_1\oplus\vw)^\perp,\\
    \vsX&\DEF(\vsZ_2\oplus\Span\vx)^\perp & \text{and}\quad\vsY&\DEF\vsZ_2\oplus\vy.
  \end{aligned}
  \label{eq:sharp:spaces}
\end{align}
For this constellation of subspaces, it is shown that
\begin{align}
  \nrm{\oiL(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})}
  &\leq \frac
    {\nrm{\restr{\oiL}{\vsV}} \cos \theta_{\min}(\vsW^\perp,\vsY)
    + \nrm{\restr{\oiL}{\vsW}}\cos \theta_{\min}(\vsV^\perp,\vsX)}
    {\cos \theta_{\max}(\vsV,\vsW^\perp)
    \cos \theta_{\max}(\vsX,\vsY^\perp)}
    \label{eq:rec:per:proj:diffsharp1}
    \\
  &\leq 2 \kappa(\oiL) \nrm{\oiP_{\vsV,\vsW}}
    \nrm{\oiL(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})}. \notag
\end{align}
The two summands in the right hand side of~\eqref{eq:rec:per:proj:diffsharp1}
are treated separately. With lemma~\ref{lem:projsum} it can be seen that
$\oiP_{\vsV,\vsW}$ and $\oiP_{\vsY,\vsX}$ are well defined and are given by
\begin{align*}
  \oiP_{\vsV,\vsW} = \oiP_{\vsZ_1} + \oiP_{\Span\vv,\Span\vw^\perp}
  \qquad\text{and}\qquad
  \oiP_{\vsY,\vsX} = \oiP_{\vsZ_2} + \oiP_{\Span\vy,\Span\vx^\perp}.
\end{align*}
Because of the orthogonality conditions a direct computation shows that
\begin{align*}
  \oiP_{\vsV,\vsW} \oiP_{\vsY,\vsX}
  = \oiP_{\Span\vv,\Span\vw^\perp} \oiP_{\Span\vy,\Span\vx^\perp}.
\end{align*}
In analogy to the discussion following lemma~\ref{lem:rec:per:proj:prod}, the
following equality holds:
\begin{align*}
  \nrm{\oiP_{\vsV,\vsW}\oiP_{\vsY,\vsX}}
  = \frac{\cos \theta_{\min}(\Span\vw,\Span\vy)}
    {\cos \theta_{\max}(\Span\vv,\Span\vw)
    \cos \theta_{\max}(\Span\vy,\Span\vx)}.
\end{align*}
Lemma~\ref{lem:projsum}, lemma~\ref{lem:orthproj} and
lemma~\ref{lem:gap} now yield
\begin{align*}
  \cos \theta_{\min}(\vsW^\perp,\vsY)
  &= \nrm{\oiP_{\vsW^\perp} \oiP_\vsY}
  = \nrm{(\oiP_{\vsZ_1} + \oiP_{\Span\vw})
    (\oiP_{\vsZ_2} + \oiP_{\Span\vy})}
  = \nrm{\oiP_{\Span\vw} \oiP_{\Span\vy}} \\
  &= \cos \theta_{\min}(\Span\vw,\Span\vy), \\
    \sin \theta_{\max}(\vsV,\vsW^\perp)
  &= \nrm{\oiP_\vsV - \oiP_{\vsW^\perp}}
  = \nrm{\oiP_{\vsZ_1} + \oiP_{\Span\vv}
    - \oiP_{\vsZ_1} -\oiP_{\Span\vw}} \\
  & = \nrm{\oiP_{\Span\vv} - \oiP_{\Span\vw}}
  = \sin \theta_{\max}(\Span\vv,\Span\vw), \\
  \sin \theta_{\max}(\vsX,\vsY^\perp)
  &= \sin \theta_{\max}(\vsX^\perp,\vsY)
  = \nrm{\oiP_{\vsX^\perp} - \oiP_\vsY}
  = \nrm{\oiP_{\vsZ_2} + \oiP_{\Span\vx}
    - \oiP_{\vsZ_2} - \oiP_{\Span\vy}} \\
  &= \nrm{\oiP_{\Span\vx} - \oiP_{\Span\vy}}
  = \sin \theta_{\max} (\Span\vx,\Span\vy)
\end{align*}
and thus
\begin{equation}
  \label{eq:rec:per:proj:diffsharpeq1}
  \nrm{\oiP_{\vsV,\vsW}\oiP_{\vsY,\vsX}}
  = \frac{\cos \theta_{\min}(\vsW^\perp,\vsY)}
    {\cos \theta_{\max}(\vsV,\vsW^\perp)
    \cos \theta_{\max}(\vsX,\vsY^\perp)}.
\end{equation}
The right hand side of equation~\eqref{eq:rec:per:proj:diffsharpeq1} is the
first summand in~\eqref{eq:rec:per:proj:diffsharp1} and now the second summand
is considered. Therefore, let $\wh{\vv},\wh{\vw},\wh{\vx},\wh{\vy}\in\vsZ_3$ be
nonzero such that $\wh{\vv}\perp\vv$, $\wh{\vw}\perp\vw$, $\wh{\vx}\perp\vx$ and
$\wh{\vy}\perp\vy$. The orthogonal complements of the subspaces defined in
\eqref{eq:sharp:spaces} then are
\begin{align*}
  \begin{aligned}
    \vsV^\perp&=\vsZ_2\oplus\Span{\wh{\vv}},
    & \vsW^\perp &= (\vsZ_2\oplus\Span{\wh{\vw}})^\perp, \\
      \vsX^\perp&=(\vsZ_1\oplus\Span{\wh{\vx}} )^\perp
    & \text{and}\quad\vsY^\perp &= \vsZ_1\oplus\Span{\wh{\vy}}.
  \end{aligned}
\end{align*}
Similar arguments as above lead to
\begin{align*}
  \nrm{\oiP_{\vsW,\vsV} \oiP_{\vsX,\vsY}}
  &= \nrm{\oiP_{\vsX,\vsY}^\adj \oiP_{\vsW,\vsV}^\adj}
  = \nrm{\oiP_{\vsY^\perp,\vsX^\perp} \oiP_{\vsV^\perp,\vsW^\perp}}
  = \nrm{\oiP_{\Span{\wh{\vy}},\Span{\wh{\vx}}^\perp}
    \oiP_{\Span{\wh{\vv}},\Span{\wh{\vw}}^\perp}} \\
  &= \frac{\cos \theta_{\min}(\Span{\wh{\vx}},\Span{\wh{\vv}})}
    {\cos \theta_{\max}(\Span{\wh{\vy}},\Span{\wh{\vx}})
    \cos \theta_{\max}(\Span{\wh{\vv}},\Span{\wh{\vw}})} \\
  &= \frac{\cos \theta_{\min}(\vsV^\perp,\vsX)}
    {\cos \theta_{\max}(\vsV,\vsW^\perp)
    \cos \theta_{\max}(\vsX,\vsY^\perp)}.
\end{align*}
The last line of the preceding equation is the second term
in~\eqref{eq:rec:per:proj:diffsharp1} and the following estimate results:
\begin{align*}
  \nrm{\oiL(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})}
  &\leq \frac
    {\nrm{\restr{\oiL}{\vsV}} \cos \theta_{\min}(\vsW^\perp,\vsY)
    + \nrm{\restr{\oiL}{\vsW}}\cos \theta_{\min}(\vsV^\perp,\vsX)}
    {\cos \theta_{\max}(\vsV,\vsW^\perp)
    \cos \theta_{\max}(\vsX,\vsY^\perp)}\\
  &= \nrm{\restr{\oiL}{\vsV}} \nrm{\oiP_{\vsV,\vsW}\oiP_{\vsY,\vsX}}
    + \nrm{\restr{\oiL}{\vsW}} \nrm{\oiP_{\vsW,\vsV}\oiP_{\vsX,\vsY}}\\
  &\leq \nrm{\oiL} \left( \nrm{\oiP_{\vsV,\vsW}\oiP_{\vsY,\vsX}}
    + \nrm{\oiP_{\vsW,\vsV}\oiP_{\vsX,\vsY}}\right)\\
  &= \nrm{\oiL} \left( \nrm{\oiP_{\vsV,\vsW}(\id - \oiP_{\vsX,\vsY})}
    + \nrm{\oiP_{\vsW,\vsV}(\id - \oiP_{\vsY,\vsX})}\right)\\
  &= \nrm{\oiL} \left( \nrm{\oiP_{\vsV,\vsW}(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})}
    + \nrm{\oiP_{\vsW,\vsV}(\oiP_{\vsW,\vsV} - \oiP_{\vsY,\vsX})}\right)\\
  &\leq \nrm{\oiL} \left( \nrm{\oiP_{\vsV,\vsW}} \nrm{\oiP_{\vsV,\vsW}
    - \oiP_{\vsX,\vsY}}
    + \nrm{\oiP_{\vsW,\vsV}}\nrm{\oiP_{\vsW,\vsV} - \oiP_{\vsY,\vsX}}\right)\\
  &= 2 \nrm{\oiL} \nrm{\oiP_{\vsV,\vsW}} \nrm{\oiP_{\vsV,\vsW} -
    \oiP_{\vsX,\vsY}} \\
  &\leq 2 \nrm{\oiL} \nrm{\oiP_{\vsV,\vsW}}
    \nrm{\oiL^\inv} \nrm{\oiL(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})} \\
  &= 2 \kappa(\oiL) \nrm{\oiP_{\vsV,\vsW}}
    \nrm{\oiL(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})}.
\end{align*}
In an application of theorem~\ref{thm:rec:per:proj:diff} (cf.\
lemma~\ref{lem:rec:per:def:pert}) the first projection is orthogonal, i.e.,
$\vsW=\vsV^\perp$, and thus $\nrm{\oiP_{\vsV,\vsW}}=1$.

In practice, the cosines in the numerator of the bound in
theorem~\ref{thm:rec:per:proj:diff} are often not available. In the following
corollary, the cosines are replaced with the sines of the maximal angle between
the ranges and the maximal angle between the null spaces of the projections.

\begin{cor}
  \label{cor:rec:per:proj:diff}
  Let the assumptions of theorem~\ref{thm:rec:per:proj:diff} hold. Then
  \begin{align*}
    \nrm{\oiL(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})}
    \leq \frac
      {\nrm{\restr{\oiL}{\vsV}} \sin \theta_{\max}(\vsW,\vsY)
      + \nrm{\restr{\oiL}{\vsW}} \sin \theta_{\max}(\vsV,\vsX)}
      {\cos \theta_{\max}(\vsV,\vsW^\perp)
      \cos \theta_{\max}(\vsX,\vsY^\perp)}.
  \end{align*}
\end{cor}

\begin{proof}
  By item~\ref{lem:orthproj:prod} of lemma~\ref{lem:orthproj},
  item~\ref{lem:gap:max} of lemma~\ref{lem:gap} and
  definition~\ref{def:maxangle} the statement follows from:
  \begin{align*}
    \cos \theta_{\min}(\vsW^\perp,\vsY)
      &= \nrm{\oiP_{\vsW^\perp}\oiP_\vsY}
      \leq \sin \theta_{\max}(\vsW,\vsY)\\
    \cos \theta_{\min}(\vsV^\perp,\vsX)
      &= \nrm{\oiP_{\vsV^\perp}\oiP_\vsX}
      \leq \sin \theta_{\max}(\vsV,\vsX).
  \end{align*}
  Note that equality holds if $\vsW^\perp\oplus\vsY=\vsV^\perp\oplus\vsX=\vsH$
  by item~\ref{lem:proj:mintomax} of lemma~\ref{lem:proj}.
\end{proof}

\begin{figure}[ht]
  \centering
  \input{figures/proj_bound_2d.tex}
  \caption{Illustration of corollary~\ref{cor:rec:per:proj:diff} with
    $\vsH=\R^2$, the Euclidean inner product, $\oiL=\id$ and one-dimensional
    subspaces $\vsV,\vsW,\vsX$ and $\vsY$. The projections $\oiP_{\vsV,\vsW}$
    and $\oiP_{\vsX,\vsY}$ are applied to a normalized vector $\vz$. The theorem
    guarantees that
    $\nrm{ (\oiP_{\vsV,\vsW}-\oiP_{\vsX,\vsY})\vz}
    \leq \frac{\sin\alpha + \sin\beta}
    {\cos\gamma \cos\delta}$.
    The angle $\alpha$ measures how close the null spaces $\vsW$ and $\vsY$ are
    and $\beta$ measures how close the ranges $\vsV$ and $\vsX$ are.  The angles
    $\gamma$ and $\delta$ indicate the departure of the projections from
    orthogonal projections. In this example
    $\nrm{\oiP_{\vsV,\vsW}-\oiP_{\vsX,\vsY}}\approx
    \inputraw{exp_proj_bound_2d_diff.txt}$,
    $\nrm{ (\oiP_{\vsV,\vsW}-\oiP_{\vsX,\vsY})\vz} \approx
    \inputraw{exp_proj_bound_2d_diff_z.txt}$
    and
    $\frac{\sin\alpha + \sin\beta}
    {\cos\gamma \cos\delta} \approx
    \inputraw{exp_proj_bound_2d_diff_bound.txt}$.
    % note: run figures/projnorm.m for numberzzz
  }
  \label{fig:rec:per:proj:diff}
\end{figure}

The bound in corollary~\ref{cor:rec:per:proj:diff} is visualized in
figure~\ref{fig:rec:per:proj:diff} with $\vsH=\R^2$ and the Euclidean inner
product. An interactive version of this figure is available
online\footnote{\url{http://andrenarchy.github.io/talk-2013-01-projections/}}.

\begin{rmk}
  Note that if the roles of $\vsV$ and $\vsX$ as well as $\vsW$ and $\vsY$ are
  interchanged in theorem~\ref{thm:rec:per:proj:diff}, then the bound can be
  improved to
  \begin{align*}
    \nrm{\oiL(\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsY})}
    &\leq \frac{\min\{\eta_1,\eta_2\}}
      {\cos \theta_{\max}(\vsV,\vsW^\perp)
      \cos \theta_{\max}(\vsX,\vsY^\perp)},\\
    \text{where}\qquad\eta_1&=\nrm{\restr{\oiL}{\vsV}}
      \cos \theta_{\min}(\vsW^\perp,\vsY)
      + \nrm{\restr{\oiL}{\vsW}}\cos \theta_{\min}(\vsV^\perp,\vsX)\\
    \text{and} \qquad\eta_2&=\nrm{\restr{\oiL}{\vsX}}
      \cos \theta_{\min}(\vsW,\vsY^\perp)
      + \nrm{\restr{\oiL}{\vsY}}\cos \theta_{\min}(\vsV,\vsX^\perp).
  \end{align*}
  Analogously, the same holds true for the bound in corollary~\ref{cor:rec:per:proj:diff}
  where
  \begin{align*}
    \eta_1&=\nrm{\restr{\oiL}{\vsV}}
      \sin \theta_{\max}(\vsW,\vsY)
      + \nrm{\restr{\oiL}{\vsW}}\sin \theta_{\max}(\vsV,\vsX)\\
    \text{and} \qquad\eta_2&=\nrm{\restr{\oiL}{\vsX}}
      \sin \theta_{\max}(\vsW,\vsY)
      + \nrm{\restr{\oiL}{\vsY}}\sin \theta_{\max}(\vsV,\vsX).
  \end{align*}
\end{rmk}

A remaining question is how the bound in theorem~\ref{thm:rec:per:proj:diff} or
corollary~\ref{cor:rec:per:proj:diff}
relates to the bound~\eqref{eq:rec:per:proj:Ber} by Berkson in the special case
$\vsW=\vsY$. The following lemma shows that the
bound~\eqref{eq:rec:per:proj:Ber} by Berkson is weaker than the new bounds in
theorem~\ref{thm:rec:per:proj:diff} or corollary~\ref{cor:rec:per:proj:diff}.

\begin{lemma}
  \label{lem:rec:per:proj:new2Ber}
  Let $\vsV,\vsW,\vsX\subseteq\vsH$ be closed and nonzero subspaces with
  $\vsV\oplus\vsW=\vsX\oplus\vsW=\vsH$. If
  \[
    \mu
    = \frac{\sin\theta_{\max}(\vsV,\vsX)}{\cos\theta_{\max}(\vsV,\vsW^\perp)}
    < 1,
  \]
  then
  \begin{align*}
    \nrm{\oiP_{\vsV,\vsW} - \oiP_{\vsX,\vsW}}
    &\leq \frac{\cos \theta_{\min}(\vsV^\perp,\vsX)}
      {\cos \theta_{\max}(\vsV,\vsW^\perp)
      \cos \theta_{\max}(\vsX,\vsW^\perp)}\\
    &\leq \frac{1}{\cos\theta_{\max}(\vsV,\vsW^\perp)} \frac{\mu}{1-\mu}.
  \end{align*}
\end{lemma}

\begin{proof}
  The first inequality directly follows from
  theorem~\ref{thm:rec:per:proj:diff} and only the second inequality remains to
  show. Because of item~\ref{lem:orthproj:prod} of lemma~\ref{lem:orthproj} and
  item~\ref{lem:proj:norm} of lemma~\ref{lem:proj}, the following holds:
  \begin{align}
    \frac{\cos \theta_{\min}(\vsV^\perp,\vsX)}
      {\cos \theta_{\max}(\vsV,\vsW^\perp)
      \cos \theta_{\max}(\vsX,\vsW^\perp)}
      &= \frac{
        \nrm{\oiP_{\vsV^\perp}\oiP_\vsX}
        \nrm{\oiP_{\vsX,\vsW}}}
        {\cos \theta_{\max}(\vsV,\vsW^\perp)}
      \leq \frac{\sin\theta_{\max}(\vsV,\vsX)}
        {\cos \theta_{\max}(\vsV,\vsW^\perp)}
        \nrm{\oiP_{\vsX,\vsW}}\notag \\
      &= \mu \nrm{\oiP_{\vsX,\vsW}}.\label{eq:rec:per:proj:new2Ber0}
  \end{align}
  The projection $\oiP_{\vsX,\vsW}$ can be represented by
  \begin{equation}
    \oiP_{\vsX,\vsW}
    = \oiP_{\vsX,\vsW} (\oiP_{\vsV,\vsW}+\oiP_{\vsW,\vsV})
    = \oiP_{\vsX,\vsW} \oiP_{\vsV,\vsW}
    = \restr{\oiP_{\vsX,\vsW}}{\vsV} \oiP_{\vsV,\vsW}.
    \label{eq:rec:per:proj:new2Ber1}
  \end{equation}
  With item~\ref{lem:proj:inverse} of lemma~\ref{lem:proj} the restricted
  operator $\restr{\oiP_{\vsX,\vsW}}{\vsV}:\vsV\lra\vsX$ can be expressed as
  \[
    \restr{\oiP_{\vsX,\vsW}}{\vsV}
    = \restr{\oiP_{\vsX,\vsW}}{\vsW^\perp} \restr{\oiP_{\vsW^\perp}}{\vsV}
    = \oiQ_{\vsX,\vsW}^\inv \oiQ_{\vsV,\vsW}.
  \]
  Thus, it is invertible and the inverse is given by
  \[
    \left(\restr{\oiP_{\vsX,\vsW}}{\vsV}\right)^\inv
    = \oiQ_{\vsV,\vsW}^\inv \oiQ_{\vsX,\vsW}
    = \oiP_{\vsV,\vsW}\restr{\oiP_{\vsW^\perp}}{\vsX}
    = \restr{\oiP_{\vsV,\vsW}}{\vsX}.
  \]
  Then the norm of $\restr{\oiP_{\vsX,\vsW}}{\vsV}$ can be estimated as follows:
  \begin{align*}
    \nrm{\restr{\oiP_{\vsX,\vsW}}{\vsV}}
    &= \sup_{0\neq\vv\in\vsV} \frac{\nrm{\oiP_{\vsX,\vsW}\vv}}{\nrm{\vv}}
    = \sup_{0\neq\vx\in\vsX}
      \frac
      {\nrm{\restr{\oiP_{\vsX,\vsW}}{\vsV}\restr{\oiP_{\vsV,\vsW}}{\vsX}\vx}}
      {\nrm{\oiP_{\vsV,\vsW}\vx}}
    = \sup_{0\neq\vx\in\vsX} \frac{\nrm{\vx}}{\nrm{\oiP_{\vsV,\vsW}\vx}} \\
    &= \sup_{0\neq\vx\in\vsX} \frac{\nrm{\vx}}{\nrm{\vx - \oiP_{\vsW,\vsV}\vx}}
    \leq \sup_{0\neq\vx\in\vsX} \frac{\nrm{\vx}}{\nrm{\vx} -
      \nrm{\oiP_{\vsW,\vsV}\vx}}
    \leq \sup_{0\neq\vx\in\vsX} \frac{\nrm{\vx}}{\nrm{\vx} -
      \nrm{\oiP_{\vsW,\vsV}\oiP_\vsX}\nrm{\vx}} \\
    & = \frac{1}{1-\nrm{\oiP_{\vsW,\vsV}\oiP_\vsX}}
    \leq \frac{1}{1-\nrm{\oiP_{\vsW,\vsV}\oiP_{\vsV^\perp}\oiP_\vsX}}
    \leq \frac{1}{1-\nrm{\oiP_{\vsW,\vsV}}\nrm{\oiP_{\vsV^\perp}\oiP_\vsX}}
    \leq \frac{1}{1-\mu}.
  \end{align*}
  This bound together with equation~\eqref{eq:rec:per:proj:new2Ber1} results in
  \begin{align*}
    \nrm{\oiP_{\vsX,\vsW}}
    &= \nrm{\restr{\oiP_{\vsX,\vsW}}{\vsV}\oiP_{\vsV,\vsW}}
    \leq \nrm{\restr{\oiP_{\vsX,\vsW}}{\vsV}}\nrm{\oiP_{\vsV,\vsW}}
    \leq \frac{1}{1-\mu} \nrm{\oiP_{\vsV,\vsW}}\\
    &\leq \frac{1}{1-\mu} \frac{1}{\cos \theta_{\max}(\vsV,\vsW^\perp)}.
  \end{align*}
  The statement of the lemma then follows with
  equation~\eqref{eq:rec:per:proj:new2Ber0}.
\end{proof}


\subsection{Spectrum of deflated operators}
\label{sec:rec:per:def}

In this subsection, the spectrum of deflated operators is analyzed for
a finite-dimensional Hilbert space $\vsH$, i.e., $N\DEF\dim\vsH<\infty$. If an
exact $\oiA$-invariant subspace $\vsV$ is used as a deflation subspace, then the
spectrum and the invariant subspaces of the deflated operator $\oiP\oiA$ are
trivial to compute for the choices of $\oiP$ that have been discussed in the
previous sections. The following theorem has been presented
in~\cite[theorem~3.3]{GauGLN13} and characterizes the spectrum and invariant
subspaces of a deflated operator with an invariant deflation subspace.

\begin{thm}
  \label{thm:rec:per:def:jordan}
  Let $\oiA\in\vsL(\vsH)$ have the Jordan decomposition
  \[
    \oiA
    = \mat{\vtS_1 & \vtS_2} \mat{\ofJ_1&0\\0&\ofJ_2}
      \mat{\widehat{\vtS}_1^\adj \\ \widehat{\vtS}_2^\adj},
  \]
  where $\vtS_1,\widehat{\vtS}_1\in\vsH^m$,
  $\vtS_2,\widehat{\vtS}_2\in\vsH^{N-m}$, $\ofJ_1\in\C^{m,m}$,
  $\ofJ_2\in\C^{N-m,N-m}$ for $m>0$ and
  $\mat{\widehat{\vtS}_1 & \widehat{\vtS}_2}^\adj=\mat{\vtS_1 & \vtS_2}^\inv$.
  If $\vsV=\Span{\vtS_1}$ and $\ofJ_1$ is nonsingular, then
  \begin{enumerate}
    \item $\oiQ_\vsV^\CG=\oiQ_\vsV^\MR = \oiP_{\vsV^\perp}$.
    \item \label{thm:rec:per:def:jordan:decomp}
      The deflated operator $\oiP_{\vsV^\perp}\oiA$ has the Jordan decomposition
      \begin{align*}
        \oiP_{\vsV^\perp}\oiA
        = \mat{\vtS_1 & \oiP_{\vsV^\perp}\vtS_2}
          \mat{0&0\\0&\ofJ_2}
          \mat{\vtS_1 & \oiP_{\vsV^\perp}\vtS_2}^\inv \\
        \text{with}\quad
          \mat{\vtS_1 & \oiP_{\vsV^\perp}\vtS_2}^\inv
          = \mat{\vtS_1\ip{\vtS_1}{\vtS_1}^\inv & \widehat{\vtS}_2}^\adj.
      \end{align*}
    \item $\Lambda(\oiP_{\vsV^\perp}\oiA) = \{0\}\cup\Lambda(\ofJ_2)$.
  \end{enumerate}
\end{thm}

\begin{proof}
  \begin{enumerate}
    \item Because $\oiA\vsV=\vsV$ the projections
      $\oiQ_\vsV^\CG=\oiP_{\vsV^\perp,\oiA\vsV}$ and
      $\oiQ_\vsV^\MR=\oiP_{(\oiA\vsV)^\perp}$ obviously equal
      $\oiP_{\vsV^\perp}$.
    \item A direct computation shows that
      \begin{align*}
        \oiP_{\vsV^\perp}\oiA
        &= \mat{0 & \oiP_{\vsV^\perp}\vtS_2} \mat{\ofJ_1&0\\0&\ofJ_2}
          \mat{\widehat{\vtS}_1^\adj \\ \widehat{\vtS}_2^\adj}
        = \oiP_{\vsV^\perp}\vtS_2 \ofJ_2 \widehat{\vtS}_2^\adj \\
        &= \mat{\vtS_1 & \oiP_{\vsV^\perp}} \mat{0&0\\0&\ofJ_2}
          \mat{\vtS_1\ip{\vtS_1}{\vtS_1}^\inv & \widehat{\vtS}_2}^\adj
      \end{align*}
      and because $\Span{\widehat{\vtS}_2}\perp\vsV$ also
      \begin{align*}
        \mat{\vtS_1\ip{\vtS_1}{\vtS_1}^\inv & \widehat{\vtS}_2}^\adj
          \mat{\vtS_1 & \oiP_{\vsV^\perp}\vtS_2}
        &= \mat{\ip{\vtS_1\ip{\vtS_1}{\vtS_1}^\inv}{\vtS_1}
          & \ip{\vtS_1\ip{\vtS_1}{\vtS_1}^\inv}{\oiP_{\vsV^\perp}\vtS_2}\\
          \ip{\widehat{\vtS}_2}{\vtS_1}
          & \ip{\widehat{\vtS}_2}{\oiP_{\vsV^\perp}\vtS_2}
        } \\
        &= \mat{\ip{\vtS_1}{\vtS_1}^\inv\ip{\vtS_1}{\vtS_1}
          & \ip{\oiP_{\vsV^\perp}\vtS_1\ip{\vtS_1}{\vtS_1}^\inv}{\vtS_2}\\
          0
          & \ip{\oiP_{\vsV^\perp}\widehat{\vtS}_2}{\vtS_2}
        } \\
        &= \ofI_N.
      \end{align*}
    \item Immediately follows from item~\ref{thm:rec:per:def:jordan:decomp}.
  \end{enumerate}
\end{proof}

Unlike the situation in the preceding theorem, usually only approximations to
invariant subspaces can be used in practice. However, things become more
complicated if only an approximation $\vsU$ to an $\oiA$-invariant subspace
$\vsV$ is used and not much is known in the literature about the spectrum of the
deflated operator $\oiP\oiA$ for $\oiP\in\{\oiQ_\vsU^\CG,\oiQ_\vsU^\MR\}$. In
the introduction of section~\ref{sec:rec:per}, it was stated for a self-adjoint
and positive operator $\oiA$, that the effective condition number of
$\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ is always less than or equal to the condition
number of the original operator $\oiA$ -- independent of the choice of the
subspace $\vsU$. However, the next example shows that this does not hold for
self-adjoint but indefinite operators.

\begin{ex}
  \label{ex:rec:per:def:indef-cond}
  Consider the operator $\oiA$ and the deflation basis $\vtU$ defined by
  \[
    \oiA=\mat{-1 &&\\ &1& \\ && 1}
    \qquad\text{and}\qquad
    \vtU=\mat{\sqrt{1+\varepsilon}\\1\\0}
  \]
  with $0<\varepsilon\ll 1$ and the Euclidean inner product. Then
  $\ip{\vtU}{\oiA\vtU}=-\varepsilon<0$ and with $\vsU=\Span{\vtU}$ a direct
  computation shows that the projected operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$
  has the spectrum
  $\Lambda(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)=\{0,1,1+\frac{2}{\varepsilon}\}$.
  The effective condition number thus is
  $\kappa_\eff(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)=1+\frac{2}{\varepsilon}$ which
  can become arbitrarily large.
\end{ex}

In practice, it is often observed that an approximation $\vsU$ to an
$\oiA$-invariant subspace only leads to a slight difference between
$\Lambda(\oiP_{\vsV^\perp,\oiA\vsV}\oiA)$ and
$\Lambda(\oiP_{\vsU,\oiA\vsU}\oiA)$ if $\vsU$ is a ``good enough''
approximation. Likewise, the convergence of Krylov subspace methods for linear
systems is often observed to behave very similarly for the exact and the
approximate invariant subspaces. The following example demonstrates this effect
for the MINRES method.

\begin{ex}
  \label{ex:rec:per:def:minres}
  Let $\oiA=\diag(\lambda_1,\dots,\lambda_{104})$ with $\lambda_1=-10^{-3}$,
  $\lambda_2=-10^{-4}$, $\lambda_3=-10^{-5}$ and
  \[
    \lambda_{i+4} = 1 + \frac{i}{100}
    \quad\text{for}\quad
    i\in\{0,\ldots,100\}
  \]
  and let $\vb=[1,1,1,0.1,\dots,0.1]^\tp$. The convergence of MINRES applied to
  $\oiA\vx=\vb$ with $\vx_0=0$ and the corresponding
  bound~\eqref{eq:back:minres:bound} from
  theorem~\ref{thm:back:minres:bound} are visualized in
  figure~\ref{fig:rec:per:def:minres}. It is observed that MINRES almost
  stagnates for the initial 20 steps, and that the
  bound~\eqref{eq:back:minres:bound} is quite descriptive in this phase.
  Deflating the three negative eigenvalues using $\vsV=\Span{\ve_1,\ve_2,\ve_3}$
  reduces the effective condition number to
  $\kappa_\eff(\oiP_{\vsV^\perp}\oiA)=2$ (note that
  $\oiP_{\vsV^\perp,\oiA\vsV}=\oiP_{\vsV^\perp}$) and the initial phase of slow
  convergence is completely removed. The faster convergence is also described by
  the bound~\eqref{eq:back:minres:bound}. Furthermore, a similar behavior is
  observed for a slightly perturbed deflation space given by
  $\vsU=\Span{[\ve_1,\ve_2,\ve_3]+ 10^{-5}\ofE}$ with a random $\ofE\in\R^{N,3}$
  of norm 1.
\end{ex}

\begin{figure}[ht]
  \centering
  \setlength{\figureheight}{0.5\textwidth}
  \setlength{\figurewidth}{0.65\textwidth}
  \inputplot{exp_rec_per_def_minres}
  \caption{Convergence history of MINRES with the linear system from
    example~\ref{ex:rec:per:def:minres}: without deflation, with deflation of an
    exact invariant subspace $\vsV$ and with deflation of an approximate
    invariant subspace $\vsU$. The curve for the approximate invariant subspace
    only differs slightly from the curve for the exact invariant subspace.
  }
  \label{fig:rec:per:def:minres}
\end{figure}

Example~\ref{ex:rec:per:def:minres} clearly shows that it can be worthwhile to
use an approximate invariant subspace to get rid of certain eigenvalues in a
deflated operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$. The experiment seems to
indicate that a small perturbation of an invariant subspace $\vsV$ only leads to
a small change in the convergence behavior of MINRES. In order to
understand the underlying mechanisms mathematically, a better understanding of
the spectrum of deflated operators is needed.

Already in the first article on deflation, Nicolaides~\cite{Nic87} gave a
characterization of the smallest and largest positive eigenvalues of
the deflated operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ in terms of a Rayleigh
quotient in the case where $\oiA$ is positive definite.
In~\cite[lemma~3.1]{Nic87}, Nicolaides states that
\begin{align}
  \min \Lambda(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)\setminus\{0\}
    & = \min_{\vu\in\vsU^\perp} \frac{\ip{\vu}{\vu}}{\ip{\vu}{\oiA^\inv\vu}}
    \label{eq:rec:per:def:nicmin}\\
  \text{and}\qquad
    \max \Lambda(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)
    & = \max_{\vu\in\vsU^\perp} \frac{\ip{\vu}{\vu}}{\ip{\vu}{\oiA^\inv\vu}}.
    \label{eq:rec:per:def:nicmax}
\end{align}
Inspired by this result, the following theorem shows how the \emph{full}
spectrum of the deflated operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ can be
characterized via $\oiA$'s inverse for \emph{any} invertible operator $\oiA$ --
even if $\oiA$ is indefinite, non-self-adjoint or non-diagonalizable. To the
knowledge of the author, no similar statements exist in the literature.

\begin{thm}
  \label{thm:rec:per:def:spectrum}
  Let $\oiA\in\vsL(\vsH)$ be nonsingular and let $\vsU\subseteq\vsH$ be a
  subspace of dimension $m>0$ such that
  $\sin\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$.

  Then $\restr{\oiP_{\vsU^\perp}\oiA^\inv}{\vsU^\perp}:\vsU^\perp\lra\vsU^\perp$
  is nonsingular and
  \[
    \oiP_{\vsU^\perp,\oiA\vsU}\oiA
    = \left(\restr{\oiP_{\vsU^\perp}\oiA^\inv}{\vsU^\perp}\right)^\inv
      \oiP_{\vsU^\perp}.
  \]
  In particular, the spectrum satisfies
  \[
    \Lambda\left(\oiP_{\vsU^\perp,\oiA\vsU}\oiA\right)
    = \{0\}\cup \Lambda\left(
        \left(\restr{\oiP_{\vsU^\perp}\oiA^\inv}{\vsU^\perp}\right)^\inv
      \right).
  \]
\end{thm}

\begin{proof}
  First note that
  \begin{align*}
    \oiP_{\vsU^\perp,\oiA\vsU}\oiA
    &= \oiA\oiP_{(\oiA^\adj\vsU)^\perp,\vsU}
    = \oiA\oiP_{(\oiA^\adj\vsU)^\perp,\vsU} \oiP_{\vsU^\perp}
    = \restr{\oiA\oiP_{(\oiA^\adj\vsU)^\perp,\vsU}}{\vsU^\perp}
      \oiP_{\vsU^\perp}.
  \end{align*}
  Then item~\ref{lem:proj:inverse} of lemma~\ref{lem:proj} can be applied by
  noticing that
  \[
    \restr{\oiP_{\vsU^\perp,\oiA\vsU}\oiA}{\vsU^\perp}
    = \restr{\oiA\oiP_{(\oiA^\adj\vsU)^\perp,\vsU}}{\vsU^\perp}
    = \oiA\oiQ_{(\oiA^\adj\vsU)^\perp,\vsU}^\inv:\vsU^\perp\lra\vsU^\perp
  \]
  is nonsingular:
  \begin{align*}
    \oiP_{\vsU^\perp,\oiA\vsU}\oiA
    = \oiA\oiQ_{(\oiA^\adj\vsU)^\perp,\vsU}^\inv
      \oiP_{\vsU^\perp}
    = \left(
        \restr{\oiQ_{(\oiA^\adj\vsU)^\perp,\vsU}\oiA^\inv}{\vsU^\perp}
      \right)^\inv
      \oiP_{\vsU^\perp}
    = \left(
        \restr{\oiP_{\vsU^\perp}\oiA^\inv}{\vsU^\perp}
      \right)^\inv
      \oiP_{\vsU^\perp}.
  \end{align*}
  The statement regarding the spectrum follows from the fact that both $\vsU$
  and $\vsU^\perp$ are $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$-invariant subspaces.
\end{proof}

The following corollary recasts the above result on the spectrum of the deflated
operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ in a more accessible form for the case
where an orthonormal basis of $\vsU^\perp$ is at hand.

\begin{cor}
  \label{cor:rec:per:def:spectrum}
  Let the assumptions of theorem~\ref{thm:rec:per:def:spectrum} hold.
  Furthermore, assume that $\vtU_\perp\in\vsH^{N-m}$ is given with
  $\vsU^\perp=\Span{\vtU_\perp}$ and $\ip{\vtU_\perp}{\vtU_\perp}=\ofI_{N-m}$.

  Then
  \begin{equation}
    \Lambda\left(\oiP_{\vsU^\perp,\oiA\vsU}\oiA\right)
    = \{0\}\cup \Lambda\left(
        \ip{\vtU_\perp}{\oiA^\inv\vtU_\perp}^\inv
      \right).
    \label{eq:rec:per:def:spectrum:cor}
  \end{equation}
\end{cor}

\begin{proof}
  The statement immediately follows from theorem~\ref{thm:rec:per:def:spectrum}
  by representing the projection $\oiP_{\vsU^\perp}$ in terms of $\vtU_\perp$:
  \[
    \restr{\oiP_{\vsU^\perp}\oiA^\inv}{\vsU^\perp}\vtU_\perp
    = \oiP_{\vsU^\perp}\oiA^\inv\vtU_\perp
    = \vtU_\perp \ip{\vtU_\perp}{\oiA^\inv\vtU_\perp}.
  \]
\end{proof}

In the case of a positive-definite operator $\oiA$, it becomes apparent from
equation~\eqref{eq:rec:per:def:spectrum:cor} and an application of Rayleigh
quotients, that Nicolaides'
characterizations~\eqref{eq:rec:per:def:nicmin}--\eqref{eq:rec:per:def:nicmax}
of the smallest and largest positive eigenvalue of
$\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ are special cases of the above results.

For a self-adjoint operator $\oiA$, it is interesting to know how the number of
positive and negative eigenvalues of $\oiA$ relate to the ones in
$\oiP_{\vsU^\perp,\oiA\vsU}\oiA$.

\begin{definition}[Inertia]
  \label{def:rec:per:def:inertia}
  For a self-adjoint operator $\oiA\in\vsL(\vsH)$, the \emph{inertia} of $\oiA$ is defined as
  \[
    \In(\oiA) = \left(n_-(\oiA),n_0(\oiA),n_+(\oiA)\right),
  \]
  where $n_-(\oiA)$, $n_0(\oiA)$ and $n_+(\oiA)$ denotes the number of negative,
  zero and positive eigenvalues of $\oiA$.
\end{definition}

The following theorem gives a precise characterization of
$\In(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)$, which appears to be new.

\begin{thm}
  \label{thm:rec:per:def:inertia}
  Let $\oiA\in\vsL(\vsH)$ be self-adjoint and let $\vtU\in\vsH^m$ be such that
  $\vsU=\Span{\vtU}$ is an $m$-dimensional subspace with
  $\sin\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$.

  Then
  \[
    \In(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)
    = \In(\oiA) - \In(\ip{\vtU}{\oiA\vtU}) + (0,m,0).
  \]
\end{thm}

\begin{proof}
  The proof's central element is the application of the Haynsworth inertia
  identity~\cite{Hay68}. For a partitioned Hermitian matrix
  \[
    \ofH=\mat{\ofH_{11} & \ofH_{12}\\ \ofH_{12}^\htp & \ofH_{22}}\in\C^{n,n}
  \]
  with a nonsingular submatrix $\ofH_{11}$, the Haynsworth inertia identity
  states that
  \[
    \In(\ofH) = \In(\ofH_{11})
      + \In\left( \ofH_{22} - \ofH_{12}^\htp\ofH_{11}^\inv\ofH_{12} \right).
  \]
  Assume that $\vtU_\perp\in\vsH^{N-m}$ is given such that
  $\vsU^\perp=\Span{\vtU_\perp}$ and $\ip{\vtU_\perp}{\vtU_\perp}=\ofI_{N-m}$.
  Because $\vsU$ is an invariant subspace of $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$,
  it follows that
  \begin{equation}
    \In(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)
    = (0,m,0) + \In\left(
        \ip{\vtU_\perp}{\oiP_{\vsU^\perp,\oiA\vsU}\oiA\vtU_\perp}
      \right).
    \label{eq:rec:per:def:inertia:split}
  \end{equation}
  By applying the Haynsworth identity to the matrix
  \[
    \ofH
    = \ip{\mat{\vtU & \vtU_\perp}}{\oiA \mat{\vtU & \vtU_\perp}}
    = \mat{\ip{\vtU,\oiA\vtU} & \ip{\vtU}{\oiA\vtU_\perp} \\
           \ip{\vtU_\perp,\oiA\vtU} & \ip{\vtU_\perp}{\oiA\vtU_\perp}},
  \]
  the following equation can be obtained:
  \begin{align*}
    \In(\oiA)
    &= \In(\oiH)
    = \In\left(\ip{\vtU}{\oiA\vtU}\right)
      + \In\left(
        \ip{\vtU_\perp}{\oiA\vtU_\perp}
        - \ip{\vtU_\perp}{\oiA\vtU}\ip{\vtU}{\oiA\vtU}^\inv
        \ip{\vtU}{\oiA\vtU_\perp}
      \right)\\
    &= \In\left(\ip{\vtU}{\oiA\vtU}\right)
      + \In\left(
        \ip{\vtU_\perp}{\oiP_{\vsU^\perp,\oiA\vsU}\oiA\vtU_\perp}
      \right).
  \end{align*}
  Note that the first equality also holds for non-orthonormal $\vtU$ because of
  Sylvester's law of inertia. The statement then follows with
  equation~\eqref{eq:rec:per:def:inertia:split}.
\end{proof}

Theorem~\ref{thm:rec:per:def:inertia} can be helpful in practice, e.g., if a
self-adjoint and indefinite operator $\oiA$ is given that exhibits $0<m\ll N$
negative eigenvalues and $N-m$ positive eigenvalues. If an $\vtU\in\vsH^m$ is
known such that $\ip{\vtU}{\oiA\vtU}$ has $m$ negative eigenvalues, then with
$\vsU=\Span{\vtU}$ the deflated operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ has
the inertia $(0,m,N-m)$ and thus is positive semidefinite. This property is
independent of the closeness of $\vsU$ to an $\oiA$-invariant subspace. In
such a case, the CG method can be used with the deflated linear system (cf.\
section~\ref{thm:rec:def:cg}) instead of the MINRES method in order to solve a
given linear system with the indefinite operator $\oiA$. Although no positive
eigenvalue of $\oiA$ can turn into a negative eigenvalue of
$\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ in such a situation, care has to be taken
because nothing can be said about how close a positive eigenvalue can come to
zero or how large they can grow. Recall that it was demonstrated in
example~\ref{ex:rec:per:def:indef-cond} that -- unlike in the positive-definite
case -- the effective condition number
$\kappa_\eff(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)$ is not bounded by $\kappa(\oiA)$
in the self-adjoint and indefinite case and can grow arbitrarily if no further
restrictions are imposed on $\vtU$.

In the remaining part of this subsection, bounds for the spectrum of a deflated
operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ are developed in the case of a
self-adjoint operator $\oiA$. In order to prove the apparently new bounds, some
well-known auxiliary perturbation results are needed and are recalled in the
course of this subsection. If knowledge about the spectrum $\Lambda(\oiA)$ of
the original operator $\oiA$ is at hand, the perturbation bounds can be used in
order to estimate the spectrum $\Lambda(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)$  of a
deflated operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ if an approximate invariant
subspace $\vsU$ is used as the deflation subspace. For the deflated CG and
MINRES methods, these spectral estimations then may give insight into the
convergence behavior of these methods, cf.\ the convergence bounds in
sections~\ref{sec:back:cg} and \ref{sec:back:mr:minres}.

For the non-self-adjoint case, the perturbation theory is more intricate and
the effects of non-normality can be disastrous. The breakdown in
example~\ref{ex:rec:def:break2} may serve as a warning sign for the sometimes
counter-intuitive behavior of non-normal operators. Furthermore, the spectrum
may have no influence on the convergence behavior of Krylov subspace methods
like the GMRES method, cf.\ the discussion following
theorem~\ref{thm:back:gmres:spectral}.

In the following, only the minimal required subset of spectral perturbation
results for self-adjoint operators is presented. An extensive treatment of
spectral perturbation theory can, e.g., be found in the books by
Bhatia~\cite{Bha07}, Stewart and Sun~\cite{SteS90}, Demmel~\cite{Dem97} and
Parlett~\cite{Par98}.

Given two self-adjoint linear operators $\oiL,\oiE\in\vsL(\vsH)$, the question
of how the eigenvalues of $\widehat{\oiL}=\oiL+\oiE$ relate to the eigenvalues of
$\oiL$ and $\oiE$ is, e.g., answered by Weyl's theorem:

\begin{thm}[Weyl]
  \label{thm:rec:per:def:weyl}
  Let $\oiL,\oiE\in\vsL(\vsH)$ be two self-adjoint linear operators and let
  $\widehat{\oiL}\DEF \oiL + \oiE$. The (real) eigenvalues of $\oiL$,
  $\widehat{\oiL}$ and $\oiE$ are denoted by $\lambda_1\leq\ldots\leq\lambda_N$,
  $\widehat{\lambda}_1\leq\ldots\leq\widehat{\lambda}_N$ and
  $\epsilon_1\leq\ldots\leq\epsilon_N$.

  Then for $i\in\{1,\ldots,N\}$
  \[
    \widehat{\lambda}_i\in[\lambda_i+\epsilon_1,\lambda_i+\epsilon_N].
  \]
\end{thm}

\begin{proof}
  See~\cite[corollary~4.9]{SteS90}.
\end{proof}

Weyl's theorem is often stated in the following weaker form:

\begin{cor}
  \label{cor:rec:per:def:weyl}
  With the assumptions and notation of theorem~\ref{thm:rec:per:def:weyl} the
  following holds:
  \[
    \max_{i\in\{1,\ldots,N\}} |\widehat{\lambda}_i - \lambda_i| \leq \nrm{\oiE}.
  \]
\end{cor}

\begin{proof}
  The statement directly follows from theorem~\ref{thm:rec:per:def:weyl} since
  $\nrm{\oiE}=\max\{|\epsilon_1|,|\epsilon_N|\}$.
\end{proof}

Assume that the spectrum of $\oiA$ is known and that an $m$-dimensional subspace
$\vsU\subseteq\vsH$ is given such that
$\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$, i.e., such that the projection
$\oiP_{\vsU^\perp,\oiA\vsU}$ is well defined. The question now is: how can bounds
on the spectrum of $\widehat{\oiA}=\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ be
established?

A naive approach would be to choose $\oiL=\oiA$ and
$\oiE=\oiP_{\oiA\vsU,\vsU^\perp}\oiA$ in Weyl's theorem. However, $m$
eigenvalues will be moved to zero in the deflated operator $\widehat{\oiA}$ and
thus the perturbation's norm $\nrm{\oiE}$ can grow exceedingly.

A more reasonable idea is to choose $\oiL=\oiP_{\vsV^\perp}\oiA$ and
$\oiE=\oiP_{\vsU^\perp,\oiA\vsU}\oiA - \oiP_{\vsV^\perp}\oiA$, where $\vsV$ is an
$\oiA$-invariant subspace that is supposed to be ``close'' to the subspace
$\vsU$. The ``closeness'' is quantified in terms of an angle or a residual
in the course of this subsection. The next lemma is an intermediate result that
makes use of Weyl's theorem and the new bound on the norm of the difference of
two projections, cf.\ corollary~\ref{cor:rec:per:proj:diff}.

\begin{lemma}
  \label{lem:rec:per:def:pert}
  Let $\oiA\in\vsL(\vsH)$ be self-adjoint and $\vsV\subseteq\vsH$ an
  $\oiA$-invariant subspace with
  \[
    \rho_{\vsV}
      \DEF\rho\left(\restr{\oiA}{\vsV}\right)
      =\max_{\lambda\in\Lambda\left(\restr{\oiA}{\vsV}\right)}|\lambda|
    \quad\text{and}\quad
    \rho_{\vsV^\perp}
      \DEF\rho\left(\restr{\oiA}{\vsV^\perp}\right)
      =\max_{\lambda\in\Lambda\left(\restr{\oiA}{\vsV^\perp}\right)}
      |\lambda|.
  \]
  Furthermore, let $\vsU\subseteq\vsH$ be a subspace such that
  $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$ and let the eigenvalues of
  $\oiP_{\vsV^\perp}\oiA$ and $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ be sorted as
  \[
    \overline{\lambda}_1\leq\ldots\leq\overline{\lambda}_N
    \quad\text{and}\quad
    \widehat{\lambda}_1\leq\ldots\leq\widehat{\lambda}_N.
  \]
  Then:
  \begin{equation}
    \max_{i\in\{1,\ldots,N\}} |\widehat{\lambda}_i-\overline{\lambda}_i|
    \leq
    \frac{\rho_\vsV \sin\theta_{\max}(\vsV,\oiA\vsU)
    + \rho_{\vsV^\perp}\sin\theta_{\max}(\vsV,\vsU)}
    {\cos\theta_{\max}(\vsU,\oiA\vsU)}.
    \label{eq:rec:per:def:pert}
  \end{equation}
\end{lemma}

\begin{proof}
  Let $\oiL=\oiP_{\vsV^\perp}\oiA$ and
  $\oiE=\oiP_{\vsU^\perp,\oiA\vsU}\oiA - \oiP_{\vsV^\perp}\oiA$. Then with
  corollary~\ref{cor:rec:per:proj:diff} the following bound holds:
  \begin{align*}
    \nrm{\oiE}
    &= \nrm{\oiA(\oiP_{\vsV^\perp} - \oiP_{(\oiA\vsU)^\perp,\vsU})} \\
    &\leq
    \frac
    {\nrm{\restr{\oiA}{\vsV}} \sin\theta_{\max}(\vsV^\perp,(\oiA\vsU)^\perp)
      + \nrm{\restr{\oiA}{\vsV^\perp}}\sin\theta_{\max}(\vsV,\vsU)}
      {\cos\theta_{\max}(\vsV,\vsV)\cos\theta_{\max}(\vsU,\oiA\vsU)} \\
    &= \frac{\rho_\vsV \sin\theta_{\max}(\vsV,\oiA\vsU)
      + \rho_{\vsV^\perp}\sin\theta_{\max}(\vsV,\vsU)}
      {\cos\theta_{\max}(\vsU,\oiA\vsU)}.
  \end{align*}
  The lemma's statement then follows with Weyl's theorem, cf.\
  corollary~\ref{cor:rec:per:def:weyl}.
\end{proof}

The bound~\eqref{eq:rec:per:def:pert} in lemma~\ref{lem:rec:per:def:pert}
exhibits an invariant subspace $\vsV$ which is not known in general. In
practice, the deflation space $\vsU$ is often chosen as an approximation to an
invariant subspace, e.g., as the subspace spanned by Ritz or harmonic Ritz
vectors from a larger subspace. Assume that an orthonormal basis of approximate
eigenvectors $\vtU\in\vsH^m$, i.e., $\ip{\vtU}{\vtU}=\ofI_m$ and
$\vsU=\Span{\vtU}$, and approximations to eigenvalues $\mu_1,\ldots,\mu_m\in\R$
are given. With $\ofD=\diag(\mu_1,\ldots,\mu_m)$, the norm of the residual
$\vtR=\oiA\vtU-\vtU\ofD$ can be used as an indicator for the quality of the
approximate eigenvectors and eigenvalues. In the Arnoldi and Lanczos methods,
the residual norm of Ritz and harmonic Ritz pairs is available as a cheap
byproduct of the iteration.

An interesting question is if a similar statement to
lemma~\ref{lem:rec:per:def:pert} can be formulated in terms of the residual norm
$\nrm{\vtR}$. In fact, such a bound can be established if additional knowledge
about the spectrum of $\oiA$ is available. In the course of this subsection, the
following definitions of the spectral gap and the spectral interval gap are
of importance.

\begin{definition}[Spectral gap]
  \label{def:rec:per:def:gap}
  Let two bounded sets $A\subset\R$ and $B\subset\R$ be given.
  \begin{enumerate}
    \item The \emph{spectral gap} between $A$ and $B$ is defined as
      \[
        \delta(A,B) = \min_{\substack{\alpha\in A\\\beta\in B}} |\alpha-\beta|.
      \]
    \item The \emph{spectral interval gap} between $A$ and $B$ is defined as
      \[
        \underline{\delta}(A,B) =
        \begin{cases}
          \delta(A,B) &
            \text{if}~B\cap[\min(A),\max(A)]=\emptyset\\
            &\text{or}~A\cap[\min(B),\max(B)]=\emptyset,\\
          0 & \text{otherwise}.
        \end{cases}
      \]
  \end{enumerate}
\end{definition}

Note that the spectral (interval) gap should not be confused with the gap
between subspaces which also appears in the following theorems in form of the
sine of the maximal canonical angle, cf.\ definition~\ref{def:gap}. The next
theorem can be found in the seminal paper of Davis and Kahan~\cite{DavK70} and
relates the maximal angle between an invariant subspace and an approximation to
the residual of the given eigenpair approximations and a spectral interval gap.

\begin{thm}[$\sin\theta$-theorem]
  \label{thm:rec:per:def:sintheta}
  Let $\oiL$ be a self-adjoint linear operator and $\vsV\subseteq\vsH$ an
  $\oiL$-invariant subspace. Furthermore, let $\vtZ\in\vsH^m$ be with
  $\ip{\vtZ}{\vtZ}=\ofI_m$, $\vsZ=\Span{\vtZ}$ and $\ofM=\ofM^\htp\in\C^{m,m}$.
  If $\underline{\delta} =
  \underline{\delta}
  \left(\Lambda(\ofM),\Lambda(\restr{\oiL}{\vsV^\perp})\right)>0$, then
  \[
    \sin\theta_{\max}(\vsV,\vsZ)
    \leq \frac{\nrm{\oiL\vtZ - \vtZ\ofM}}{\underline{\delta}}.
  \]
\end{thm}

\begin{proof}
  See~\cite[theorem~3.6]{SteS90}. The original proof by Davis and Kahan can be
  found in~\cite{DavK70}. Their theorem does not require the Hilbert space
  $\vsH$ to be finite-dimensional and even allows unbounded operators.
\end{proof}

In the proof of the following theorem, the $\sin\theta$-theorem and
lemma~\ref{lem:rec:per:def:pert} are used in order to obtain a bound that
depends on computable quantities in terms of the given eigenpair approximations
and the spectral interval gap between the given eigenvalue approximations and a
part of $\oiA$'s spectrum.

\begin{thm}
  \label{thm:rec:per:def:angle}
  Let $\oiA\in\vsL(\vsH)$ be self-adjoint and let $\vtU\in\vsH^m$ such that
  $\ip{\vtU}{\vtU}=\ofI_m$, $\vsU=\Span{\vtU}$ and
  $\sin\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$. Furthermore, let
  $\ofD=\diag(\mu_1,\ldots,\mu_m)\in\R^{m,m}$ be given and
  $\vtR=\oiA\vtU-\vtU\ofD$. Let the eigenvalues of $\oiA$ be
  $\lambda_1,\ldots,\lambda_N$.

  If there exists a permutation $k_1,\ldots,k_N$ of $1,\ldots,N$ such that
  \[
    \underline{\delta} = \underline{\delta} \left(
      \Lambda(\ofD),\{\lambda_{k_{m+1}},\ldots,\lambda_{k_N}\}
    \right) > 0,
  \]
  then the eigenvalues $\widehat{\lambda}_1\leq\ldots\leq\widehat{\lambda}_N$ of
  $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ satisfy
  \begin{equation}
    \max_{i\in\{1,\ldots,N\}} |\widehat{\lambda}_i - \underline{\lambda}_i|
    \leq \frac
      {(\rho+\underline{\rho})\frac{\nrm{\vtR}}{\underline{\delta}}
      + \rho \sin\theta_{\max}(\vsU,\oiA\vsU)}
      {\cos\theta_{\max}(\vsU,\oiA\vsU)},
    \label{eq:rec:per:def:angle}
  \end{equation}
  where $\underline{\lambda}_1\leq\ldots\leq\underline{\lambda}_N$ are the
  ordered values $\lambda_{k_{m+1}},\ldots,\lambda_{k_N},0,\ldots,0$ (zero
  is added $m$ times), $\rho = \max_{j\in\{1,\ldots,m\}}|\lambda_{k_j}|$ and
  $\underline{\rho} = \max_{j\in\{m+1,\ldots,N\}}|\lambda_{k_j}|$.
\end{thm}

\begin{proof}
  Let $\vsV\subseteq\vsH$ be an $\oiA$-invariant subspace that is associated
  with the eigenvalues $\lambda_{k_{m+1}},\ldots,\lambda_{k_N}$. Note that
  $\Lambda(\oiP_{\vsV^\perp}\oiA)=
  \{\underline{\lambda}_1\ldots,\underline{\lambda}_N\}$. With
  $\oiL=\oiA$, $\vtZ=\vtU$ and $\ofM=\ofD$, the requirements of the
  $\sin\theta$-theorem~\ref{thm:rec:per:def:sintheta} are satisfied and thus
  \begin{equation}
    \sin\theta_{\max}(\vsV,\vsU)\leq\frac{\nrm{\vtR}}{\underline{\delta}}.
    \label{eq:rec:per:def:pert:sintheta}
  \end{equation}
  With $\rho_{\vsV}=\rho$ and $\rho_{\vsV^\perp}=\underline{\rho}$, the
  following inequality holds according to lemma~\ref{lem:rec:per:def:pert}:
  \[
    \max_{i\in\{1,\ldots,N\}}
    |\widehat{\lambda}_i - \underline{\lambda}_i|
    \leq
    \frac{\rho \sin\theta_{\max}(\vsV,\oiA\vsU)
    + \underline{\rho}\sin\theta_{\max}(\vsV,\vsU)}
    {\cos\theta_{\max}(\vsU,\oiA\vsU)}.
  \]
  With item~\ref{lem:gap:triangle} of lemma~\ref{lem:gap}, the inequality
  \[
    \sin\theta_{\max}(\vsV,\oiA\vsU)\leq\sin\theta_{\max}(\vsV,\vsU)
    + \sin\theta_{\max}(\vsU,\oiA\vsU)
  \]
  holds and thus
  \[
    \max_{i\in\{1,\ldots,N\}}
    |\widehat{\lambda}_i - \underline{\lambda}_i|
    \leq
    \frac{(\rho+\underline{\rho})\sin\theta_{\max}(\vsV,\vsU)
    + \rho \sin\theta_{\max}(\vsV,\oiA\vsU)}
    {\cos\theta_{\max}(\vsU,\oiA\vsU)}.
  \]
  The proof is complete after inserting the
  inequality~\eqref{eq:rec:per:def:pert:sintheta} into the last inequality.
\end{proof}

The quantities $\rho$ and $\underline{\rho}$ in
theorem~\ref{thm:rec:per:def:angle} can both be bounded by $\nrm{\oiA}$ which
can often be approximated cheaply. As mentioned before, the residual norm
$\nrm{\vtR}$ can be obtained as a byproduct in the Arnoldi or Lanczos
algorithms, cf.\ section~\ref{sec:back:arnoldi}. The maximal angle
$\theta_{\max}(\vsU,\oiA\vsU)$ can be computed by
\begin{equation}
  \theta_{\max}(\vsU,\oiA\vsU) =
  \arccos\left(\sigma_{\min}(\ip{\vtU}{\vtQ})\right),
  \label{eq:rec:per:def:maxangle}
\end{equation}
where $\sigma_{\min}(\ip{\vtU}{\vtQ})$ denotes the minimal singular value of
$\ip{\vtU}{\vtQ}$ and $\vtQ\in\vsH^m$ is such that $\ip{\vtQ}{\vtQ}=\ofI_m$ and
$\oiA\vsU=\Span{\vtQ}$. However, due to round-off errors, care has to be taken
because small angles cannot be found accurately with naive implementations,
e.g., with equation~\eqref{eq:rec:per:def:maxangle}. The issue is discussed
briefly in section~\ref{sec:back:round-off}.

The only quantity that is hard to get in practical applications is the spectral
interval gap $\underline{\delta}$ which describes how well the approximate
eigenvalues $\mu_1,\ldots,\mu_m$ are separated from $N-m$ eigenvalues
$\lambda_{k_{m+1}},\ldots,\lambda_{k_N}$ of $\oiA$. The exact value of this
quantity is rarely available in practice but it can sometimes be estimated from
properties of the originating problem or from eigenvalue approximations of a
similar operator. For example, an operator $\oiA$ may be known to have only a
few negative eigenvalues $\lambda_1\leq\ldots\leq\lambda_m< 0$ while the other
eigenvalues $0<\lambda_{m+1}\leq\ldots\leq\lambda_N$ are positive. If then
eigenvalue approximations $\mu_1\leq\ldots\leq\mu_m<0$ are given, then
$\underline{\delta}(\{\mu_1,\ldots,\mu_m\},\{\lambda_{m+1},\ldots,\lambda_N\})
= \lambda_{m+1}-\mu_m>|\mu_m|$.  Furthermore, the spectral interval gap is
rather demanding since it requires that one part of the spectrum lies in an
interval that does not contain any of the complementary eigenvalues. The
spectral interval gap requirement is visualized in
figure~\ref{fig:rec:per:def:gap}.  However, it is not unusual to remove a
contiguous set of eigenvalues by deflation, e.g., only a few eigenvalues that
are closest to the origin.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{\textwidth}
    \centering
    \input{figures/rec_per_def_gap1.tex}
    \caption{$\underline{\delta}>0$ because
      $\{\lambda_{k_{m+1}},\ldots,\lambda_{k_N}\}\cap[\mu_1,\mu_m]
      = \emptyset$.}
    \label{fig:rec:per:def:gap1}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \centering
    \input{figures/rec_per_def_gap2.tex}
    \caption{$\underline{\delta}>0$ because
    $\{\mu_1,\ldots,\mu_m\}\cap[\lambda_{k_{m+1}},\lambda_{k_N}]
      = \emptyset$.}
    \label{fig:rec:per:def:gap2}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \centering
    \input{figures/rec_per_def_gap3.tex}
    \caption{$\underline{\delta}=0$. See figure~\ref{fig:rec:per:def:gap:quad}
      for a visualization of the spectral gap in this situation.}
    \label{fig:rec:per:def:gap3}
  \end{subfigure}
  \caption{Visualization of the assumption on the spectral interval gap in
    theorem~\ref{thm:rec:per:def:angle}. See
    definition~\ref{def:rec:per:def:gap} for the definition of the spectral
    interval gap.}
  \label{fig:rec:per:def:gap}
\end{figure}

In the next example, it is analyzed to what extent the eigenvalue bound in
theorem~\ref{thm:rec:per:def:angle} can capture the actual eigenvalue error.

\begin{ex}
  \label{ex:rec:per:def:angle}
  Let the matrix $\oiA$ be defined as in example~\ref{ex:rec:per:def:minres}.
  This example consists of the following two experiments:

  \begin{enumerate}
    \item \label{ex:rec:per:def:angle1}
      For the first experiment, let the subspace $\vsV=\Span{\ofI_{N,3}}$ be
      also given as in example~\ref{ex:rec:per:def:minres}. Recall that the
      eigenvalues of $\oiP_{\vsV^\perp}\oiA$ are
      $0,0,0<\lambda_4\leq\ldots\leq\lambda_N$. For $\varepsilon>0$, let
      $\vsU_\varepsilon=\Span{\ofI_{N,3}+\varepsilon\ofE}$ with a random
      $\ofE\in\R^{N,3}$ of norm 1 and let $\vtU_\varepsilon\in\R^{N,3}$ an
      orthonormal Ritz vector basis of $\vsU_\varepsilon$, i.e.,
      $\vsU_\varepsilon = \Span{\vtU_\varepsilon}$,
      $\ip{\vtU_\varepsilon}{\vtU_\varepsilon}=\ofI_3$ and
      $\ip{\vtU_\varepsilon}{\oiA\vtU_\varepsilon} = \ofD_\varepsilon =
      \diag(\mu_1^{(\varepsilon)},\mu_2^{(\varepsilon)},\mu_3^{(\varepsilon)})
      \in\R^{3,3}$, where
      $\mu_1^{(\varepsilon)}\leq\mu_2^{(\varepsilon)}\leq\mu_3^{(\varepsilon)}$
      are the Ritz values. The Ritz residual then is
      $\vtR_\varepsilon=\oiA\vtU_\varepsilon -
      \vtU_\varepsilon\ofD_\varepsilon$.

      With $\vtU=\vtU_\varepsilon$, both sides of the
      inequality~\eqref{eq:rec:per:def:angle} in
      theorem~\ref{thm:rec:per:def:angle} are plotted in
      figure~\ref{fig:rec:per:def:angle1} versus the perturbation size
      $\varepsilon$.  The figure clearly shows that the bound in
      theorem~\ref{thm:rec:per:def:angle} lies above the actual eigenvalue error
      by several orders of magnitude if the perturbation $\varepsilon$ is small.
      The overestimation is not that severe for larger Ritz residual norms. For
      small perturbations, the plot suggests that the actual eigenvalue error
      depends quadratically on the Ritz residual norm while the bound in
      theorem~\ref{thm:rec:per:def:angle} only provides a linear bound.

    \item \label{ex:rec:per:def:angle2}
      The second experiment only differs from
      experiment~\ref{ex:rec:per:def:angle1} in the choice of the subspace
      $\vsV$. Here, $\vsV=\Span{\ofI_{N,2}}$ is chosen and thus the eigenvalues
      of $\oiP_{\vsV^\perp}\oiA$ are $0,0,\lambda_3,\ldots,\lambda_N$.  Let the
      Ritz vector basis $\vtU_\varepsilon$, the Ritz value matrix
      $\ofD_\varepsilon=\diag(\mu_1^{(\varepsilon)},\mu_2^{(\varepsilon)})$ and
      the Ritz residual $\vtR_\varepsilon$ be defined analogously to
      experiment~\ref{ex:rec:per:def:angle1}.

      Analogous to experiment~\ref{ex:rec:per:def:angle1},
      figure~\ref{fig:rec:per:def:angle2} shows both sides of the inequality
      theorem~\ref{thm:rec:per:def:angle}. Now the difference between the bound
      the actual error is even around $10^3$ for large perturbations. The reason
      for this severe overestimation is the presence of the spectral interval
      gap
      $\underline{\delta}(\{\mu_1^{(\varepsilon)},\mu_2^{(\varepsilon)}\},
      \{\lambda_3,\ldots,\lambda_N\})$ which becomes very small. For
      $\varepsilon\approx 10^{-2}$, the spectral interval gap even is zero because none
      of the two spectra $\{\mu_1^{(\varepsilon)},\mu_2^{(\varepsilon)}\}$ and
      $\{\lambda_3,\ldots,\lambda_N\}$ can be placed into an interval such that
      it does not contain any of the eigenvalues from the other one.
  \end{enumerate}
  Note that the ``exact'' eigenvalue error
  $\max_{i\in\{1,\ldots,N\}}|\widehat{\lambda}_i - \underline{\lambda}_i|$ gets
  stuck around machine precision since the involved eigenvalues are computed
  numerically.
\end{ex}

\begin{figure}[!htb]
  \centering
  \setlength{\figureheight}{0.5\textwidth}
  \begin{subfigure}[b]{\textwidth}
    \setlength{\figurewidth}{0.65\textwidth}
    \inputplot{exp_rec_per_def_angle1}
    \caption{Deflation of the 3 smallest eigenvalues
      $\lambda_1,\lambda_2$ and $\lambda_3$.}
    \label{fig:rec:per:def:angle1}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \setlength{\figurewidth}{0.65\textwidth}
    \inputplot{exp_rec_per_def_angle2}
    \caption{Deflation of the 2 smallest eigenvalues
    $\lambda_1$ and $\lambda_2$.}
    \label{fig:rec:per:def:angle2}
  \end{subfigure}
  \caption{Spectral bound from theorem~\ref{thm:rec:per:def:angle} for deflated
    operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ with approximate invariant
    subspaces. The setup is described and discussed in
    example~\ref{ex:rec:per:def:angle}.}
  \label{fig:rec:per:def:angle}
\end{figure}

It could be observed in the previous experiment, that the
bound~\eqref{eq:rec:per:def:angle} in theorem~\ref{thm:rec:per:def:angle}
deviates from the actual eigenvalue error by several orders of magnitude and it
was conjectured that the eigenvalue errors depend quadratically on the
perturbation or the residual norm. In \cite{Ste91}, Stewart showed that the
eigenvalue error for self-adjoint (undeflated) operators can be bounded
quadratically in terms of the residual. However, since his eigenvalue error
bound is also based on the $\sin\theta$-theorem~\ref{thm:rec:per:def:sintheta}
by Davis and Kahan, it still has the same limitation: the spectrum has to be
separated such that the spectral interval gap is positive, cf.\
definition~\ref{def:rec:per:def:gap} and figure~\ref{fig:rec:per:def:gap}.
Mathias~\cite{Mat98} improved the eigenvalue bounds significantly by allowing
the eigenvalues and eigenvalue approximations to be scattered throughout the
spectrum.
A subset of the results from~\cite{Mat98} is stated here in terms of Ritz
residuals rather than in the original form of a perturbed block matrix.

\begin{thm}
  \label{thm:rec:per:def:mathias}
  Let $\oiL\in\vsL(\vsH)$ be self-adjoint with eigenvalues
  $\lambda_1\leq\ldots\leq\lambda_N$. Assume that $\vtX\in\vsH^m$ and
  $\vtY\in\vsH^{N-m}$ are given such that
  $\ip{[\vtX,\vtY]}{[\vtX,\vtY]}=\ofI_N$ and
  $\tilde{\lambda}_1\leq\ldots\leq\tilde{\lambda}_N$ are the eigenvalues of
  $\diag(\ofM,\ofN)$, where $\ofM=\ip{\vtX}{\oiL\vtX}$ and
  $\ofN=\ip{\vtY}{\oiL\vtY}$. Furthermore, let $i_1<\ldots<i_m$ be such that
  $\tilde{\lambda}_{i_1}\leq\ldots\leq\tilde{\lambda}_{i_m}$ are the
  eigenvalues of $\ofM$. Then with $\vtR=\oiL\vtX-\vtX\ofM$ the following holds:
  \begin{enumerate}
    \item If
        $\delta_i=\delta\left(\{\lambda_i\},\Lambda(\ofN)\right)>0$ for a
        $i\in\{1,\ldots,N\}$, then
        \[
          |\lambda_i - \tilde{\lambda}_i|
          \leq \frac{\nrm{\vtR}^2}{\delta_i}.
        \]
      \item If $\delta=\delta\left(\{\lambda_{i_1},\ldots,\lambda_{i_m}\},
        \Lambda(\ofN)\right)>0$ then
        \[
          \max_{k\in\{1,\ldots,m\}} |\lambda_{i_k} - \tilde{\lambda}_{i_k}|
          \leq \frac{\nrm{\vtR}^2}{\delta}.
        \]
  \end{enumerate}
\end{thm}

\begin{proof}
  \begin{enumerate}
    \item The statement immediately follows by applying the first statement of
      theorem~1 from~\cite{Mat98} to the matrices
      $\ofA=\ip{[\vtX,\vtY]}{\oiL[\vtX,\vtY]}$ and
      $\tilde{\ofA}=\diag(\ofM,\ofN)$. Note that
      $\delta_i=\delta\left(\{\lambda_i\},\Lambda(\ofN)\right)>0$ is equivalent
      to the condition $\lambda_i\notin\Lambda(\ofN)$ in~\cite{Mat98}.
    \item The result follows from
      \[
        \max_{k\in\{1,\ldots,m\}} |\lambda_{i_k} - \tilde{\lambda}_{i_k}|
        \leq \frac{\nrm{\vtR}^2}{\min_{k\in\{1,\ldots,m\}}\delta_{i_k}}
        \leq \frac{\nrm{\vtR}^2}{\delta}.
      \]
  \end{enumerate}
\end{proof}

With Mathias' quadratic residual norm bound for the eigenvalues of a
self-adjoint operator $\oiA$, an apparently new quadratic bound can be derived
for the deflated operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$.

\begin{thm}
  \label{thm:rec:per:def:quad}
  Let $\oiA\in\vsL(\vsH)$ be self-adjoint with eigenvalues
  $\lambda_1\leq\ldots\leq\lambda_N$ and let $\vtU\in\vsH^m$ and
  $\vtU_\perp\in\vsH^{N-m}$ be such that
  $\ip{[\vtU,\vtU_\perp]}{[\vtU,\vtU_\perp]}=\ofI_N$ and
  $\sin\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$ with $\vsU=\Span{\vtU}$.
  Furthermore, assume that $\tilde{\lambda}_1\leq\ldots\leq\tilde{\lambda}_N$
  are the eigenvalues of $\diag(\ofM,\ofN)$, where
  $\ofM=\ip{\vtU_\perp}{\oiA\vtU_\perp}$ and $\ofN=\ip{\vtU}{\oiA\vtU}$ and let
  $1\leq i_1<\ldots<i_{N-m}\leq N$ be given such that
  $\tilde{\lambda}_{i_1}\leq\ldots\leq\tilde{\lambda}_{i_{N-m}}$ are the
  eigenvalues of $\ofM$.

  Then with $\vtR=\oiA\vtU-\vtU\ofN$ and
  $\mu_{\min}\DEF\min_{\mu\in\Lambda(\ofN)}|\mu|$ the following holds for the spectrum
  $\Lambda(\oiP_{\vsU^\perp,\oiA\vsU}\oiA) =
  \{0\}\cup\{\widehat{\lambda}_1\leq\ldots\leq\widehat{\lambda}_{N-m}\}$:
  \begin{enumerate}
    \item If $\delta_k=\delta(\{\lambda_{i_k}\},\Lambda(\ofN))>0$ for a
      $k\in\{1,\ldots,N-m\}$, then
      \begin{equation}
        |\lambda_{i_k} - \widehat{\lambda}_k|
        \leq \nrm{\vtR}^2 \left(
          \frac{1}{\delta_k} + \frac{1}{\mu_{\min}}
        \right).
        \label{eq:rec:per:def:quad1}
      \end{equation}
    \item If
      $\delta=\delta(\{\lambda_{i_1},\ldots,\lambda_{i_{N-m}}\},
      \Lambda(\ofN))>0$,
      then
      \begin{equation}
        \max_{k\in\{1,\ldots,N-m\}} |\lambda_{i_k} - \widehat{\lambda}_k|
        \leq \nrm{\vtR}^2 \left(
          \frac{1}{\delta} + \frac{1}{\mu_{\min}}
        \right).
        \label{eq:rec:per:def:quad2}
      \end{equation}
  \end{enumerate}
\end{thm}

\begin{proof}
  Because $\vsU\subseteq\vsN(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)$, the spectrum of
  $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ can be characterized as
  \[
    \Lambda(\oiP_{\vsU^\perp,\oiA\vsU}\oiA)
    = \{0\}\cup\Lambda\left(
        \ip{\vtU_\perp}{\oiP_{\vsU^\perp,\oiA\vsU}\oiA\vtU_\perp}
      \right),
  \]
  where the eigenvalues of
  $\ip{\vtU_\perp}{\oiP_{\vsU^\perp,\oiA\vsU}\oiA\vtU_\perp}$ are
  $\widehat{\lambda}_1\leq\ldots\leq\widehat{\lambda}_{N-m}$. Note that
  \[
    \ip{\vtU_\perp}{\oiP_{\vsU^\perp,\oiA\vsU}\oiA\vtU_\perp}
    = \ip{\vtU_\perp}{\oiA\vtU_\perp}
     -
     \ip{\vtU_\perp}{\oiA\vtU}\ip{\vtU}{\oiA\vtU}^\inv\ip{\oiA\vtU}{\vtU_\perp}
    = \ofM - \widehat{\vtR}\ofN^\inv\widehat{\vtR}^\htp,
  \]
  where $\widehat{\vtR}=\ip{\vtU_\perp}{\oiA\vtU}$. Since
  $\tilde{\lambda}_{i_1}\leq\ldots\leq\tilde{\lambda}_{i_{N-m}}$ are the
  eigenvalues of $\ofM$, Weyl's theorem (cf.\
  corollary~\ref{cor:rec:per:def:weyl}) and the equality
  \begin{align*}
    \nrm[2]{\widehat{\vtR}}
    &= \nrm[2]{\ip{\vtU_\perp}{\oiA\vtU}}
    = \nrm{\vtU_\perp\ip{\vtU_\perp}{\oiA\vtU}}
    = \nrm{\oiP_{\vsU^\perp}{\oiA\vtU}}
    = \nrm{(\id - \oiP_\vsU)\oiA\vtU}\\
    &= \nrm{\oiA\vtU - \vtU\ip{\vtU}{\oiA\vtU}}
    = \nrm{\vtR}
  \end{align*}
  yield for $k\in\{1,\ldots,N-m\}$
  \begin{equation}
    |\tilde{\lambda}_{i_k} - \widehat{\lambda}_k|
    \leq \nrm[2]{\widehat{\vtR}\ofN^\inv\widehat{\vtR}^\htp}
    \leq \nrm[2]{\widehat{\vtR}}^2 \nrm[2]{\ofN^\inv}
    = \frac{\nrm{\vtR}^2}{\mu_{\min}}.
    \label{eq:rec:per:def:quad:pf1}
  \end{equation}
  With theorem~\ref{thm:rec:per:def:mathias}, the eigenvalues
  $\tilde{\lambda}_{i_1}\leq\ldots\leq\tilde{\lambda}_{i_{N-m}}$ of $\ofM$ can
  now be related with the eigenvalues
  $\lambda_{i_1}\leq\ldots\leq\lambda_{i_{N-m}}$ of $\oiA$. For
  $k\in\{1,\ldots,N-m\}$, the theorem states that if $\delta_k>0$, then
  \begin{equation}
    |\lambda_{i_k} - \tilde{\lambda}_{i_k}|
    \leq \frac{\nrm{\vtR}^2}{\delta_k}.
    \label{eq:rec:per:def:quad:pf2}
  \end{equation}
  The inequalities~\eqref{eq:rec:per:def:quad:pf1} and
  \eqref{eq:rec:per:def:quad:pf2} now yield
  \[
    |\lambda_{i_k} - \widehat{\lambda}_k|
    \leq |\lambda_{i_k} - \tilde{\lambda}_{i_k}|
      + |\tilde{\lambda}_{i_k} - \widehat{\lambda}_k|
    \leq \nrm{\vtR}^2 \left(
      \frac{1}{\delta_k} + \frac{1}{\mu_{\min}}
    \right)
  \]
  which proves the first inequality of the theorem. Analogous to the second
  inequality of theorem~\ref{thm:rec:per:def:mathias}, the second inequality of
  this theorem follows by maximizing $\frac{1}{\delta_k}$.
\end{proof}

Compared to the angle-based bound in theorem~\ref{thm:rec:per:def:angle}, the
new bound in theorem~\ref{thm:rec:per:def:quad} is more attractive in several
ways. First, the most obvious feature is it's quadratic dependency on the
residual norm $\nrm{\vtR}$. Second, the bound uses the spectral gap instead of
the spectral interval gap and thus also allows the spectra to be scattered. The
only requirement is that the subset $\{\lambda_{i_1},\ldots,\lambda_{i_{N-m}}\}$
of the eigenvalues of $\oiA$ is disjoint from the spectrum of
$\ofN=\ip{\vtU}{\oiA\vtU}$. The spectral gap requirement is visualized in
figure~\ref{fig:rec:per:def:gap:quad}. Another benefit is that the first
inequality~\eqref{eq:rec:per:def:quad1} of theorem~\ref{thm:rec:per:def:quad}
gives an individual bound for each eigenvalue of
$\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ by taking into account the location of the
corresponding eigenvalue of the original operator $\oiA$.

\begin{figure}[htb]
  \centering
  \input{figures/rec_per_def_gap_quad.tex}
  \caption{Visualization of the assumption on the spectral gap in
    theorem~\ref{thm:rec:per:def:quad} for the case of two eigenvalues $\mu_1$
    and $\mu_2$ of $\ip{\vtU}{\oiA\vtU}$ that are scattered through the
    eigenvalues $\lambda_{k_1}\leq\ldots\leq\lambda_{k_{N-2}}$ of $\oiA$. The
    spectral gap $\delta$ is positive, whereas the spectral interval gap
    $\underline{\delta}$ is zero, cf.\ figure~\ref{fig:rec:per:def:gap3}. See
    definition~\ref{def:rec:per:def:gap} for the definition of the spectral
    gap.}
  \label{fig:rec:per:def:gap:quad}
\end{figure}

In order to assess the bound in theorem~\ref{thm:rec:per:def:quad}, the
following example demonstrates the behavior for the setup that was described in
example~\ref{ex:rec:per:def:angle}.

\begin{figure}[!htb]
  \centering
  \setlength{\figureheight}{0.5\textwidth}
  \begin{subfigure}[b]{\textwidth}
    \setlength{\figurewidth}{0.65\textwidth}
    \inputplot{exp_rec_per_def_quad1}
    \caption{Deflation of the 3 smallest eigenvalues
      $\lambda_1,\lambda_2$ and $\lambda_3$.}
    \label{fig:rec:per:def:quad1}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \setlength{\figurewidth}{0.65\textwidth}
    \inputplot{exp_rec_per_def_quad2}
    \caption{Deflation of the 2 smallest eigenvalues
      $\lambda_1$ and $\lambda_2$.}
    \label{fig:rec:per:def:quad2}
  \end{subfigure}
  \caption{Spectral bound from theorem~\ref{thm:rec:per:def:quad} for a deflated
    operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ with approximate invariant
    subspaces. The setup is described and discussed in
    example~\ref{ex:rec:per:def:quad}.}
  \label{fig:rec:per:def:quad}
\end{figure}

\begin{ex}
  \label{ex:rec:per:def:quad}
  Both experiments in example~\ref{ex:rec:per:def:angle} are repeated with the
  new bound. In figure~\ref{fig:rec:per:def:quad} the
  bound~\eqref{eq:rec:per:def:quad2} is plotted versus the perturbation
  $\varepsilon$. For comparison, the angle-based
  bound~\eqref{eq:rec:per:def:angle} is also included. In both cases, i.e., when
  deflating $\lambda_1,\lambda_2$ and $\lambda_3$ or when deflating $\lambda_1$
  and $\lambda_2$, the quadratic bound is clearly favorable for the full range
  of the perturbation $\varepsilon$. Remarkably, the quadratic bound does not
  show the offset of $\approx10^3$ for large perturbations and stays rather
  close to the actual eigenvalue error.
\end{ex}

The derived bounds are used in section~\ref{sec:rec:sel} in order to
select a deflation subspace when a sequence of linear systems has to be solved.

\subsection{Krylov subspace methods}
\label{sec:rec:per:kry}

In section~\ref{sec:rec:sel}, it is analyzed how a suitable deflation
subspace can be extracted from a Krylov subspace that was generated with an
operator and initial vector that correspond to a previous linear system in a
sequence of linear systems. The analysis leads to the following question: how
does the already known behavior of a Krylov subspace method that is applied to
$\oiA\vx=\vb$ relate to the behavior of the Krylov subspace method when it is
applied to a perturbed linear system $(\oiA+\oiF)\widehat{\vx}=\vb+\vf$?

The following statement describes a widespread opinion:
\begin{quote}
  A small perturbation $\oiF$ of the operator and a small perturbation $\vf$ of
  the initial vector only leads to a small perturbation of the convergence
  behavior of Krylov subspace methods for solving linear systems.
\end{quote}
The following example shows that this statement is not true in general.

\begin{ex}
  \label{ex:rec:per:kry:stag}
  The example shows results of the GMRES and MINRES methods with academic
  examples where the operator or the right hand side is perturbed.
  \begin{enumerate}
    \item For $n\in\N$ and $\alpha\gg\varepsilon>0$, let the matrices
      \[
        \ofB = \mat{0&0&1\\1&0&0\\0&1&0},
        \quad
        \ofC = \alpha\mat{0& &      &      &1\\
                    1&0&      &      & \\
                     &1&0     &      & \\
                     & &\ddots&\ddots& \\
                     & &      &1     &0}
                     \in\R^{n,n},
        \quad
        \ofF = \varepsilon \ve_4 \ve_1^\tp\in\R^{n+3,n+3}
      \]
      and the vectors
      \[
        \vb=\ve_1\in\R^{n+3}
        \quad\text{and}\quad
        \vf=\varepsilon\ve_4
      \]
      be given. With $n=20$, $\alpha=100$, $\varepsilon=10^{-8}$ and
      $\ofA=\diag(\ofB,\ofC)$, the GMRES method is now applied to the linear
      systems $\ofA\vx=\vb$, $(\ofA+\ofF)\vx_\ofF=\vb$ and
      $\ofA\vx_\vf=\vb+\vf$. The resulting convergence histories are shown in
      figure~\ref{fig:rec:per:kry:stag:gmres}. The original linear system is solved up
      to machine precision after 3 iterations while both perturbed versions
      exhibit $n=20$ iterations where the residual norm stagnates. In fact,
      increasing $n$, e.g., to $10^3$ or larger, still leads to $n$ iterations
      of stagnation. Here, $n=20$ is simply chosen because the resulting
      convergence histories begin to look ambiguous for large values of $n$.
      Varying $\varepsilon$ or $\alpha$ results in different levels where the
      stagnation takes place.

      Clearly, the example is constructed such that the Krylov subspace
      $\vsK_3(\ofA,\vb)$ is $\oiA$-invariant and the exact solution is thus
      found after 3 iterations (in fact, the exact solution is found
      numerically without round-off errors in this special case because only
      integer operations are performed). Adding the perturbation $\ofF$ or $\vf$
      drastically changes the behavior because the right hand side is no longer
      an element of a 3-dimensional invariant subspace and due to the dominance
      of the submatrix $\ofC$, a significant residual norm reduction just
      happens once the Krylov subspace has full dimension $n+3$.

    \item An interesting question is if such a behavior can also be observed
      with self-adjoint operators. Therefore, let
      \[
        \widehat{\ofB}=\ofB+\ofB^\tp,
        \quad
        \widehat{\ofC}
        =\alpha\mat{0&1&      &      \\
                    1&0&\ddots&       \\
                     &\ddots&\ddots&1 \\
                     &      &1&0 }
        \quad\text{and}\quad
        \widehat{\ofF}=\ofF+\ofF^\htp.
      \]
      Again with $n=20$, $\alpha=100$, $\varepsilon=10^{-8}$ and
      $\widehat{\ofA}=\diag(\widehat{\ofB},\widehat{\ofC})$, the MINRES method
      is applied to the three linear systems $\widehat{\ofA}\widehat{\vx}=\vb$,
      $(\widehat{\ofA}+\widehat{\ofF})\widehat{\vx}_\ofF=\vb$ and
      $\widehat{\ofA}\widehat{\vx}_\vf=\vb+\vf$. The resulting convergence
      histories are presented in figure~\ref{fig:rec:per:kry:stag:minres}.
      There is no phase of persistent complete stagnation like in the first
      experiment but the reduction of the residual norm is severely impeded by
      adding the perturbation $\widehat{\ofF}$ or $\vf$.
  \end{enumerate}
  Although this example is contrived, it stresses the fact that the GMRES
  residual can stagnate at any time during the iteration and that it can by no
  means be concluded that the residual norm reduction by a factor of $10^{-4}$
  from iteration 2 to iteration 3 is continued in subsequent iterations.
\end{ex}

\begin{figure}[!htb]
  \centering
  \setlength{\figureheight}{0.5\textwidth}
  \begin{subfigure}[b]{\textwidth}
    \setlength{\figurewidth}{0.65\textwidth}
    \inputplot{exp_rec_per_kry_stag_gmres}
    \caption{The GMRES method with a perturbed operator and right hand side.}
    \label{fig:rec:per:kry:stag:gmres}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \setlength{\figurewidth}{0.65\textwidth}
    \inputplot{exp_rec_per_kry_stag_minres}
    \caption{The MINRES method with a perturbed operator and right hand side.}
    \label{fig:rec:per:kry:stag:minres}
  \end{subfigure}
  \caption{Krylov subspace methods with a perturbed operator and right hand
    side. The setup is described and discussed in
    example~\ref{ex:rec:per:kry:stag}.}
  \label{fig:rec:per:kry:stag}
\end{figure}

Although the previous example draws a dark picture of perturbation theory for
Krylov subspace methods, useful bounds can be derived that characterize the
behavior of Krylov subspaces under perturbations. Similar to the perturbation
theory for deflated operators, the bounds often exhibit some information about
the spectrum that is usually not available in practice. However, the bounds lead
to a better understanding of the behavior of Krylov subspace methods and can
explain under which circumstances the statement in the fictional quote at the
beginning of this subsection is true.

Consider the GMRES method with initial guess $\vx_0=0$ for the linear systems
$\oiA\vx=\vb$ and $\widehat{\oiA}\widehat{\vx}=\widehat{\vb}$, where
$\widehat{\oiA}=\oiA+\oiF$ and $\widehat{\vb}=\vb+\vf$. Let the constructed
iterates and residuals be denoted by $\vx_n$, $\vr_n$, $\widehat{\vx}_n$ and
$\widehat{\vr}_n$. Assume that the residual $\vr_n$ or its norm $\nrm{\vr_n}$ is
known and the task is to bound the deviation of $\widehat{\vr}_n$ from $\vr_n$,
e.g., measured by $\nrm{\widehat{\vr}_n-\vr_n}$ or
$\nrm{\widehat{\vr}_n}-\nrm{\vr_n}\leq\nrm{\widehat{\vr}_n-\vr_n}$.

Several approaches to perturbations of Krylov subspaces and Krylov subspace
methods are known in the literature and serve very different purposes. Here, a
brief overview is presented before extending one of the approaches for the
situation described above.

In~\cite{CarGK96}, Carpraux, Godunov and Kuznetsov studied the sensitivity of a
Krylov subspace to a perturbation of the operator $\oiA$ and present an analysis
of the condition number of Arnoldi bases\footnote{The Arnoldi basis is called
  \emph{Krylov basis} in~\cite{CarGK96} which is used here for the basis
  $\vv,\oiA\vv,\ldots,\oiA^{n-1}\vv$, cf.\ section~\ref{sec:back:arnoldi}.}
and Krylov subspaces.  Unfortunately, the presented algorithm for the
estimation of the condition numbers involves the in practice infeasible solution
of a large linear system and does not lead to a deeper analytic understanding of
the behavior of Krylov subspaces. Kuznetsov extended the treatment
in~\cite{Kuz97} to perturbations of the initial vector $\vv$ and gave further
bounds for the Arnoldi basis and the corresponding Hessenberg matrix in terms of
the condition numbers derived in~\cite{CarGK96}.

Given any subspace $\vsU\subset\vsH$, Stewart showed in~\cite{Ste02}, how a
backward perturbation $\oiF$ with minimal norm $\nrm{\oiF}$ can be constructed
such that $\vsU$ is a Krylov subspace of $\oiA+\oiF$. A similar strategy is
applied in section~\ref{sec:rec:sel:kry} in order to construct a perturbation
$\oiF$ such that $\vsU$ is a Krylov subspace of $\oiA+\oiF$ with a
\emph{specific}
initial vector $\vu\in\vsU$ and such that the influence of the perturbation is
minimal \emph{in each iteration}.

For the case of dual Krylov subspace methods like BiCG or QMR,
Wu, Wei, Jia and Ling~\cite{WuWJL13} considered the case of two subspaces
$\vsU,\vsV\subset\vsH$ of same dimension and determined a backward perturbation
$\oiF$ such that $\vsU$ and $\vsV$ are Krylov subspaces of $\oiA+\oiF$ and its
adjoint, respectively.

The theory of inexact Krylov subspace methods by Simoncini and Szyld
in~\cite{SimS03} focuses on Krylov subspace methods where the application of
the operator $\oiA$ is not carried out exactly but instead a perturbed operator
$\oiA+\oiF_i$ is applied in each iteration $i$. The goal is to determine an
admissible perturbation norm $\nrm{\oiF_i}$ for each iteration such that the
method still constructs iterates whose residual norm eventually drops below a
given tolerance. The perturbations $\oiF_i$ of the original operator $\oiA$ are
allowed to change in each iteration but can be seen as the application of
$\oiA+\oiF$ with a constant operator $\oiF=\sum_{i=1}^n \oiF_i
\oiP_{\Span{\vv_i}}$, where $\vv_1,\ldots,\vv_n$ is the constructed orthonormal
basis. Thus, the iterates and residuals in \cite{SimS03} are $\widehat{\vx}_n$
and $\widehat{\vr}_n$ in the notation that is used here.  However, their
analysis differs from the situation here because in~\cite{SimS03}, the residual
norm $\nrm{\vb-\oiA\widehat{\vx}_n}$ is bounded and the residual $\vr_n$ that
corresponds to the Krylov subspace method applied to the unperturbed linear
system is not considered. The representation of the perturbation $\oiF$ as a sum
where each summand acts on one Arnoldi vector reappears in
section~\ref{sec:rec:sel:kry}.

A recent approach that almost fits the situation of this subsection has been
presented by Sifuentes, Embree and Morgan in~\cite{SifEM13}. They provide bounds
for the quantity $\nrm{\widehat{\vr}_n}-\nrm{\vr_n}$ for the case of a perturbed
operator but an unperturbed right hand side, i.e., $\vf=0$. Their analysis is
based on resolvent estimates and the pseudospectrum of the operator $\oiA$ and
is closely related to the pseudospectral residual norm bound for GMRES, cf.\
theorem~\ref{thm:back:gmres:pseudo}. The approach of Sifuentes, Embree and
Morgan is presented here in more detail and is slightly generalized in order to
also allow arbitrary perturbations $\vf$ of the right hand side and arbitrary
initial guesses.
Also, the same approach can be used for the CG method, where the theory becomes
much simpler due to the favorable spectral properties. First, the strategy for
the proof of the main result in the work of Sifuentes, Embree and
Morgan~\cite{SifEM13} is outlined.

Let $\oiA\vx=\vb$ and $\widehat{\oiA}\widehat{\vx}=\vb$ with nonsingular
$\oiA\in\vsL(\vsH)$ and $\widehat{\oiA}=\oiA+\oiF$ be two linear systems and let
$d_1=d(\oiA,\vb)$ and $d_2=d(\widehat{\oiA},\vb)$. For $n\leq\min \{d_1,d_2\}$,
the residuals $\vr_n$ and $\widehat{\vr}_n$ of the GMRES method applied to the
two linear systems with the zero initial guess can be expressed as
$\vr_n=p_n(\oiA)\vb$ and $\widehat{\vr}_n=\widehat{p}_n(\widehat{\oiA})\vb$ with
polynomials $p_n,\widehat{p}_n\in\Polys_{n,0}$. Due to the minimization property
of the GMRES polynomials $p_n$ and $\widehat{p}_n$ and the triangle inequality,
the following inequality can be obtained:
\begin{align*}
  \nrm{\widehat{\vr}_n} - \nrm{\vr}
  & = \nrm{\widehat{p}_n(\widehat{\oiA})\vb} - \nrm{p_n(\oiA)\vb}
  \leq \nrm{p_n(\widehat{\oiA})\vb} - \nrm{p_n(\oiA)\vb}
  \leq \nrm{p_n(\widehat{\oiA})\vb - p_n(\oiA)\vb} \\
  &\leq \nrm{p_n(\widehat{\oiA}) - p_n(\oiA)} \nrm{\vb}.
\end{align*}
If now $\Gamma\subset\C$ is a finite union of Jordan curves that enclose the
spectra $\Lambda(\oiA)$ and $\Lambda(\widehat{\oiA})$, then the polynomial
expressions can be written as a Cauchy integral similar to the proof of
theorem~\ref{thm:back:gmres:pseudo}:
\begin{align*}
  p_n(\oiA) = \frac{1}{2\pi\iu} \int_\Gamma p_n(\lambda) (\lambda\id-\oiA)^\inv
    d\lambda
  \quad\text{and}\quad
  p_n(\widehat{\oiA}) = \frac{1}{2\pi\iu} \int_\Gamma p_n(\lambda)
    (\lambda\id-\widehat{\oiA})^\inv d\lambda.
\end{align*}
Thus
\begin{align}
  \frac{\nrm{\widehat{\vr}_n} -\nrm{\vr_n}}{\nrm{\vb}}
  &\leq \frac{1}{2\pi}\nrm
    {\int_\Gamma p_n(\lambda) \left(
      (\lambda\id-\widehat{\oiA})^\inv - (\lambda\id-\oiA)^\inv
    \right) d\lambda} \notag \\
  &\leq \frac{1}{2\pi}
    \int_\Gamma |p_n(\lambda)|
    \nrm{(\lambda\id-\widehat{\oiA})^\inv - (\lambda\id-\oiA)^\inv}
    d\lambda. \label{eq:rec:per:kry:int1}
\end{align}
The difference of the resolvents can be tackled with the second resolvent
identity, cf.\ Nevanlinna~\cite{Nev93}:
\begin{equation}
  (\lambda\id-\widehat{\oiA})^\inv - (\lambda\id-\oiA)^\inv
  = (\lambda\id-\widehat{\oiA})^\inv \oiF (\lambda\id-\oiA)^\inv.
  \label{eq:rec:per:kry:resolvent}
\end{equation}
The first resolvent can be expressed as
\begin{align*}
  (\lambda\id-\widehat{\oiA})^\inv
  &= (\lambda\id-\oiA-\oiF)^\inv
  = \left(
      (\lambda\id-\oiA)\left(
        \id - (\lambda\id-\oiA)^\inv\oiF
      \right)^\inv
    \right)^\inv\\
  &= \left(
      \id - (\lambda\id-\oiA)^\inv\oiF
    \right)^\inv (\lambda\id-\oiA)^\inv.
\end{align*}
If $\varepsilon\DEF\nrm{\oiF}<\frac{1}{\nrm{(\lambda\id-\oiA)^\inv}}\FED\delta_\lambda$, then
$\nrm{(\lambda\id-\oiA)^\inv\oiF}\leq\frac{\varepsilon}{\delta_\lambda}<1$ and
$\left(\id - (\lambda\id-\oiA)^\inv\oiF\right)^\inv$ can be expressed as a
Neumann series, cf. Kato~\cite{Kat95}:
\[
  \left(\id - (\lambda\id-\oiA)^\inv\oiF\right)^\inv
  = \sum_{j=0}^\infty \left(
      (\lambda\id-\oiA)^\inv\oiF
    \right)^j.
\]
Its norm can be bounded by the geometric series
\[
  \nrm{\left(\id - (\lambda\id-\oiA)^\inv\oiF\right)^\inv}
  \leq \sum_{j=0}^\infty
    \left(\frac{\varepsilon}{\delta_\lambda}\right)^j
  = \frac{1}{1-\frac{\varepsilon}{\delta_\lambda}}
\]
and the norm of the resolvent difference~\eqref{eq:rec:per:kry:resolvent} can
thus be bounded by
\[
  \nrm{(\lambda\id-\widehat{\oiA})^\inv - (\lambda\id-\oiA)^\inv}
  \leq
    \frac{\varepsilon}{\delta_\lambda\left(\delta_\lambda-\varepsilon\right)}.
\]
For any $\delta>\varepsilon$, the boundary of the $\delta$-pseudospectrum
$\Lambda_\delta(\oiA)$ of $\oiA$ can be used as the contour of integration
in~\eqref{eq:rec:per:kry:int1} because it encloses by
definition~\ref{def:back:pseudo} the spectra $\Lambda(\oiA)$ and
$\Lambda(\widehat{\oiA})$. Note that $\delta = \delta_\lambda =
\frac{1}{\nrm{(\lambda\id-\oiA)^\inv}}$ holds for any
$\lambda\in\partial\Lambda_\delta(\oiA)$ and the following bound follows
directly:
\begin{align*}
  \frac{\nrm{\widehat{\vr}_n} - \nrm{\vr_n}}{\nrm{\vb}}
  &\leq \frac{\varepsilon}{\delta-\varepsilon}
    ~\frac{|\partial\Lambda_\delta(\oiA)|}{2\pi\delta}
    \max_{\lambda\in\partial\Lambda_\delta(\oiA)} |p_n(\lambda)|
  = \frac{\varepsilon}{\delta-\varepsilon}
    ~\frac{|\partial\Lambda_\delta(\oiA)|}{2\pi\delta}
    \sup_{\lambda\in\Lambda_\delta(\oiA)} |p_n(\lambda)|.
\end{align*}
As in theorem~\ref{thm:back:gmres:pseudo}, the last equality holds because $p_n$
is holomorphic and thus its maximum is attained on the boundary. Note that
$|\partial\Lambda_\delta(\oiA)|$ again denotes the curve's arc length. The above
inequality is the main result of of Sifuentes, Embree and Morgan~\cite{SifEM13}
and is gathered in the following theorem for later reference.

\begin{thm}
  \label{thm:rec:per:kry:op}
  Assume that $\oiA\in\vsL(\vsH)$ is nonsingular, $\oiF\in\vsL(\vsH)$,
  $\vb\in\vsH$ and $n\leq d(\oiA,\vb)$.  Let the $n$-th residual of the GMRES
  method applied to $\oiA\vx=\vb$ with initial guess $\vx_0=0$ be denoted by
  $\vr_n=p_n(\oiA)\vb$ with $p_n\in\Polys_{n,0}$.

  Then for all $\delta>\varepsilon\DEF\nrm{\oiF}$, the $n$-th residual
  $\widehat{\vr}_n$ of the GMRES method applied to
  $(\oiA+\oiF)\widehat{\vx}=\vb$ with initial guess $\vx_0=0$ satisfies
  \begin{equation}
    \frac{\nrm{\widehat{\vr}_n} - \nrm{\vr_n}}{\nrm{\vb}}
    \leq \frac{\varepsilon}{\delta-\varepsilon}
      ~\frac{|\partial\Lambda_\delta(\oiA)|}{2\pi\delta}
      \sup_{\lambda\in\Lambda_\delta(\oiA)} |p_n(\lambda)|.
    \label{eq:rec:per:kry:op}
  \end{equation}
\end{thm}

\begin{proof}
  See above or the original proof in~\cite{SifEM13}.
\end{proof}

Sifuentes, Embree and Morgan noted in~\cite{SifEM13}, that the bound in
theorem~\ref{thm:rec:per:kry:op} has major parts in common with the
pseudospectral GMRES bound for an unperturbed linear system in
theorem~\ref{thm:back:gmres:pseudo}. Furthermore, they provided another bound
that resembles the bound in theorem~\ref{thm:back:gmres:pseudo} even more
closely. However, this bound is not of relevance here and it is referred
to~\cite{SifEM13} for details.

Similar to the pseudospectral GMRES bound in theorem~\ref{thm:back:gmres:pseudo},
theorem~\ref{thm:rec:per:kry:op} does not provide an answer to the question of
how $\delta>\varepsilon$ should be chosen. Again, on the one hand, $\delta$
should be chosen larger than $\varepsilon$ such that the factor $\frac{1}{\delta-\varepsilon}$
becomes small. On the other hand $\delta$ should be close to $\varepsilon$
such that the pseudospectrum $\Lambda_\delta(\oiA)$ and with it its boundary
length $|\partial\Lambda_\delta(\oiA)|$ and the supremum of the polynomial on
the pseudospectrum does not grow too large.

Before moving on, theorem~\ref{thm:rec:per:kry:op} is discussed with regard
to example~\ref{ex:rec:per:kry:stag}.

\begin{ex}
  \label{ex:rec:per:kry:stag:bound}
  In example~\ref{ex:rec:per:kry:stag}, the convergence of the GMRES method was
  severely delayed by a perturbation of the size
  $\varepsilon=\nrm{\oiF}=10^{-8}$. The inequality in
  theorem~\ref{thm:rec:per:kry:op} can only bound the residual for GMRES applied
  to the perturbed linear system for the first 3 iterations because then the
  exact solution of the unperturbed linear system is found. In the third
  iteration, the GMRES polynomial for the unperturbed linear system is given by
  \[
    p_n(\lambda) = 1 - \lambda^3 = \prod_{j=1}^{3}(1-\frac{\lambda}{\zeta_3^j}),
  \]
  where $\zeta_k=e^{\frac{2\pi\iu}{k}}$ is an $k$-th root of unity for $k\in\N$.
  Note that $\zeta_3,\zeta_3^2$ and $\zeta_3^3$ are the eigenvalues of $\ofB$.
  However, the polynomial has no roots close to the eigenvalues
  $\alpha\zeta_n,\alpha\zeta_n^2,\ldots,\alpha\zeta_n^n$ of $\ofC$ and the
  supremum in bound~\eqref{eq:rec:per:kry:op} is always greater than
  $\alpha^3-1=10^6-1$. In
  figure~\ref{fig:rec:per:kry:stag:gmres:bound}, the bound is evaluated
  numerically for a wide range of $\delta$. The minimal value of the bound is
  $\inputraw{exp_rec_per_kry_bound.txt}$ and thus differs by one order of
  magnitude from the level of the residual norm stagnation in
  figure~\ref{fig:rec:per:kry:stag:gmres} which is
  $\inputraw{exp_rec_per_kry_stag.txt}$. However, beyond the third iteration,
  the bound can give no insight into the convergence behavior of the perturbed
  linear system. This example is constructed such that the GMRES
  method for the unperturbed problem has gathered no ``knowledge'' about the
  spectrum of the submatrix $\ofC$ and the supremum in
  bound~\eqref{eq:rec:per:kry:op} cannot be small. The bound can only give more
  insightful answers to the question of how perturbations affect the GMRES
  convergence if the GMRES polynomial $p_n$ has zeros distributed across the
  full spectrum of $\oiA$ and not only a small part of it that is well-separated
  from the rest.

  The construction of the boundaries of pseudospectra is achieved with the free
  software Python package PseudoPy~\cite{pseudopy} which was developed by the
  author. PseudoPy is a Python version of the original EigTool~\cite{Wri02} by
  Wright. Details on the algorithms in PseudoPy and EigTool can be found in the
  works by Trefethen~\cite{Tre90,Tre99} and his coauthors Toh~\cite{TohT96} and
  Wright~\cite{WriT01,WriT02}. An extensive treatment of the computation of
  pseudospectra can be found in the book of Trefethen and Embree~\cite{TreE05}.
  In this example, the matrix $\oiA$ is normal and hence the pseudospectrum is
  the union of circles around the eigenvalues of $\oiA$:
  \[
    \Lambda_\delta(\oiA)
    = \bigcup_{\lambda\in\Lambda(\oiA)} \{ \mu\in\C~|~|\lambda-\mu|<\delta\}.
  \]
\end{ex}

\begin{figure}[htb]
  \centering
  \setlength{\figureheight}{0.5\textwidth}
  \setlength{\figurewidth}{0.65\textwidth}
  \inputplot{exp_rec_per_kry_stag_gmres_bound}
\caption{Evaluation of the bound~\eqref{eq:rec:per:kry:op} for
  example~\ref{ex:rec:per:kry:stag:bound} with $\delta\in]10^{-8},10^8]$. The
  minimum is attained for $\delta\approx\inputraw{exp_rec_per_kry_delta.txt}$
  where the bound evaluates to $\inputraw{exp_rec_per_kry_bound.txt}$. Note that
  the level of the residual norm stagnation in
  figure~\ref{fig:rec:per:kry:stag:gmres} is
  $\inputraw{exp_rec_per_kry_stag.txt}$.}
  \label{fig:rec:per:kry:stag:gmres:bound}
\end{figure}

The following theorem generalizes the result of Sifuentes, Embree and Morgan to
the case where not only the linear operator $\oiA$ but also the right hand side
$\vb$ and the initial guess $\vx_0$ can be perturbed.

\begin{thm}
  \label{thm:rec:per:kry:full}
  Assume that $\oiA\vx=\vb$ is a linear system with an operator
  $\oiA\in\vsL(\vsH)$, right hand side $\vb\in\vsH$ and let $\vx_0\in\vsH$ be an
  initial guess with corresponding initial residual $\vr_0=\vb-\oiA\vx_0$. For
  $n\leq d(\oiA,\vr_0)$, let the $n$-th residual of the GMRES method applied to
  $\oiA\vx=\vb$ with initial guess $\vx_0$ be denoted by $\vr_n=p_n(\oiA)\vr_0$
  with $p_n\in\Polys_{n,0}$.

  Let $\widehat{\oiA}\widehat{\vx}=\widehat{\vb}$ be a perturbed
  linear system with $\widehat{\oiA}=\oiA+\oiF$ and $\widehat{\vb}=\vb+\vf$,
  where $\oiF\in\vsL(\vsH)$ and $\vf\in\vsH$, and let $\widehat{\vx}_0\in\vsH$
  be an initial guess with corresponding residual
  $\widehat{\vr}_0=\widehat{\vb}-\widehat{\oiA}\widehat{\vx}_0$.

  Then for all $\delta>\varepsilon\DEF\nrm{\oiF}$, the $n$-th residual
  $\widehat{\vr}_n$ of the GMRES method applied to
  $(\oiA+\oiF)\widehat{\vx}=\vb+\vf$ with initial guess $\widehat{\vx}_0$
  satisfies
  \begin{align}
    \nrm{\widehat{\vr}_n} - \nrm{\vr_n}
    &\leq \frac{|\partial\Lambda_\delta(\oiA)|}{2\pi\delta}
      \left( \frac{\varepsilon}{\delta-\varepsilon}
        \nrm{\widehat{\vr}_0} +\nrm{\widehat{\vr}_0 - \vr_0}
      \right)
      \sup_{\lambda\in\Lambda_\delta(\oiA)} |p_n(\lambda)|
      \label{eq:rec:per:kry:full1}
    \intertext{and also}
    \nrm{\widehat{\vr}_n} - \nrm{\vr_n}
    &\leq \frac{|\partial\Lambda_\delta(\widehat{\oiA})|}{2\pi\delta}
      \left( \frac{\varepsilon}{\delta-\varepsilon}
        \nrm{\vr_0} +\nrm{\widehat{\vr}_0 - \vr_0}
      \right)
      \sup_{\lambda\in\Lambda_\delta(\widehat{\oiA})} |p_n(\lambda)|.
      \notag
  \end{align}
\end{thm}

\begin{proof}
  The proof is very similar to the proof of theorem~\ref{thm:rec:per:kry:op} and
  its major difference is that the separation of the operator polynomials and
  the initial residuals is deferred until the end. In order to prove
  inequality~\eqref{eq:rec:per:kry:full1}, let
  $\widehat{\vr}_n=\widehat{p}_n(\widehat{\oiA})\widehat{\vr}_0$ with
  $\widehat{p}_n\in\Polys_{n,0}$. The minimization property of $\widehat{p}_n$
  and the triangle inequality yield
  \[
    \nrm{\widehat{\vr}_n} - \nrm{\vr_n}
    = \nrm{\widehat{p}_n(\widehat{\oiA})\widehat{\vr}_0} - \nrm{p_n(\oiA)\vr_0}
    \leq \nrm{p_n(\widehat{\oiA})\widehat{\vr}_0}
    - \nrm{p_n(\oiA)\vr_0}
    \leq \nrm{p_n(\widehat{\oiA}) \widehat{\vr}_0
    - p_n(\oiA)\vr_0}.
  \]
  Because $\delta>\epsilon=\nrm{\oiF}$, the pseudospectrum
  $\Lambda_\delta(\oiA)$ contains the spectra
  $\Lambda(\oiA)$ and $\Lambda(\widehat{\oiA})$ and thus the operator polynomials
  can be expressed as Cauchy integrals over the pseudospectrum's boundary:
  \begin{align}
    \nrm{\widehat{\vr}_n} - \nrm{\vr_n}
    &\leq \frac{1}{2\pi}\nrm{\int_{\partial\Lambda_\delta(\oiA)} p_n(\lambda)
      \left(
        (\lambda\id-\widehat{\oiA})^\inv \widehat{\vr}_0
        - (\lambda\id-\oiA)^\inv \vr_0
      \right)
      d\lambda} \notag \\
    &\leq \frac{1}{2\pi}\int_{\partial\Lambda_\delta(\oiA)} |p_n(\lambda)|
      \nrm{(\lambda\id-\widehat{\oiA})^\inv \widehat{\vr}_0
        - (\lambda\id-\oiA)^\inv \vr_0}
      d\lambda.
    \label{eq:rec:per:kry:full:pf1}
  \end{align}
  Analogous to the proof of
  theorem~\ref{thm:rec:per:kry:op}, the first resolvent can be expressed as
  \[
    (\lambda\id-\widehat{\oiA})^\inv
    = (\id-(\lambda\id-\oiA)^\inv\oiF)^\inv(\lambda\id-\oiA)^\inv.
  \]
  By the definition of the pseudospectrum,
  $\delta=\frac{1}{\nrm{(\lambda\id-\oiA)^\inv}}$ for all
  $\lambda\in\partial\Lambda_\delta(\oiA)$ and the above resolvent expression
  can be formulated with a Neumann series:
  \[
    (\lambda\id-\widehat{\oiA})^\inv
    = \sum_{j=0}^\infty \left(
        (\lambda\id-\oiA)^\inv\oiF
      \right)^j (\lambda\id-\oiA)^\inv.
  \]
  This equation yields with the geometric series
  \begin{align*}
    &\nrm{(\lambda\id-\widehat{\oiA})^\inv
      \widehat{\vr}_0
      - (\lambda\id-\oiA)^\inv \vr_0} \\
    &\qquad\qquad = \nrm{\sum_{j=1}^\infty \left(
          (\lambda\id-\oiA)^\inv\oiF
        \right)^j
        (\lambda\id-\oiA)^\inv \widehat{\vr}_0
        +
        (\lambda\id-\oiA)^\inv \left(
          \widehat{\vr}_0 - \vr_0
        \right)} \\
        &\qquad\qquad \leq \frac{1}{\delta} \left(
            \sum_{j=1}^\infty \left(\frac{\varepsilon}{\delta}\right)^j
            \nrm{\widehat{\vr}_0}
            + \nrm{\widehat{\vr}_0 - \vr_0}
          \right)
          = \frac{1}{\delta}\left(
            \left(\frac{1}{1-\frac{\varepsilon}{\delta}} - 1\right)
            \nrm{\widehat{\vr}_0}
            + \nrm{\widehat{\vr}_0 - \vr_0}
          \right) \\
        &\qquad\qquad = \frac{1}{\delta}\left(
            \frac{\varepsilon}{\delta-\varepsilon}
            \nrm{\widehat{\vr}_0}
            + \nrm{\widehat{\vr}_0 - \vr_0}
            \right).
  \end{align*}
  Inserting this inequality into~\eqref{eq:rec:per:kry:full:pf1} and bounding
  the polynomial yields the inequality~\eqref{eq:rec:per:kry:full1}.

  The second inequality can be proved analogously by using the pseudospectrum
  $\Lambda_\delta(\widehat{\oiA})$ and expressing the resolvent
  $(\lambda\id-\oiA)^\inv$ with a Neumann series in terms of the resolvent
  $(\lambda\id-\widehat{\oiA})^\inv$ and the perturbation $\oiF$.
\end{proof}

\begin{rmk}
  Theorem~\ref{thm:rec:per:kry:full} gives a bound on the absolute residual
  norm. Relative residual norms can be achieved trivially by pre-multiplying the
  linear systems with $\frac{1}{\nrm{\widehat{\vb}}}$ and $\frac{1}{\nrm{\vb}}$,
  respectively.

  The operator $\oiA$ is not required to be nonsingular in
  theorem~\ref{thm:rec:per:kry:full} but the statement becomes trivial if
  $0\in\Lambda(\oiA)\subseteq\Lambda_\delta(\oiA)$. Nevertheless, allowing the
  operator $\oiA$ to be nonsingular in the above formulation
  eases the application of the theorem in section~\ref{sec:rec:sel:kry}.

  Also note that if the right hand sides coincide and the zero initial guess is
  used for both linear systems in theorem~\ref{thm:rec:per:kry:full}, i.e.,
  $\vb=\widehat{\vb}$ and $\vx_0=\widehat{\vx}_0=0$, then the
  bound~\eqref{eq:rec:per:kry:full1} reduces to the bound by Sifuentes, Embree
  and Morgan in theorem~\ref{thm:rec:per:kry:op}.
\end{rmk}

Clearly, theorem~\ref{thm:rec:per:kry:full} also holds for MINRES
if the operators $\oiA$ and $\oiF$ are self-adjoint. In this case, the
pseudospectrum in inequality~\eqref{eq:rec:per:kry:full1} can be replaced by the
union of $\delta$-intervals around the eigenvalues of $\oiA$, i.e.,
\[
  \Lambda_\delta(\oiA)
  =\bigcup_{\lambda\in\Lambda(\oiA)} [\lambda-\delta,\lambda+\delta].
\]

Similarly, the approach of theorem~\ref{thm:rec:per:kry:full} can be applied to
the $\oiA$-norm of the error in the CG method. The derivation is analogous and
is not presented here.

Another perturbation result for Krylov subspaces is based on
theorem~\ref{thm:rec:per:kry:full} in section~\ref{sec:rec:sel:kry} in the
context of selection strategies of deflation vectors for sequences of linear
systems.


\section{Selection of recycling vectors}
\label{sec:rec:sel}

In the introduction of chapter~\ref{ch:rec}, the question of recycling data in
Krylov subspace methods was split into two separate questions. The first one was
the question of how to incorporate external data into a Krylov subspace method
in order to influence its convergence behavior. This question was addressed in
sections~\ref{sec:rec:strat} and \ref{sec:rec:def} with a focus on deflated
Krylov subspace methods. The previous section~\ref{sec:rec:per} paved the way to
attack the second question, which is: which data from a dataset with possible
recycling data results in the best overall performance? Since this thesis
concentrates on deflation, the question boils down to the following: given a
subspace $\vsW\subseteq\vsH$ with candidates for recycling data, select a
deflation subspace $\vsU\subseteq\vsW$ such that the deflated Krylov subspace
method performs best.

Before moving on, it has to be made clear what is asked for by stating more
precisely what ``best performance'' means here. When analyzing deflated Krylov
subspace methods, it appears to be mathematically sound to measure the number of
iterations that are needed in order to reduce the residual below a prescribed
tolerance. Of course, the goal is in practice to reduce the time that is needed
to solve the linear system up to a given tolerance with the available memory.
Thus, instead of counting the raw number of iterations, the \emph{computational
cost} can be measured in terms of computation time and memory requirements. With
respect to the computational cost, some thoughts have to be kept in mind. The
first addresses the tolerance itself. In many applications, the linear systems
are solved up to a very high accuracy which is often unnecessary because other
errors like discretization errors for discretized partial differential equations
or errors in parameters or measurements dominate the overall error. Thus, the
tolerance should be adapted to the properties of the underlying problem, see,
e.g., the articles by Arioli, Noulard and Russo~\cite{AriNR01} and Arioli,
Loghin and Wathen~\cite{AriLW05} for linear systems and the PhD thesis of
Mi\k{e}dlar~\cite{Mie11} for eigenvalue problems in the context of partial
differential equations. The second issue is that the finite precision behavior
of Krylov subspace methods may deviate significantly from their exact arithmetic
counterparts, see section~\ref{sec:back:round-off}.  Another issue is that
counting the overall number of floating point operations (\emph{flops}) is often
impossible because of complex preconditioners, e.g., (algebraic) multigrid
preconditioners that again constitute an iterative method with possibly varying
runtime performance inside the Krylov subspace method.

Unfortunately, the convergence behavior of Krylov subspace methods is hard to
predict accurately a priori (cf.\ sections~\ref{sec:back:cg} and \ref{sec:back:mr})
and thus -- even if the above problems were non-existent -- the determination of
an \emph{optimal} deflation subspace seems to be far from possible with
today's knowledge. It comes as no real surprise that simple heuristics are
customary in the literature for the selection of deflation subspaces. As already
mentioned in section~\ref{sec:rec:def:mot}, there is a strong focus on using
eigenvector approximations for the deflation subspace, e.g., Ritz or harmonic
Ritz vectors from a given subspace $\vsW$.

For a prescribed integer $m$, Parks et al.~\cite{ParSMJM06} as well as Wang, de
Sturler and Paulino~\cite{WanSP07} chose $m$ harmonic Ritz vectors from
previously computed Krylov subspaces in the GCRO-DR and the RMINRES method (cf.\
Morgan~\cite{Mor02} and section~\ref{sec:rec:def:aug}). An analogous strategy is
performed in the context of sequences of shifted linear systems in the works of
Darnell, Morgan and Wilcox~\cite{DarMW08} and Soodhalter, Szyld and
Xue~\cite{SooSX13}.  In~\cite{GauS13}, the author and Schl\"{o}mer used $m$ Ritz
vectors in the deflated MINRES method (cf.\ the second variant of the deflated
MINRES method in section~\ref{sec:rec:def:minres}). The heuristic of picking the
$m$ (harmonic) Ritz vectors that correspond to the (harmonic) Ritz values of
smallest magnitude works well in the above cases, but a major drawback of all
methods is that the number of chosen deflation vectors $m$ has to be defined in
advance. In each iteration, the application of the projections
$\oiP_{\vsU^\perp,\oiA\vsU}$ and $\oiP_{(\oiA\vsU)^\perp}$ with an
$m$-dimensional subspace $\vsU$ to a vector needs at least $m$ inner products
and $m$ vector updates. Furthermore, adding a ``bad'' Ritz vector to the
deflation subspace may result in no decrease but instead in an increase of the
number of iterations. For example, in the context of Newton's method for a
nonlinear Schr\"{o}dinger equation, it was noticed in~\cite{GauS13} that the
strategy of always choosing $m$ deflation vectors does not pay off in the early
phase of Newton's method because of large changes in the operators and because
the Ritz vectors are poor eigenvector approximations in the first steps of
Newton's method.

As mentioned above, the determination of an \emph{optimal} deflation subspace
without any heuristics seems to be impossible today. However, the perturbation
theory that was developed in section~\ref{sec:rec:per} can be used to assess a
given deflation subspace and to drive the existing heuristics forward. A careful
evaluation of given candidates for a deflation subspace can lead to deflated
Krylov subspace methods that automatically determine the number of needed
deflation vectors in order to reduce the overall computation time. In order to
do so, such methods may need to keep track of timings of potentially costly
operations inside the iterations, such as the application of the operator $\oiA$
and a preconditioner. The construction of such estimations is the subject of
this section.

In order to keep the notational overhead at a minimum, only two subsequent
linear systems of a sequence~\eqref{eq:rec:seq} are considered in this section.
The two linear systems are
\begin{equation}
  \oiA \vx = \vb
  \qquad\text{and}\qquad
  \oiB \vy = \vc
  \label{eq:rec:sel:ls}
\end{equation}
with nonsingular operators $\oiA,\oiB\in\vsL(\vsH)$ and right hand sides
$\vb,\vc\in\vsH$.  Furthermore, it is assumed that the first linear system
$\oiA\vx=\vb$ has been solved approximately with a well-defined deflated Krylov
subspace method from section~\ref{sec:rec:def}, cf.\
table~\ref{tab:rec:def:imp:overview} for an overview. Thus, for an initial guess
$\vx_0$ and a deflation subspace $\vsU\subseteq\vsH$ with
$\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$, the underlying Krylov subspace
method was not applied to $\oiA\vx=\vb$ but to the deflated linear system
\begin{equation}
  \widehat{\oiA}\widehat{\vx}=\widehat{\vb},
  \label{eq:rec:sel:ls-def}
\end{equation}
where $\widehat{\oiA}=\oiP\oiA$ and $\widehat{\vb}=\oiP\vb$ for a
$\oiP\in\{\oiP_{\vsU^\perp,\oiA\vsU},\oiP_{(\oiA\vsU)^\perp}\}$. If no deflation
space is used, e.g., for the first linear system, then all results hold with
$\vsU=\{0\}$ and $\oiP=\id$.
Let $n\in\N$ be
the number of required iterations and $\vx_n$ the approximate solution
of the original linear system~\eqref{eq:rec:sel:ls}, i.e.,
$\vx_n=c(\overline{\vx}_n)$ is the corrected version of the approximate solution
$\overline{\vx}_n\in\vx_0+\vsK_n(\widehat{\oiA},\widehat{\vr}_0)$ of the
deflated linear system~\eqref{eq:rec:sel:ls-def} with
$\widehat{\vr}_0=\widehat{\vb}-\widehat{\oiA}\vx_0$.

The next subsection deals with the extraction of Ritz and harmonic Ritz pairs
for the ``undeflated'' operator $\oiA$ from the data that was generated while
solving the first deflated linear system~\eqref{eq:rec:sel:ls-def}, i.e.,
with the deflated operator $\oiP\oiA$. The two subsequent subsections describe
new strategies to assess a given set of deflation vectors. The first strategy is
to use the perturbation theory for the spectrum of deflated operators from
section~\ref{sec:rec:per:def} in order to evaluate a priori bounds for the
convergence of the deflated Krylov subspace method for the next linear system,
e.g., the CG or MINRES bounds in sections~\ref{sec:back:cg} and
\ref{sec:back:mr:minres}. The second strategy is to extract an Arnoldi relation
for a deflated linear system that is ``close'' to the deflated linear system
that is about to be solved by only using data from the linear system that has
already been solved. Then the perturbation theory for Krylov subspace methods
from section~\ref{sec:rec:per:kry} can be applied in order to estimate a few
steps of the convergence behavior of the next linear system with the given set
of deflation vectors.


\subsection{Computation of Ritz and harmonic Ritz pairs}
\label{sec:rec:sel:ritz}

Before discussing the actual selection of recycling vectors, this subsection
briefly describes how Ritz and harmonic Ritz pairs can be obtained in the
setting of a sequence of linear systems.
The computation of Ritz and harmonic Ritz pairs is a fairly straightforward
application of the results in section~\ref{sec:back:ritz} and is only slightly
complicated by the fact that Ritz or harmonic Ritz pairs have to be computed for
the original operator $\oiA$ but the Krylov subspace has been built with a
projected operator $\oiP\oiA$. This constellation makes the presentation in this
subsection look very technical but the underlying ideas are simple and the
extraction procedures for Ritz and harmonic Ritz pairs are easy to implement.

In this section, it is assumed that
\begin{equation}
  \vtV_{n+1}=[\vv_1,\ldots,\vv_{n+1}]\in\vsH^{n+1}
  \quad\text{and}\quad
  \underline{\ofH}_n=\mat{\ofH_n\\h_{n+1,n}\ve_n^\tp}\in\C^{n+1,n}
  \label{eq:rec:sel:arnoldi}
\end{equation}
define an Arnoldi relation for the Krylov subspace
$\vsK_n=\vsK_n(\widehat{\oiA},\widehat{\vr}_0)$, cf.\
section~\ref{sec:back:arnoldi}. If the Krylov subspace $\vsK_n$ is
$\oiA$-invariant, it is assumed that $\vtV_n=[\vv_1,\ldots,\vv_n]\in\vsH^n$ and
$\ofH_n\in\C^{n,n}$ define an invariant Arnoldi relation for $\vsK_n$.
Because the invariant Arnoldi relation is a rare case in practice, only the
regular Arnoldi relation is considered for reasons of better readability. The
invariant Arnoldi relation can always be treated analogously and only leads to
minor modifications of the statements and their proofs. The data for the Arnoldi
relation is generated explicitly or implicitly in all Krylov subspace methods,
e.g., it is directly available after $n$ steps of the GMRES and MINRES
algorithms.  If $\oiA$ is self-adjoint, the Arnoldi relation reduces to the
Lanczos relation, i.e., $\ofH_n=\ofH_n^\htp$, and the tridiagonality of $\ofH_n$
leads to short recurrences in the CG and MINRES algorithms. Although these
methods only require to store 3 vectors, all Arnoldi/Lanczos vectors are assumed
to be available for deflation purposes. In the CG algorithm, the Lanczos basis
$\vtV_{n+1}$ and Lanczos matrix $\underline{\ofH}_n$ can be recovered
efficiently from the quantities that are computed in
algorithm~\ref{alg:back:pcg}. The extraction of the Lanczos relation from the CG
method is, e.g., implemented in \lstinline{krypy.linsys.Cg} in~\cite{krypy}.

The subspaces that are considered here for the extraction of Ritz and harmonic
Ritz pairs are the Krylov subspace
$\vsK_n=\vsK_n(\widehat{\oiA},\widehat{\vr}_0)$ and the deflation
subspace $\vsU$ that defines the used projection $\oiP$. From these two subspaces
the Ritz pairs can be computed as follows in the case
$\oiP=\oiP_{\vsU^\perp,\oiA\vsU}$.

\begin{lemma}[Ritz pairs with $\oiP_{\vsU^\perp,\oiA\vsU}$]
  \label{lem:rec:sel:ritz-cg}
  Let $\oiA\in\vsL(\vsH)$, $\vv\in\vsH$ and $\vtU\in\vsH^m$ with
  $\ip{\vtU}{\vtU}=\ofI_m$ and $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$,
  where $\vsU=\Span{\vtU}$.  Assume that $\vtV_{n+1}\in\vsH^{n+1}$ and
  $\underline{\ofH}_n\in\C^{n+1,n}$ define an Arnoldi relation for
  $\vsK_n=\vsK_n(\oiP_{\vsU^\perp,\oiA\vsU}\oiA,\oiP_{\vsU^\perp,\oiA\vsU}\vv)$
  with the usual decomposition~\eqref{eq:rec:sel:arnoldi}. Let
  $\ofY=[\vy_1,\ldots,\vy_{n+m}]\in\C^{n+m,n+m}$ and
  $\mu_1,\ldots,\mu_{n+m}\in\C$ solve the eigenvalue problem
  \begin{equation}
    \mat{\ofH_n + \ofB\ofE^\inv\ofC & \ofB\\ \ofC & \ofE}\ofY
    = \ofY\diag(\mu_1,\ldots,\mu_{n+m})
    \label{eq:rec:sel:ritz-cg}
  \end{equation}
  with $\ofB=\ip{\vtV_n}{\oiA\vtU}$, $\ofE=\ip{\vtU}{\oiA\vtU}$ and
  $\ofC=\ip{\vtU}{\oiA\vtV_n}$.

  Then $(\vw_1,\mu_1),\ldots,(\vw_{n+m},\mu_{n+m})$ are the Ritz pairs of $\oiA$
  with respect to $\vsK_n+\vsU$, where $\vw_i=[\vtV_n,\vtU]\vy_i$ for
  $i\in\{1,\ldots,n+m\}$. The Ritz residuals satisfy
  \begin{equation*}
    \nrm{\oiA\vw_i - \mu_i\vw_i}
    = \sqrt{(\ofG_i\vy_i)^\htp\ofS \ofG_i\vy_i},
  \end{equation*}
  where
  \begin{align*}
    \ofG_i = \mat{\underline{\ofH}_n -\mu_i\underline{\ofI}_n & 0\\
                \ofE^\inv\ofC & \ofI_m \\
                0 & -\mu_i\ofI_m},
    \quad
    \ofS = \mat{\ofI_{n+1} & \underline{\ofB} & 0 \\
                \underline{\ofB}^\htp & \ofF & \ofE \\
                0 & \ofE^\htp & \ofI_m},\\
    \underline{\ofI}_n = \mat{\ofI_n\\0},\quad
    \underline{\ofB}=\ip{\vtV_{n+1}}{\oiA\vtU}
    = \mat{\ofB\\\ip{\vv_{n+1}}{\oiA\vtU}}
    \quad\text{and}\quad
    \ofF=\ip{\oiA\vtU}{\oiA\vtU}.
  \end{align*}
\end{lemma}

\begin{proof}
  First note that $[\vtV_n,\vtU]$ is orthonormal and that
  \begin{align*}
    \oiP_{\vsU^\perp,\oiA\vsU}\oiA\vtV_n &= \vtV_{n+1}\underline{\ofH}_n\\
    \LLRA\qquad
    \oiA\vtV_n
    &= \vtV_{n+1}\underline{\ofH}_n + \oiP_{\oiA\vsU,\vsU^\perp}\oiA\vtV_n
    = \vtV_{n+1}\underline{\ofH}_n
    + \oiA\vtU\ip{\vtU}{\oiA\vtU}^\inv\ip{\vtU}{\oiA\vtV_n} \\
    &= \vtV_{n+1}\underline{\ofH}_n
    + \oiA\vtU\ofE^\inv\ofC.
  \end{align*}
  That $(\vw_i,\mu_i)_{i\in\{1,\ldots,n+m\}}$ are the Ritz pairs of $\oiA$ with
  respect to $\vsK_n+\vsU$ follows directly from remark~\ref{rmk:back:ritz} by
  noticing that
  \[
    \ip{[\vtV_n,\vtU]}{\oiA[\vtV_n,\vtU]}
    = \mat{\ip{\vtV_n}{\oiA\vtV_n} & \ip{\vtV_n}{\oiA\vtU}\\
           \ip{\vtU}{\oiA\vtV_n} & \ip{\vtU}{\oiA\vtU}}
    = \mat{\ofH_n+\ofB\ofE^\inv\ofC & \ofB \\
           \ofC & \ofE}.
  \]
  It remains to show the residual norm identity. Therefore observe that
  \begin{align*}
    \oiA\vw_i - \mu_i\vw_i
    &= \oiA[\vtV_n,\vtU]\vy_i - \mu_i[\vtV_n,\vtU]\vy_i
    = [\vtV_{n+1}\underline{\ofH}_n + \oiA\vtU\ofE^\inv\ofC,\oiA\vtU]\vy_i
      - \mu_i [\vtV_n,\vtU]\vy_i\\
    &= [\vtV_{n+1},\oiA\vtU,\vtU]
    \mat{\underline{\ofH}_n-\mu_i\underline{\ofI}_n & 0\\
         \ofE^\inv\ofC & \ofI_m \\
         0 & -\mu_i\ofI_m} \vy_i
    = [\vtV_{n+1},\oiA\vtU,\vtU] \ofG_i \vy_i.
  \end{align*}
  The proof is complete by computing
  \[
    \nrm{\oiA\vw_i-\mu_i\vw_i}^2
    = (\ofG_i\vy_i)^\htp
      \ip{[\vtV_{n+1},\oiA\vtU,\vtU]}{[\vtV_{n+1},\oiA\vtU,\vtU]} \ofG_i \vy_i
    = (\ofG_i\vy_i)^\htp \ofS \ofG_i \vy_i.
  \]
  Note that
  $\ofS=\ip{[\vtV_{n+1},\oiA\vtU,\vtU]}{[\vtV_{n+1},\oiA\vtU,\vtU]}$ is
  Hermitian and positive semidefinite.
\end{proof}

The following remark gives details on the impact of the Ritz pair computation on
the computational cost.

\begin{rmk}
  \label{rmk:rec:sel:ritz}
  The matrix $\ofE=\ip{\vtU}{\oiA\vtU}$ has to be formed anyway in order to
  construct the projection $\oiP_{\vsU^\perp,\oiA\vsU}$. The matrix $\ofC$ can
  be retrieved from the Arnoldi or Lanczos algorithm because in each iteration
  $1\leq i\leq n$, the operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ has to be
  applied to the $i$-th Arnoldi/Lanczos vector $\vv_i$ and from
  \[
    \oiP_{\vsU^\perp,\oiA\vsU}\oiA\vv_i
    = \oiA\vv_i - \oiA\vtU\ofE^\inv \ip{\vtU}{\oiA\vv_i}
  \]
  it becomes clear that the columns of the matrix $\ofC$ are formed one after
  another during the iteration. In the CG algorithm, the matrix $\ofC$ can also
  be formed efficiently with a three-term recurrence which is implemented
  in~\cite{krypy} in \lstinline{krypy.deflation.DeflatedCg}. Furthermore, note
  that if $\oiA$ is self-adjoint, then $\ofC=\ofB^\htp$ and $\ofE=\ofE^\htp$.
  Only if $\oiA$ is non-self-adjoint, $n\cdot m$ additional inner products have
  to be performed in order to compute the matrix $\ofB=\ip{\vtV_n}{\oiA\vtU}$.
  However, no additional applications of $\oiA$ are necessary because $\oiA\vtU$
  can and should be kept in memory. For the computation of the Ritz residual
  norms, $\frac{m(m+1)}{2}$ additional inner products have to be computed for
  $\ip{\vv_{n+1}}{\oiA\vtU}$ and the matrix $\ofF=\ip{\oiA\vtU}{\oiA\vtU}$ but
  again no applications of $\oiA$ are necessary.
\end{rmk}

The harmonic Ritz pairs of $\oiA$ with respect to $\vsK_n+\vsU$ can be computed
as follows in the case $\oiP=\oiP_{\vsU^\perp,\oiA\vsU}$.

\begin{lemma}[Harmonic Ritz pairs with $\oiP_{\vsU^\perp,\oiA\vsU}$]
  \label{lem:rec:sel:hritz-cg}
  Let $\oiA\in\vsL(\vsH)$ be nonsingular, $\vv\in\vsH$ and $\vtU\in\vsH^m$ with
  $\ip{\vtU}{\vtU}=\ofI_m$ and $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$,
  where $\vsU=\Span{\vtU}$. Assume that $\vtV_{n+1}\in\vsH^{n+1}$ and
  $\underline{\ofH}_n\in\C^{n+1,n}$ define an Arnoldi relation for
  $\vsK_n=\vsK_n(\oiP_{\vsU^\perp,\oiA\vsU}\oiA,\oiP_{\vsU^\perp,\oiA\vsU}\vv)$
  with the usual decomposition~\eqref{eq:rec:sel:arnoldi}. Let
  $\ofY=[\vy_1,\ldots,\vy_{n+m}]\in\C^{n+m,n+m}$ and
  $\sigma_1,\ldots,\sigma_{n+m}\in\C$ solve the generalized eigenvalue problem
  \begin{equation*}
    \mat{\ofH_n + \ofB\ofE^\inv\ofC & \ofB\\ \ofC &\ofE}^\htp\ofY
    = (\ofL^\htp\ofK\ofL) \ofY \diag(\sigma_1,\ldots,\sigma_{n+m})
  \end{equation*}
  with $\ofL=\mat{\underline{\ofH}_n & 0\\ \ofE^\inv\ofC & \ofI_m}$,
  $\ofK=\mat{\ofI_{n+1} & \underline{\ofB} \\ \underline{\ofB}^\htp &\ofF}$,
  where the matrices $\ofB,\underline{\ofB},\ofE,\ofC,\ofF$ are defined as in
  lemma~\ref{lem:rec:sel:ritz-cg}.

  Then with
  \[
    \mu_i\DEF
    \begin{cases}
      \frac{1}{\sigma_i} & \text{if}~\sigma_i\neq0\\
      \infty & \text{else}
    \end{cases}
    \quad\text{and}\quad
    \vw_i=[\vtV_n,\vtU]\vy_i
    \quad\text{for}~i\in\{1,\ldots,n+m\},
  \]
  the pairs $(\vw_1,\mu_1),\ldots,(\vw_{n+m},\mu_{n+m})$ are the harmonic Ritz
  pairs of $\oiA$ with respect to $\vsK_n+\vsU$. If $\mu_i\neq\infty$ for a
  $i\in\{1,\ldots,n+m\}$, then the Ritz pair $(\vw_i,\mu_i)$ satisfies
  \begin{equation*}
    \nrm{\oiA\vw_i - \mu_i\vw_i}
    = \sqrt{(\ofG_i\vy_i)^\htp\ofS\ofG_i\vy_i}
    = \sqrt{|\mu_i(\mu_i-\rho_i)|}
    \leq |\mu_i|,
  \end{equation*}
  where the matrices $\ofG_i,\ofS$ are defined as in
  lemma~\ref{lem:rec:sel:ritz-cg} and
  \[
    \rho_i=\vy_i^\htp\mat{\ofH_n+\ofB\ofE^\inv\ofC & \ofB\\\ofC & \ofE}\vy_i.
  \]
\end{lemma}

\begin{proof}
  According to definition~\eqref{def:back:ritz-harm} and
  equation~\eqref{eq:back:ritz-harm}, the generalized eigenvalue problem
  \[
    \ip{\oiA\vtS}{\vtS}\ofY
    = \ip{\oiA\vtS}{\oiA\vtS}\ofY\diag(\sigma_1,\ldots,\sigma_{n+m})
  \]
  can be solved with $\vtS=[\vtV_n,\vtU]$ in order to obtain the desired
  harmonic Ritz pairs. The matrix on the left hand side is
  just the Hermitian transpose of the matrix in~\eqref{eq:rec:sel:ritz-cg}.
  For the matrix on the right hand side, observe that
  \[
    \oiA[\vtV_n,\vtU]
    = [\vtV_{n+1},\oiA\vtU]
      \mat{\underline{\ofH}_n & 0 \\ \ofE^\inv\ofC & \ofI_m}
    = [\vtV_{n+1},\oiA\vtU] \ofL
  \]
  and thus
  \begin{align*}
    \ip{\oiA[\vtV_n,\vtU]}{\oiA[\vtV_n,\vtU]}
    &= \ofL^\htp \ip{[\vtV_{n+1},\oiA\vtU]}{[\vtV_{n+1},\oiA\vtU]}\ofL\\
    &= \ofL^\htp \mat{\ip{\vtV_{n+1}}{\vtV_{n+1}} & \ip{\vtV_{n+1}}{\oiA\vtU}\\
                     \ip{\oiA\vtU}{\vtV_{n+1}} & \ip{\oiA\vtU}{\oiA\vtU}}\ofL
    = \ofL^\htp \ofK \ofL.
  \end{align*}
  The first residual norm equality is analogous to the corresponding part of the
  proof of lemma~\ref{lem:rec:sel:ritz-cg} while the second equality and the
  inequality follow from lemma~\ref{lem:back:ritz-harm-res}.
\end{proof}

If the projection $\oiP=\oiP_{(\oiA\vsU)^\perp}$ is used, the Ritz pairs can be
obtained as described in the following lemma:

\begin{lemma}[Ritz pairs with $\oiP_{(\oiA\vsU)^\perp}$]
  \label{lem:rec:sel:ritz-mr}
  Let $\oiA\in\vsL(\vsH)$, $\vv\in\vsH$ and $\vtU\in\vsH^m$ with
  $\ip{\vtU}{\vtU}=\ofI_m$ and $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$,
  where $\vsU=\Span{\vtU}$. Furthermore, let $\ofR\in\C^{m,m}$ be such that
  $\vtZ=\oiA\vtU\ofR$ fulfills $\ip{\vtZ}{\vtZ}=\ofI_m$. Assume that
  $\vtV_{n+1}\in\vsH^{n+1}$ and $\underline{\ofH}_n\in\C^{n+1,n}$ define an
  Arnoldi relation for
  $\vsK_n=\vsK_n(\oiP_{(\oiA\vsU)^\perp}\oiA,\oiP_{(\oiA\vsU)^\perp}\vv)$ with
  the usual decomposition~\eqref{eq:rec:sel:arnoldi}. Let
  $\ofY=[\vy_1,\ldots,\vy_{n+m}]\in\C^{n+m,n+m}$ and
  $\mu_1,\ldots,\mu_{n+m}\in\C$ solve the generalized eigenvalue problem
  \begin{equation}
    \mat{\ofH_n & 0 \\
      \underline{\widehat{\ofB}}^\htp\underline{\ofH}_n
      + \ofE\ofR\widehat{\ofC}& \ofE}\ofY
      = \mat{\ofI_n & \widehat{\ofB}\\
             \widehat{\ofB}^\htp & \ofI_m}
        \ofY\diag(\mu_1,\ldots,\mu_{n+m})
    \label{eq:rec:sel:ritz-mr}
  \end{equation}
  with
  $\underline{\widehat{\ofB}}=\ip{\vtV_{n+1}}{\vtU}
  =\mat{\widehat{\ofB}\\\ip{\vv_{n+1}}{\vtU}}$,
  $\widehat{\ofC}=\ip{\vtZ}{\oiA\vtV_n}$, $\oiE=\ip{\vtU}{\oiA\vtU}$ and
  $\underline{\ofI}_n=\mat{\ofI_n\\0}$.

  Then $(\vw_1,\mu_1),\ldots,(\vw_{n+m},\mu_{n+m})$ are the Ritz pairs of $\oiA$
  with respect to $\vsK_n+\vsU$, where $\vw_i=[\vtV_n,\vtU]\vy_i$ for
  $i\in\{1,\ldots,n+m\}$. The Ritz residuals satisfy
  \begin{equation*}
    \nrm{\oiA\vw_i - \mu_i\vw_i}
    = \sqrt{(\widehat{\ofG}_i\vy_i)^\htp\widehat{\ofS} \widehat{\ofG}_i\vy_i},
  \end{equation*}
  where
  \[
    \widehat{\ofG}_i
    = \mat{\underline{\ofH}_n - \mu_i\underline{\ofI}_n & 0\\
           \widehat{\ofC} & \ofR^\inv \\
           0 & -\mu_i\ofI_m},
    \quad\text{and}\quad
    \widehat{\ofS}
    = \mat{\ofI_{n+1} & 0 & \widehat{\ofB}\\
           0 & \ofI_m  & (\ofE\ofR)^\htp\\
          \widehat{\ofB}^\htp & \ofE\ofR & \ofI_m}.
  \]
\end{lemma}

\begin{proof}
  First note that $\oiP_{(\oiA\vsU)^\perp}\vx=\vx-\vtZ\ip{\vtZ}{\vx}$ holds for
  $\vx\in\vsH$. Then $\vsK_n\subseteq\Span{\vtZ}^\perp$ shows that
  $[\vtV_n,\vtZ]$ is orthogonal. From the Arnoldi relation and the definition
  of $\vtZ$ it can be seen that
  \begin{equation}
    \oiA[\vtV_n,\vtU]
    = [\vtV_{n+1}\underline{\ofH}_n + \vtZ\ip{\vtZ}{\oiA\vtV_n},\oiA\vtU]
    = [\vtV_{n+1},\vtZ] \underbrace{
                             \mat{\underline{\ofH}_n & 0\\
                             \widehat{\ofC} & \ofR^\inv}
                           }_{\FED\widehat{\ofL}}.
    \label{eq:rec:sel:ritz-mr:pf}
  \end{equation}
  With $\vtS=[\vtV_n,\vtU]$, the Ritz pairs can be obtained by solving the
  generalized eigenvalue problem
  \[
    \ip{\vtS}{\oiA\vtS}\ofY = \ip{\vtS}{\vtS}\ofY\diag(\mu_1,\ldots,\mu_{n+m}).
  \]
  The inner product matrix on the left hand side is
  \[
    \ip{\vtS}{\oiA\vtS}
    = \ip{[\vtV_n,\vtU]}{[\vtV_{n+1},\vtZ]}\widehat{\ofL}
    = \mat{\underline{\ofI}_n^\htp & 0 \\
      \widehat{\ofB}^\htp & \ofE\ofR} \widehat{\ofL}
    = \mat{\ofH_n & 0 \\
      \underline{\widehat{\ofB}}^\htp\underline{\ofH}_n
      + \ofE\ofR\widehat{\ofC}& \ofE}
  \]
  while the matrix on the right hand side is
  \[
    \ip{[\vtV_n,\vtU]}{[\vtV_n,\vtU]}
    = \mat{\ofI_n & \ip{\vtV_n}{\vtU}\\
           \ip{\vtU}{\vtV_n} & \ofI_m}
    = \mat{\ofI_n & \widehat{\ofB} \\
           \widehat{\ofB}^\htp & \ofI_m}.
  \]
  It remains to show the residual norm equality. Note that
  \begin{align*}
    \oiA\vw_i -\mu_i\vw_i
    &= [\vtV_{n+1},\vtZ]\widehat{\ofL}\vy_i - \mu_i[\vtV_n,\vtU]\vy_i
    = [\vtV_{n+1},\vtZ,\vtU]
      \mat{\underline{\ofH}_n - \mu_i\underline{\ofI}_n & 0 \\
           \widehat{\ofC} & \ofR^\inv \\
           0 & -\mu_i\ofI_m}\vy_i \\
    &= [\vtV_{n+1},\vtZ,\vtU] \widehat{\ofG}_i\vy_i
  \end{align*}
  and thus
  \begin{align*}
    \nrm{\oiA\vw_i-\mu_i\vw_i}^2
    &= (\widehat{\ofG}_i\vy_i)^\htp
      \ip{[\vtV_{n+1},\vtZ,\vtU]}{[\vtV_{n+1},\vtZ,\vtU]}
      \widehat{\ofG}_i\vy_i \\
    &= (\widehat{\ofG}_i\vy_i)^\htp
      \mat{\ofI_{n+1} & 0 & \ip{\vtV_{n+1}}{\vtU} \\
            0 & \ofI_m & \ip{\vtZ}{\vtU} \\
            \ip{\vtU}{\vtV_{n+1}} & \ip{\vtU}{\vtZ} & \ofI_m}
      \widehat{\ofG}_i\vy_i
    = (\widehat{\ofG}_i\vy_i)^\htp \widehat{\ofS}
      \widehat{\ofG}_i\vy_i.
  \end{align*}
\end{proof}

\begin{rmk}
  As noted in remark~\ref{rmk:rec:sel:ritz} for the case of
  $\oiP=\oiP_{\vsU^\perp,\oiA\vsU}$, the matrix $\widehat{\ofC}$ is implicitly
  constructed in the Arnoldi/Lanczos algorithm. The matrices $\ofR$,
  $\underline{\widehat{\ofB}}$ and $\oiE$ have to be computed but no
  applications of $\oiA$ are involved.

  If $\oiA$ is self-adjoint, then the matrix on the left hand side of
  equation~\eqref{eq:rec:sel:ritz-mr} is Hermitian, i.e.,
  $\widehat{\underline{\ofB}}^\htp\underline{\ofH}_n +
  \ofE\ofR\widehat{\ofC}=0$.
\end{rmk}

\begin{lemma}[Harmonic Ritz pairs with $\oiP_{(\oiA\vsU)^\perp}$]
  \label{lem:rec:sel:hritz-mr}
  Let $\oiA\in\vsL(\vsH)$ be nonsingular, $\vv\in\vsH$ and $\vtU\in\vsH^m$ with
  $\ip{\vtU}{\vtU}=\ofI_m$ and $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$,
  where $\vsU=\Span{\vtU}$. Furthermore, let $\ofR\in\C^{m,m}$ be such that
  $\vtZ=\oiA\vtU\ofR$ fulfills $\ip{\vtZ}{\vtZ}=\ofI_m$. Assume that
  $\vtV_{n+1}\in\vsH^{n+1}$ and $\underline{\ofH}_n\in\C^{n+1,n}$ define an
  Arnoldi relation for
  $\vsK_n=\vsK_n(\oiP_{(\oiA\vsU)^\perp}\oiA,\oiP_{(\oiA\vsU)^\perp}\vv)$ with
  the usual decomposition~\eqref{eq:rec:sel:arnoldi}. Let
  $\ofY=[\vy_1,\ldots,\vy_{n+m}]\in\C^{n+m,n+m}$ and
  $\sigma_1,\ldots,\sigma_{n+m}\in\C$ solve the generalized eigenvalue problem
  \begin{equation*}
    \mat{\ofH_n & 0 \\
      \underline{\widehat{\ofB}}^\htp\underline{\ofH}_n
      + \ofE\ofR\widehat{\ofC}& \ofE}^\htp \ofY
      = \widehat{\ofL}^\htp \widehat{\ofL}
        \ofY\diag(\sigma_1,\ldots,\sigma_{n+m}),
  \end{equation*}
  where $\widehat{\ofL}=\mat{\underline{\ofH}_n & 0 \\
  \widehat{\ofC} & \ofR^\inv}$ and $\widehat{\ofB}$, $\widehat{\ofC}$ and $\ofE$
  are defined as in lemma~\ref{lem:rec:sel:ritz-mr}.

  Then with
  \[
    \mu_i\DEF
    \begin{cases}
      \frac{1}{\sigma_i} & \text{if}~\sigma_i\neq0\\
      \infty & \text{else}
    \end{cases}
    \quad\text{and}\quad
    \vw_i=[\vtV_n,\vtU]\vy_i
    \quad\text{for}~i\in\{1,\ldots,n+m\},
  \]
  the pairs $(\vw_1,\mu_1),\ldots,(\vw_{n+m},\mu_{n+m})$ are the harmonic Ritz
  pairs of $\oiA$ with respect to $\vsK_n+\vsU$. If $\mu_i\neq\infty$ for a
  $i\in\{1,\ldots,n+m\}$, then the Ritz pair $(\vw_i,\mu_i)$ satisfies
  \begin{equation*}
    \nrm{\oiA\vw_i - \mu_i\vw_i}
    = \sqrt{(\widehat{\ofG}_i\vy_i)^\htp\widehat{\ofS}\widehat{\ofG}_i\vy_i}
    = \sqrt{|\mu_i(\mu_i-\rho_i)|}
    \leq |\mu_i|,
  \end{equation*}
  where the matrices $\widehat{\ofG}_i,\widehat{\ofS}$ are defined as in
  lemma~\ref{lem:rec:sel:ritz-mr} and
  \[
    \rho_i=\vy_i^\htp\mat{\ofH_n & 0 \\
    \widehat{\underline{\ofB}}^\htp\underline{\ofH}_n+\ofE\ofR\widehat{\ofC} &
    \ofE}\vy_i.
  \]
\end{lemma}

\begin{proof}
  With $\vtS=[\vtV_n,\vtU]$, the Ritz pairs can again be computed by solving the
  generalized eigenvalue problem
  \[
    \ip{\oiA\vtS}{\vtS}\ofY
    = \ip{\oiA\vtS}{\oiA\vtS}\ofY\diag(\sigma_1,\ldots,\sigma_{n+m}).
  \]
  The inner product matrix on the left hand side is the Hermitian transpose of
  the matrix on the left hand side of equation~\eqref{eq:rec:sel:ritz-mr}. For
  the inner product matrix on the right hand side, note that
  equation~\eqref{eq:rec:sel:ritz-mr:pf} also holds with the assumptions of this
  lemma and the inner product becomes
  \[
    \ip{\oiA\vtS}{\oiA\vtS}
    =\widehat{\ofL}^\htp \widehat{\ofL}.
  \]
  The residual norm statement can be shown analogously to the corresponding part
  of the proof of lemma~\ref{lem:rec:sel:hritz-cg}.
\end{proof}

In the literature, the computation of Ritz pairs or harmonic Ritz pairs has been
carried out along the lines of the (generalized) eigenvalue problems in the
above lemmas. Parks et al.~\cite{ParSMJM06} and Wang, de Sturler and
Paulino~\cite{WanSP07} computed harmonic Ritz pairs similarly to
lemma~\ref{lem:rec:sel:hritz-mr}. It should be noted, that the possibility of an
infinite harmonic Ritz value is largely overlooked in the literature when it
comes to the actual computation of harmonic Ritz pairs, see also
section~\ref{sec:back:ritz}. Regular Ritz pairs have been computed in the manner
of lemma~\ref{lem:rec:sel:ritz-cg} by the author and Schl\"{o}mer
in~\cite{GauS13}.


\subsection{Estimation with a priori bounds}
\label{sec:rec:sel:pri}

In this subsection, it is assumed that both operators $\oiA$ and $\oiB$ of the
linear systems in~\eqref{eq:rec:sel:ls} are self-adjoint and that the first
linear system has been solved approximately via the deflated linear
system~\eqref{eq:rec:sel:ls-def}. Furthermore, it is assumed that Ritz or
harmonic Ritz pairs of the original operator $\oiA$ have been computed as
presented in the previous subsection. The aim of this subsection is to derive a
strategy for selecting some of the computed Ritz or harmonic Ritz vectors in
order to use them as a basis for the deflation subspace for the second linear
system in~\eqref{eq:rec:sel:ls}. The selection strategy takes into account
the a priori bounds for the CG and MINRES methods that have been presented in
sections~\ref{sec:back:cg} and \ref{sec:back:mr:minres}. For this subsection let
$(\vw_1,\mu_1),\dots,(\vw_t,\mu_t)\in\vsH\times\R$ be the computed Ritz or
harmonic Ritz pairs with corresponding residual norms
$\nrm{\oiA\vw_i-\mu_i\vw_i}$ for $i\in\{1,\dots,t\}$. As usual, it is assumed
that the Ritz or harmonic Ritz values are sorted such that
$\mu_1\leq\dots\leq\mu_t$ and that the corresponding vectors
$\vw_1,\ldots,\vw_t$ are normalized.

In section~\ref{sec:rec:per:def} it was shown for a self-adjoint operator
$\oiA$, that if an approximation $\vsU\subseteq\vsH$ to an $\oiA$-invariant
subspace with associated spectrum $\Lambda_1\subset\R$ is used as the deflation
subspace, then the spectrum of the deflated operator
$\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ consists of zero and a perturbed version of the
spectrum $\Lambda_2$ of $\oiA$ that is complementary to $\Lambda_1$.
Theorem~\ref{thm:rec:per:def:quad} shows that the difference of the eigenvalues
can be bounded quadratically by the Ritz residual norm
$\nrm{\vtR}=\nrm{\oiP_{\vsU^\perp}\oiA\oiP_{\vsU}}$ of the subspace $\vsU$ and a
the spectral gap between the Ritz values that correspond to the deflation
subspace $\vsU$ and the spectrum $\Lambda_2$. The results of
theorem~\ref{thm:rec:per:def:quad} help to theoretically understand the spectrum
of the deflated operator $\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ in the case where the
spectrum of $\oiA$ is fully disclosed. In practice, this is usually not the case
but some insight into the spectrum of $\oiA$ can be gained from the computed
Ritz or harmonic Ritz values and their residuals. The following lemma is due to
Cao, Xie and Li~\cite{CaoXL96} and is an improved version of a theorem by
Kahan~\cite{Kah67}; see also Davis, Kahan and Weinberger~\cite{DavKW82}.

\begin{lemma}
  \label{lem:rec:sel:pri}
  Let $\oiA\in\vsH$ be self-adjoint, $\ofM=\ofM^\htp\in\C^{m,m}$ and
  $\vtZ\in\vsH^m$ of rank $m$. Let $\lambda_1\leq\dots\leq\lambda_N$ be the
  eigenvalues of $\oiA$ and let $\mu_1\leq\dots\leq\mu_m$ be the eigenvalues of
  $\ofM$.

  Then there exist indices $1\leq j_1<\dots<j_m\leq N$ such that
  \[
    \max_{i\in\{1,\dots,m\}} |\lambda_{j_i} - \mu_i|
    \leq \frac{\nrm{\oiA\vtZ-\vtZ\ofM}}{\sigma_{\min}(\vtZ)},
  \]
  where $\sigma_{\min}(\vtZ)$ is the smallest singular value of $\vtZ$.
\end{lemma}

\begin{proof}
  See Cao, Xie and Li~\cite{CaoXL96}.
\end{proof}

Let $I=\{i_1<\dots<i_m\}\subset\{1,\dots,t\}$,
$J=\{j_1<\dots<j_{t-m}\}
=\{1,\dots,t\}\setminus I$ and define
$\vtW_I=[\vw_{i_1},\dots,\vw_{i_m}]$,
$\ofM_I=\diag(\mu_{i_1},\dots,\mu_{i_m})$,
$\vtW_J=[\vw_{j_1},\dots,\vw_{j_{t-m}}]$ and
$\ofM_J=\diag(\mu_{j_1},\dots,\mu_{j_{t-m}})$.
Lemma~\ref{lem:rec:sel:pri} now guarantees that there exist indices $1\leq
k_1<\dots<k_m\leq N$ such that
\[
  \max_{s\in\{1,\dots,m\}} |\lambda_{k_s} - \mu_{i_s}|
  \leq \frac{\nrm{\oiA\vtW_I-\vtW_I\ofM_I}}{\sigma_{\min}(\vtW_I)}
  \FED \delta_I,
\]
where $\lambda_1\leq\dots\leq\lambda_N$ are the eigenvalues of $\oiA$.
Likewise, the lemma guarantees that there exist indices $1\leq
l_1<\dots<l_{t-m}\leq N$ such that
\[
  \max_{s\in\{1,\dots,t-m\}} |\lambda_{l_i} - \mu_{l_i}|
  \leq \frac{\nrm{\oiA\vtW_J -\vtW_J\ofM_J}} {\sigma_{\min}(\vtW_J)}
  \FED \delta_J.
\]
Assume that the Ritz or harmonic Ritz pairs satisfy the following spectral gap
condition (cf.\ definition~\ref{def:rec:per:def:gap}):
\begin{equation}
  \delta_I+\delta_J <
  \delta(\{\mu_{i_1},\dots,\mu_{i_m}\},
  \{\mu_{j_1},\dots,\mu_{j_{t-m}}\}) \FED \delta.
  \label{eq:rec:sel:pri:gap}
\end{equation}
Then it can be concluded that there exist indices $1\leq p_1<\dots<p_t\leq N$
such that
\begin{align*}
  \max_{i\in I} |\lambda_{p_i} - \mu_i|
  \leq \delta_I 
  \quad\text{and}\quad
  \max_{j\in J} |\lambda_{p_j} - \mu_j|
  \leq \delta_J.
\end{align*}
The above condition~\eqref{eq:rec:sel:pri:gap} and a possible distribution of
$\oiA$'s eigenvalues is visualized in figure~\ref{fig:rec:sel:pri:gap}.
Lemma~\ref{lem:rec:sel:pri} can give valuable insight in the distribution of $t$
eigenvalues of $\oiA$ but nothing can be said about the remaining $N-t$
eigenvalues in general.

\begin{figure}[ht]
  \centering
  \input{figures/rec_sel_pri_gap.tex}
  \caption{Visualization of the spectral gap
    condition $\delta_I+\delta_J<\delta$ for Ritz or harmonic Ritz
    pairs $(\vw_1,\mu_1),\dots,(\vw_t,\mu_t)$ and a possible distribution of the
    eigenvalues $\lambda_1,\dots,\lambda_N$ of $\oiA$, cf.\
    equation~\eqref{eq:rec:sel:pri:gap}.
    Lemma~\ref{lem:rec:sel:pri} states that the intervals around $\mu_1$,
    $\mu_2$ and $\mu_t$ each contain at least one eigenvalue of $\oiA$ while the
    union of the intervals around $\mu_3$ and $\mu_4$ contains at least two
    eigenvalues. The union of the intervals around $\mu_5,\dots,\mu_{t-1}$ has
    to contain at least $t-5$ eigenvalues. Note that lemma~\ref{lem:rec:sel:pri}
    cannot provide information about the remaining $N-m$ eigenvalues and that
    eigenvalues like $\lambda_2$ can lie outside the intervals.}
  \label{fig:rec:sel:pri:gap}
\end{figure}

This is the point where the heuristics enter the analysis because the actual
spectrum of $\oiA$ is unknown in general. If more is known about $\oiA$'s
spectrum, e.g., containment intervals for the eigenvalues and information about
the number of eigenvalues in these intervals, then this information can be used
to improve the heuristics of this subsection or even construct actual a priori
bounds.
If nothing more is known about $\oiA$'s eigenvalues, then one strategy
is to assume that the set $\bigcup_{i\in I}[\mu_i-\delta_I,\mu_i+\delta_I]$
contains exactly $m$ eigenvalues of $\oiA$ and $\bigcup_{j\in
J}[\mu_j-\delta_J,\mu_j+\delta_J]$
contains \emph{all} remaining $N-m$ eigenvalues and not only the $t-m$
eigenvalues that are guaranteed to be contained by lemma~\ref{lem:rec:sel:pri}.

In practice, it can often be observed that some Ritz or harmonic Ritz
pairs approximate certain eigenpairs of $\oiA$ very well once the Krylov
subspace method has found an approximate solution. For example, after the last
iteration of MINRES with the undeflated linear system in
example~\ref{ex:rec:per:def:minres}, the three Ritz pairs with smallest Ritz
value magnitude approximate the three eigenpairs that correspond to the
eigenvalues $\lambda_1=-10^{-3}$, $\lambda_2=-10^{-4}$ and $\lambda_3=-10^{-5}$.
The residual norms of these Ritz pairs lie around $10^{-11}$ while all other
Ritz pairs exhibit large residual norms between $10^{-2}$ and $10^{-1}$. It was
shown by Simoncini and Szyld in~\cite{SimS13}, that MINRES enters the
phase of superlinear convergence once some harmonic Ritz values approximate well
the eigenvalues of smallest or largest magnitude. Often, the corresponding
eigenvalues are well-separated from the remaining spectrum and the above
heuristic can be considered as justified.

In the described situation, the following theorem narrows down the spectrum of
the deflated operator $\oiP_{\vsW_I^\perp,\oiB\vsW_I}\oiB$ with the deflation
subspace $\vsW_I=\Span{\vtW_I}$ based on properties of the difference
$\oiF=\oiB-\oiA$ and Ritz residuals. The statement appears to be complicated at
first sight because of quite technical assumptions but its underlying arguments
are simple and the train of thought is the following: with proper assumptions on
$\oiA$'s spectrum and the available Ritz pairs, the spectrum of $\oiB$ is
characterized with the difference $\oiF$ via Weyl's theorem, cf.\
theorem~\ref{thm:rec:per:def:weyl}. Then the new quadratic residual bound from
theorem~\ref{thm:rec:per:def:quad} can be used in order to localize the
eigenvalues of $\oiP_{\vsW_I^\perp,\oiB\vsW_I}\oiB$. To the knowledge of the
author, no comparable result is known in the literature.

\begin{thm}
  \label{thm:rec:sel:pri}
  Let $\oiA,\oiB\in\vsL(\vsH)$ be self-adjoint with
  $\Lambda(\oiA)=\{\lambda_1\leq\dots\leq\lambda_N\}$, $\oiF=\oiB-\oiA$,
  $\Lambda(\oiF)=\{\epsilon_1\leq\dots\leq\epsilon_N\}$ and let
  $(\vw_1,\mu_1),\dots,(\vw_t,\mu_t)\in\vsH\times\R$ be $t$ Ritz pairs of $\oiA$
  with $\nrm{\vw_1}=\dots=\nrm{\vw_t}=1$.
  For $m\in\N$ and an index set
  $I=\{i_1<\dots<i_m\}\subset\{1,\dots,t\}$, let $J=\{j_1<\dots<j_{t-m}\}
  =\{1,\dots,t\}\setminus I$ and define
  \begin{align*}
    \vtW_I &= [\vw_{i_1},\dots,\vw_{i_m}],&
    \vsW_I &= \Span{\vtW_I},&
    \ofD_I &= \diag(\mu_{i_1},\dots,\mu_{i_m}),&
    \Lambda_I &= \{\mu_i\}_{i\in I},\\
    \vtW_J &= [\vw_{j_1},\dots,\vw_{j_{t-m}}],&
    \vsW_J &= \Span{\vtW_J},&
    \ofD_J &= \diag(\mu_{j_1},\dots,\mu_{j_{t-m}}),&
    \Lambda_J &= \{\mu_j\}_{j\in J}.
  \end{align*}
  Assume that
  \begin{equation}
    \delta_I + \delta_J + \epsilon_N - \epsilon_1
    < \delta(\Lambda_I,\Lambda_J) \FED \delta
    \label{eq:rec:sel:pri:ass1}
  \end{equation}
  for
  \[
    \delta_I = \frac{\nrm{\oiA\vtW_I-\vtW_I\ofD_I}}{\sigma_{\min}(\vtW_I)}
    \quad\text{and}\quad
    \delta_J = \frac{\nrm{\oiA\vtW_J-\vtW_J\ofD_J}}{\sigma_{\min}(\vtW_J)}.
  \]
  Furthermore, assume that there exists an index set
  $K=\{k_1<\dots<k_m\}\subset\{1,\dots,N\}$ with
  $\{l_1<\dots<l_{N-m}\}=\{1,\dots,N\}\setminus K$ such
  that
  \begin{equation}
    \lambda_{k_1},\dots,\lambda_{k_m}
    \in\bigcup_{i\in I}[\mu_i-\delta_I,\mu_i+\delta_I]
    \quad\text{and}\quad
    \lambda_{l_1},\dots,\lambda_{l_{N-m}}
    \in\bigcup_{j\in J}[\mu_j-\delta_J,
                        \mu_j+\delta_J].
    \label{eq:rec:sel:pri:ass2}
  \end{equation}

  If
  \[
    \mu_{\min}
    \DEF \min_{\mu\in\bigcup_{i\in I}[\mu_i+\epsilon_1,\mu_i+\epsilon_N]} |\mu|
    > 0,
  \]
  then $\oiP_{\vsW_J^\perp,\oiB\vsW_J}$ is well defined and
  $\Lambda(\oiP_{\vsW_J^\perp,\oiB\vsW_J}\oiB)=\{0\}\cup
  \{\widehat{\lambda}_1,\dots,\widehat{\lambda}_{N-m}\}$ satisfies
  \begin{equation}
    \widehat{\lambda}_i
    \in\bigcup_{j\in J}[\mu_j-\delta_J+\epsilon_1-\eta,
      \mu_j+\delta_J+\epsilon_N+\eta]
    \label{eq:rec:sel:pri:incl}
  \end{equation}
  with
  \[
    \eta = \left(\delta_I+\nrm{\oiP_{\vsW_I^\perp}\oiF\oiP_{\vsW_I}}\right)^2
      \left(
        \frac{1}{\delta-\delta_J -\epsilon_N+\epsilon_1}
        +\frac{1}{\mu_{\min}}
      \right).
  \]
\end{thm}

\begin{proof}
  Let the eigenvalues of $\oiB$ be denoted by
  $\overline{\lambda}_1\leq\dots\leq\overline{\lambda}_N$. Because of Weyl's theorem
  (see theorem~\ref{thm:rec:per:def:weyl} the eigenvalues of $\oiB$ satisfy for
  $k\in\{1,\dots,N\}$
  \[
    \overline{\lambda}_k \in [\lambda_k+\epsilon_1,\lambda_k+\epsilon_N].
  \]
  With assumption~\eqref{eq:rec:sel:pri:ass1} on the Ritz
  pairs and assumption \eqref{eq:rec:sel:pri:ass2} on $\oiA$'s eigenvalues,
  it follows that
  \[
    \overline{\lambda}_{k_1},\dots,\overline{\lambda}_{k_m}
    \in \bigcup_{i\in I}[\mu_i-\delta_I+\epsilon_1,\mu_i+\delta_I+\epsilon_N]
  \]
  and
  \begin{equation}
    \overline{\lambda}_{l_1},\dots,\overline{\lambda}_{l_{N-m}}
    \in \bigcup_{j\in J}[\mu_j-\delta_J+\epsilon_1,\mu_j+\delta_J+\epsilon_N].
    \label{eq:rec:sel:pri:lamB}
  \end{equation}
  It is now shown that the projection $\oiP_{\vsW_I^\perp,\oiB\vsW_I}$ is
  well defined by analyzing the eigenvalue $\nu$ of smallest magnitude of
  \[
    \ip{\vtW_I}{\oiB\vtW_I}
    = \ip{\vtW_I}{\oiA\vtW_I}+\ip{\vtW_I}{\oiF\vtW_I}
    = \ofD_I + \ip{\vtW_I}{\oiF\vtW_I}.
  \]
  It follows from Weyl's theorem that $\nu\in\bigcup_{i\in
  I}[\mu_i+\epsilon_1,\mu_i+\epsilon_N]$ and thus $|\nu|\geq \mu_{\min} >0$ by
  assumption. This means that $\ip{\vtW_I}{\oiB\vtW_I}$ is nonsingular and thus
  the projection $\oiP_{\vsW_I^\perp,\oiB\vsW_I}$ is well defined.

  Now let $\vtW_\perp\in\vsH^{N-m}$ such that
  $\ip{\vtW_I^\perp}{\vtW_I^\perp}=\ofI_{N-m}$ and
  $\Span{\vtW_I^\perp}=\vsW_I^\perp$ and let
  $\ofM=\ip{\vtW_I^\perp}{\oiA\vtW_I^\perp}$. The eigenvalues of
  $\diag(\ofM,\ofD_I)$ are assumed to be
  $\tilde{\lambda}_1\leq\dots\leq\tilde{\lambda}_N$ and the indices $1\leq
  n_1<\dots<n_{N-m}\leq N$ are assumed to be given such that
  $\tilde{\lambda}_{n_1}\leq\dots\leq\tilde{\lambda}_{n_{N-m}}$ are the
  eigenvalues of $\ofM$ and $1\leq p_1<\dots<p_m\leq N$ such that
  $\tilde{\lambda}_{p_s}=\mu_{i_s}$ for $s\in\{1,\dots,m\}$. Because
  $\nrm{\oiA\vtW_I^\perp - \vtW_I^\perp \ofM}= \nrm{\oiA\vtW_I -
  \vtW_I\ofD_I}=\delta_I$, the eigenvalues of $\diag(\ofM,\ofD_I)$ satisfy with
  lemma~\ref{lem:rec:sel:pri}
  \[
    \max_{i\in\{1,\dots,N\}}|\lambda_i - \tilde{\lambda}_i|
    \leq \delta_I.
  \]
  With assumption~\eqref{eq:rec:sel:pri:ass2} it follows that
  \[
    \tilde{\lambda}_{n_1},\dots,\tilde{\lambda}_{n_{N-m}}
    \in\bigcup_{j\in J} [\mu_j - \delta_I -\delta_J,\mu_j+\delta_I+\delta_J].
  \]
  Assumption~\eqref{eq:rec:sel:pri:ass1} thus guarantees that $n_s=l_s$ for
  $s\in\{1,\dots,N-m\}$. Now theorem~\ref{thm:rec:per:def:quad} can be applied
  in order to bound the difference of the nonzero eigenvalues of
  $\oiP_{\vsW_I^\perp,\oiB\vsW_I}\oiB$ and the eigenvalues
  $\overline{\lambda}_{l_1},\dots,\overline{\lambda}_{l_{N-m}}$ of $\oiB$. This
  results in
  \begin{equation}
    \max_{s\in\{1,\dots,N-m\}} |\overline{\lambda}_{l_s} - \widehat{\lambda}_s|
    \leq \nrm{\vtR}^2 \left(
      \frac{1}{\alpha} + \frac{1}{\beta}
    \right),
    \label{eq:rec:sel:pri:quad}
  \end{equation}
  where
  \begin{align*}
    \vtR&=\oiB\vtW_I - \vtW_I\ip{\vtW_I}{\oiB\vtW_I},\\
    \alpha
      &=\delta(\{\overline{\lambda}_{l_1},\dots,\overline{\lambda}_{l_{N-m}}\},
        \Lambda(\ip{\vtW_I}{\oiB\vtW_I})\\
    \text{and}\quad
    \beta
      &= \min_{\mu\in\Lambda(\ip{\vtW_I}{\oiB\vtW_I})} |\mu|.
  \end{align*}
  Now note that by definition
  \begin{align*}
    \nrm{\vtR}
    &= \nrm{\oiP_{\vsW_I^\perp}\oiB\oiP_{\vsW_I}}
    = \nrm{\oiP_{\vsW_I^\perp}(\oiA+\oiF)\oiP_{\vsW_I}}
    \leq \nrm{\oiP_{\vsW_I^\perp}\oiA\oiP_{\vsW_I}}
      + \nrm{\oiP_{\vsW_I^\perp}\oiF\oiP_{\vsW_I}}\\
    &\leq \delta_I
      + \nrm{\oiP_{\vsW_I^\perp}\oiF\oiP_{\vsW_I}}.
  \end{align*}
  If the eigenvalues of $\ip{\vtW_I}{\oiB\vtW_I}$ are
  $\overline{\mu}_1\leq\dots\leq\overline{\mu}_m$, then with Weyl's theorem and
  Cauchy interlacing it follows that
  \[
    \overline{\mu}_i\in[\mu_i+\epsilon_1,\mu_i+\epsilon_N]
  \]
  for $i\in\{1,\dots,m\}$. With this observation it can be seen with
  equation~\eqref{eq:rec:sel:pri:lamB} and
  assumption~\eqref{eq:rec:sel:pri:ass1} that
  \[
    \alpha
    \geq \delta\Big(
      \bigcup_{j\in J}[\mu_j-\delta_J+\epsilon_1,\mu_j+\delta_J+\epsilon_N],
      \bigcup_{i\in I} [\mu_i+\epsilon_1,\mu_i+\epsilon_N]
    \Big)
    \geq \delta - \delta_J - \epsilon_N + \epsilon_1 > 0.
  \]
  It has already been shown in the course of the proof that $\beta\geq
  \mu_{\min}>0$ and thus
  \[
    \nrm{\vtR}^2 \left(
      \frac{1}{\alpha} + \frac{1}{\beta}
    \right) \leq \eta.
  \]
  The theorem's inclusion statement~\eqref{eq:rec:sel:pri:incl} now follows from
  \eqref{eq:rec:sel:pri:lamB} and \eqref{eq:rec:sel:pri:quad}.
\end{proof}

\begin{rmk}
  Theorem~\ref{thm:rec:sel:pri} can be improved trivially such that an
  individual interval for each eigenvalue is provided if the location of the
  original eigenvalues is known. This generalization is not presented here
  because the notation becomes even more cumbersome and the above version is
  sufficient for the intended use.
\end{rmk}

A brief discussion of the theorem seems to be appropriate. Roughly speaking, the
theorem's assumptions are satisfied if the eigenvalues of $\oiA$ can be split
into two parts: the first part consists of $m$ eigenvalues and lies in the
neighborhood of $m$ Ritz values while the remaining $N-m$ eigenvalues lie in a
(potentially large) neighborhood of the remaining $t-m$ Ritz values which are
required to be separated from the other $m$ Ritz values. The condition on
$\mu_{\min}$ is required to ensure that the projection
$\oiP_{\vsW_I^\perp,\oiB\vsW_I}$ is well defined. Note that throughout the
theorem, the values $\epsilon_1$ and $\epsilon_N$ can be replaced by $-\epsilon$
and $\epsilon$, where $\epsilon=\max\{|\epsilon_1|,|\epsilon_N|\}=\nrm{\oiF}$.
However, in certain cases, the more general formulation in the theorem pays off.
For example, if several shifted linear systems have to be solved, then
$\oiF=\alpha\id$ for an $\alpha\in\R$ and thus also all Ritz values are simply
shifted and $\mu_{\min}$ can be determined exactly because
$\epsilon_1=\epsilon_N=\alpha$. Likewise, the intervals around the Ritz values
in~\eqref{eq:rec:sel:pri:incl} are not enlarged by the perturbation $\oiF$ but
just shifted in this case. The term $\nrm{\oiP_{\vsW_I^\perp}\oiF\oiP_{\vsW_I}}
=\nrm{\oiF\vtW_I-\vtW_I\ip{\vtW_I}{\oiF\vtW_I}}$ can also be bounded by
$\nrm{\oiF}$ but such a rough estimation may be penalized severely because it
enters the bound quadratically. For example, in the shifted case,
$\nrm{\oiP_{\vsW_I^\perp}\oiF\oiP_{\vsW_I}}=0$ but $\nrm{\oiF}=|\alpha|$.

Theorem~\ref{thm:rec:sel:pri} does not reveal which Ritz vectors should be used
for deflation but can help to assess a possible choice of vectors in combination
with a priori bounds for the used Krylov subspace method. If
theorem~\ref{thm:rec:sel:pri} can be applied for a choice of Ritz vectors, then
the resulting inclusion intervals for eigenvalues of the deflated operator
can be used with the $\kappa$-bound for the CG method (see
theorem~\ref{thm:back:cg-kappa}), the MINRES bound in
theorem~\ref{thm:back:minres:bound} or any other a priori bound that can be
evaluated using inclusion intervals for the eigenvalues. Of course, the bounds
can only be applied if the inclusion intervals do not contain zero. If $t$ Ritz
pairs $R=\{(\vw_1,\mu_1),\dots,(\vw_t,\mu_t)\}$ have been computed, then one
could simply test all elements of the power set $\mathcal{P}(R)$. Clearly, this
becomes unattractive for large numbers of $t$ because $|\mathcal{P}(R)|=2^t$.
What follows is a description of a strategy for selecting a subset of Ritz
vectors from an already computed set of Ritz vectors. The strategy incorporates
theorem~\ref{thm:rec:sel:pri} and a simple heuristic. Both the CG $\kappa$-bound
in theorem~\ref{thm:back:cg-kappa} and the MINRES bound in
theorem~\ref{thm:back:minres:bound} only take into account the extremal
eigenvalues, where this means
\[
  \lambda_{\min} = \min_{\lambda\in\Lambda}\lambda,\quad
  \lambda_{\max}^- = \max_{\substack{\lambda\in\Lambda\\\lambda<0}} \lambda,\quad
  \lambda_{\min}^+ = \min_{\substack{\lambda\in\Lambda\\\lambda<0}} \lambda
  \quad\text{and}\quad
  \lambda_{\max} = \max_{\lambda\in\Lambda} \lambda
\]
in the indefinite case and $\lambda_{\min}$ and $\lambda_{\max}$ in the
positive-definite case. Algorithm~\ref{alg:rec:sel:sa} is an $O(t)$-algorithm
which successively considers Ritz vectors for deflation that correspond to
the extremal negative and extremal positive Ritz values. The selections are then
evaluated with a cost function
$\omega:\mathcal{P}(\{1,\dots,t\})\lra\R_\geq\cup\{\infty\}$. The cost function
is allowed to return $\infty$ if no estimation is possible, e.g., if assumptions
are not satisfied. The index set with the smallest cost is returned if at least
one index set could be evaluated. Otherwise the empty set is returned.

\begin{algorithm}[htbp]
  \begin{algorithmic}[1]
    \Require Assume that the following is given:
    \begin{itemize}[noitemsep]
      \item $\oiA,\oiB\in\vsL(\vsH)$ self-adjoint and $\vc\in\vsL(\vsH)$.
      \item Ritz pairs of $\oiA\in\vsL(\vsH)$:
        $(\vw_1,\mu_1),\ldots,(\vw_t,\mu_t)\in\vsH\times\R$ with $\mu_i\neq 0$
        and Ritz residual norm $\nrm{\vr_i}=\nrm{\oiA\vw_i-\mu_i\vw_i}$
        for $i\in M\DEF\{1,\dots,t\}$.
      \item A cost function $\omega:\mathcal{P}(M)\lra\R_\geq\cup\{\infty\}$.
        For $I=\{i_1,\dots,i_k\}\subseteq M$, $\omega(I)$ is an estimation of
        the time that is required to solve the deflated linear system
        $\oiP_{\vsU_I,\oiB\vsU_I}\oiB\tilde{\vy}=\oiP_{\vsU_I,\oiB\vsU_I}\vc$ up
        to a prescribed tolerance $\varepsilon$ with the deflation space
        $\vsU_I=\Span{\vw_{i_1},\dots,\vw_{i_k}}$. The value $\omega(I)=\infty$
        is allowed to indicate that no estimation is possible for the given set
        $I$.
    \end{itemize}
    \State $C=\emptyset$.
      \Comment gathers subsets of $\{1,\dots,t\}$ with finite time
        estimations via $\omega$
    \State $I=\emptyset$, $J=\{1,\dots,t\}$.
    \While{$J\neq\emptyset$}
      \State $\Lambda_J \gets \{\mu_j~|~j\in J\}$.
      \State $\Lambda_- \gets \Lambda_J\cap]-\infty,0[$.
        \Comment negative Ritz values in $\Lambda_J$
      \State $\Lambda_+ \gets \Lambda_J\cap]0,\infty[$.
        \Comment positive Ritz values in $\Lambda_J$
      \State $\Lambda_S \gets \{\min\Lambda_J,\max\Lambda_J\}$
      \State $\Lambda_S \gets \Lambda_S\cup\{\max\Lambda_-\}$\quad if\quad
        $\Lambda_-\neq\emptyset$.
      \State $\Lambda_S \gets \Lambda_S\cup\{\min\Lambda_+\}$\quad if\quad
        $\Lambda_+\neq\emptyset$.
      \State $S \gets \{j\in J~|~\mu_j\in\Lambda_S\}$.
        \Comment indices of extremal negative and positive Ritz values
      \If{$\min_{s\in S}\omega(I\cup\{s\})<\infty$}
        \State $s_{\text{sel}} \gets \argmin_{s\in S} \omega(I\cup\{s\})$.
        \State $C \gets C\cup\left\{ I\cup\{s_{\text{sel}}\}\right\}$.
      \Else
        \State $s_{\text{sel}} \gets \argmin_{s\in S} \nrm{\vr_s}$.
          \Comment fallback: pick Ritz vector with smallest Ritz residual
      \EndIf
      \State $I \gets I\cup\{s_{\text{sel}}\}$.
      \State $J \gets J\setminus\{s_{\text{sel}}\}$.
    \EndWhile
    \If{$C\neq\emptyset$}
      \State\Return $\argmin_{K\in C}\omega(K)$.
        \Comment return index set with smallest time estimation
    \Else
      \State\Return $\emptyset$.
    \EndIf
  \end{algorithmic}
  \caption{Basic selection strategy of Ritz vectors for self-adjoint operators.
    Implemented as \lstinline{krypy.recycling.generators.RitzExtremal}
    in~\cite{krypy}.}
  \label{alg:rec:sel:sa}
\end{algorithm}

One possibility to construct a cost functional $\omega$ with the insight of
theorem~\ref{thm:rec:sel:pri} is to first compute estimated inclusion intervals
for the eigenvalues of the deflated operator
$\widehat{\oiB}=\oiP_{\vsU^\perp,\oiB\vsU}\oiB$ by simply assuming that the
requirement~\eqref{eq:rec:sel:pri:ass2} on the eigenvalues of $\oiA$ is
satisfied. If also the other assumptions of theorem~\ref{thm:rec:sel:pri} are
satisfied, then the right hand side of~\eqref{eq:rec:sel:pri:incl} defines a set
$\widehat{\Lambda}$ that contains all nonzero eigenvalues of $\widehat{\oiB}$.
If $0\notin\widehat{\Lambda}$, then asymptotic convergence bounds can be used to
determine the first iteration where the bound falls below a prescribed tolerance
$\varepsilon$ for the relative $\oiB$-norm of the error (CG) or the relative
residual norm (MINRES). For example, if $\oiA$ and $\oiB$ are positive definite,
then the $\kappa$-bound for the CG method leads to
\[
  \frac{\nrm[\widehat{\oiB}]{\vy - \vy_n}}{\nrm[\widehat{\oiB}]{\vy - \vy_0}}
  \leq 2 \left(
    \frac{\sqrt{\kappa_{\eff}(\widehat{\oiB})} - 1}
    {\sqrt{\kappa_{\eff}(\widehat{\oiB})} + 1}
  \right)^n
  \leq 2 \left(
    \frac{\sqrt{\widehat{\kappa}} - 1}
    {\sqrt{\widehat{\kappa}} + 1}
  \right)^n
  = 2 \rho_\CG^n,
\]
where
\[
  \widehat{\kappa}
  = \frac{\max \widehat{\Lambda}}{\min \widehat{\Lambda}}
  \geq \kappa(\widehat{\oiB})
  \quad\text{and}\quad
  \rho_\CG=\frac{\sqrt{\widehat{\kappa}} - 1}
    {\sqrt{\widehat{\kappa}} + 1}.
\]
Thus
\[
  \frac{\nrm[\widehat{\oiB}]{\vy - \vy_n}}{\nrm[\widehat{\oiB}]{\vy - \vy_0}}
  \leq \varepsilon
\]
is satisfied for $n=\lceil \log_{\rho_\CG}\frac{\varepsilon}{2}\rceil$.
Analogously, for the indefinite case and the MINRES method, the inequality
\[
  \frac{\nrm{\vc-\oiB\vy_n}}{\nrm{\vc-\oiB\vy_0}} \leq \varepsilon
\]
is satisfied for $n=2\lceil \log_{\rho_\MR} \frac{\varepsilon}{2}\rceil$, where
\[
  \rho_\MR = \frac{\alpha-\beta}{\alpha+\beta}
  \quad\text{with}\quad
  \alpha = \sqrt{|\min\widehat{\Lambda}\cdot\max\widehat{\Lambda}|}
  \quad\text{and}\quad
  \alpha= \sqrt{|
    \max_{\lambda\in\widehat{\Lambda},\lambda<0}\lambda
    \cdot
    \min_{\lambda\in\widehat{\Lambda},\lambda>0}\lambda
  |}.
\]
Note that the $\kappa$-bound can be used if $\widehat{\Lambda}$ only contains
positive (or only negative) values.

A cost function $\omega$ can now be constructed by using the iteration count $n$
and the number of deflation vectors. As mentioned before, also timings for the
most expensive parts of the iteration can be included in order to balance the
number of deflation vectors with the number of iterations $n$.

\begin{ex}
  \label{ex:rec:sel:pri:minres}
  Consider again the situation of example~\ref{ex:rec:per:def:minres}. After 27
  iterations, the original linear system $\oiA\vx=\vb$ has been solved with a
  relative residual norm below $10^{-6}$. Now assume -- just out of curiosity --
  that the same linear system has to be solved again and the goal is to find an
  optimal deflation subspace from the Krylov subspace that was built in the
  first solution process. If the Ritz pairs of $\oiA$ with respect to this
  Krylov subspace are computed and algorithm~\ref{alg:rec:sel:sa} is employed
  with the cost function $\omega$ described above, then actually the three Ritz
  vectors corresponding to the Ritz values of smallest magnitude are chosen
  automatically. Also, all assumptions of theorem~\ref{thm:rec:sel:pri} are
  satisfied such that the prediction is not just a heuristic but an actual bound
  in this case. The bound is plotted in figure~\ref{fig:rec:sel:pri:minres}
  along with the actual convergence of MINRES if the deflation subspace $\vsW$
  is chosen as the span of the suggested Ritz vectors.
\end{ex}

\begin{figure}[ht]
  \centering
  \setlength{\figureheight}{0.5\textwidth}
  \setlength{\figurewidth}{0.61\textwidth}
  \inputplot{exp_rec_per_def_minres_apriori}
  \caption{Convergence bound for deflated MINRES via the a priori
    bound~\eqref{eq:back:cg:kappa} based on the eigenvalue inclusion
    intervals~\eqref{eq:rec:sel:pri:incl}. The setup is described in
    example~\ref{ex:rec:sel:pri:minres}.
  }
  \label{fig:rec:sel:pri:minres}
\end{figure}

Instead of the proposed strategy, a zoo of more sophisticated approaches can be
used if more is known about the spectrum of $\oiA$. Also, the Ritz values can be
preprocessed by a clustering algorithm such that the Ritz pairs are not treated
individually in algorithm~\ref{alg:rec:sel:sa} but Ritz values in a small
neighborhood are treated as one set whose Ritz vectors are included for
deflation or are left out. However, clustering algorithms need to be told how
many clusters should be created or at which distance a value is included in a
cluster.

It should be noted that linear asymptotic bounds like the CG or MINRES bounds
that were used here discard a lot of information about the problem. First, only
the extremal eigenvalues are considered and the actual distribution is not taken
into account. Second, the bounds eliminate the effects of the right hand side
and the initial guess. The often observed nonlinear behavior of Krylov subspace
methods such as the transition from a phase of slow convergence to a
``superlinear'' phase (e.g., in figure~\ref{fig:rec:per:def:minres}) cannot be
characterized with purely linear asymptotic bounds since they are based on the
minimal reduction of the $\oiA$-norm of the error or the residual norm
from one iteration to the next. A discussion worth reading about
the odd habit of interpreting the inherently nonlinear Krylov subspace methods
by means of linear methods can be found in chapter~5.5.4 in the book of Liesen
and Strako\v{s}~\cite{LieS13}.

In principle, the a priori approach of this subsection could be extended to the
GMRES method and non-normal operators. For example, the spectral GMRES bound in
theorem~\ref{thm:back:gmres:spectral} can be considered for this purpose.
However, two obstacles render this approach useless in many cases. The first
problem is that the spectral bound in equation~\eqref{eq:back:gmres:spectral} is
even less descriptive than the a priori bounds for CG or MINRES due to the
presence of the eigenvector basis condition number $\kappa(\oiS)$. The second
obstacle is that the perturbation theory for non-normal operators becomes much
more intricate and general bounds lead to rough eigenvalue regions that are
likely to include the origin and thus render the spectral bound irrelevant.

The difficulties with a priori bounds are avoided in the next subsection by
using a very different approach that includes prior knowledge which is available
from already computed quantities or can be approximated. % TODO: too vague?


\subsection{Estimation with approximate Krylov subspaces}
\label{sec:rec:sel:kry}

A major drawback of the strategy in section~\ref{sec:rec:sel:pri} is that the
employed a priori bounds for the CG and MINRES methods only take into account
the extremal values of the eigenvalue inclusion intervals. The actual
distribution of the eigenvalues as well as the right hand side are not
considered and -- as mentioned in the previous subsection -- the linear
asymptotic bounds cannot reproduce nonlinear convergence histories.
Furthermore, the a priori bound approach in section~\ref{sec:rec:sel:pri} is
unlikely to yield usable results for non-normal operators and the GMRES method.

In this subsection, a new strategy is proposed that is able to exploit more
information from the already completed solution process for the previous linear
system.  In order to maintain readability, it is assumed in this subsection
that all initial guesses are chosen as the zero vector. The general case is a
straightforward extension and only adds notational complexity without leading to
valuable insight. Apart from the initial guess, the setting in this subsection
is identical to the one described in the introduction of
section~\ref{sec:rec:sel}: the first of the two linear systems
in~\eqref{eq:rec:sel:ls} has been solved approximately via a Krylov subspace
method applied to the deflated linear system~\eqref{eq:rec:sel:ls-def} with the
$m$-dimensional deflation space $\vsU$. For the first linear system, an Arnoldi
relation~\eqref{eq:rec:sel:arnoldi} has been constructed for the Krylov subspace
$\vsK_n=\vsK_n(\widehat{\oiA},\widehat{\vb})$. The goal is again to
assess a given $k$-dimensional subspace $\vsW\subseteq\vsK_n+\vsU$
that is considered as a deflation subspace for the next linear system, i.e.,
\begin{equation}
  \widehat{\oiB}\widehat{\vy} = \widehat{\vc}
  \label{sec:rec:sel:kry:ls2}
\end{equation}
with $\widehat{\oiB}=\oiP_\oiB\oiB$ and $\widehat{\vc}=\oiP_\oiB\vc$ for
$\oiP_\oiB\in\{\oiP_{\vsW^\perp,\oiB\vsW},\oiP_{(\oiB\vsW)^\perp}\}$.

The idea here is to first figure out how the considered subspace $\vsW$ performs
for the first linear system by constructing an approximate Krylov subspace for
the deflated operator $\oiP_\oiA\oiA$ with initial vector $\oiP_\oiA\vb$, where
$\oiP_\oiA\in\{\oiP_{\vsW^\perp,\oiA\vsW},\oiP_{(\oiA\vsW)^\perp}\}$. Note that
$\oiP_\oiA$ is the projection that is built with the new subspace $\vsW$ and not
the projection $\oiP$ that was used to construct the Krylov subspace $\vsK_n$.
For each $i\leq n+m-k$, a self-adjoint perturbation $\oiF_i\in\vsL(\vsH)$ is
determined along with a perturbation $\vf\in\vsH$ of the right hand side such
that an Arnoldi relation for $\vsK_i(\oiP_\oiA\oiA+\oiF_i,\oiP_\oiA\vb+\vf)$ is
computable without any additional applications of the operator $\oiA$ or the
projection $\oiP_\oiA$. The norm of the perturbations $\oiF_i$ and $\vf$ are
also available and are shown to be optimal for each step $i$. If $\oiA$ is
self-adjoint, then the self-adjoint perturbation $\oiF_i$ shows that the Arnoldi
relation in fact is a Lanczos relation. Once the Arnoldi relation for the
approximate Krylov subspace is computed, the residual norms of the CG, MINRES or
GMRES methods in the $i$-th iteration for the linear system
\begin{equation*}
  (\oiP_\oiA\oiA + \oiF_i) \widehat{\vx} = \oiP_\oiA\vb + \vf
\end{equation*}
can be extracted from the Arnoldi relation. In a last step,
theorem~\ref{thm:rec:per:kry:full} can be employed to bound the
$i$-th residual norm of the used Krylov subspace method when it is applied to
the second linear system~\eqref{sec:rec:sel:kry:ls2} with the considered
deflation subspace $\vsW$. The last step involves knowledge about the
(pseudo-)spectrum of the operator $\oiA$. Similar to the previous subsection, it
is reasonable to draw on approximations of the (pseudo-)spectrum due to a lack
of exact spectral information in practice.

The basic rationale behind the idea of approximate Krylov subspaces is the
following: if it is known how the operator $\oiA$ acts on the subspace
$\vsK_n+\vsU$, i.e., if $\oiA[\vtV_n,\vtU]$ is known for bases
$\vtV_n$ and $\vtU$ of $\vsK_n$ and $\vsU$, then
$\oiP_\oiA\oiA[\vtV_n,\vtU]$ can also be computed without additional
applications of $\oiA$. Thus, for any vector $\vv\in\vsK_n+\vsU$, the
vector $\oiP_\oiA\oiA\vv$ can be computed but another application of
$\oiP_\oiA\oiA$, i.e., $(\oiP_\oiA\oiA)^2\vv$ is usually not computable without
further applications of $\oiA$ because the vector $\oiP_\oiA\oiA\vv$ will in
general not lie in the subspace $\vsK_n+\vsU$ where the behavior of
$\oiA$ is known. However, the closest vector in $\vsK_n+\vsU$ to
$\oiP_\oiA\oiA\vv$ can be computed as
$\oiP_{\vsK_n+\vsU}\oiP_\oiA\oiA\vv$ with the orthogonal projection
$\oiP_{\vsK_n+\vsU}$. The error that is made by projecting onto
$\vsK_n+\vsU$ after the application of $\oiP_\oiA\oiA$ in the $i$-th
iteration can be interpreted as a perturbation $\oiF_i$ of $\oiP_\oiA\oiA$.

The next theorem constitutes the starting point of the further analysis and
states a basic observation about the construction of approximate Krylov
subspaces. To the knowledge of the author, the result is new.

\begin{thm}[Backward error for approximate Krylov subspaces]
  \label{thm:rec:sel:kry:pert}
  Let $\vtV\in\vsH^n$ and $\vtW\in\vsH^m$ such that
  $\ip{[\vtV,\vtW]}{[\vtV,\vtW]}=\ofI_{n+m}$ and assume that
  \begin{equation}
    \oiA\vtV = \vtV\ofG + \vtW\ofR
    \label{eq:rec:sel:kry:pert:ass}
  \end{equation}
  for $\oiA\in\vsL(\vsH)$, $\ofG\in\C^{n,n}$ and $\ofR\in\C^{m,n}$. Furthermore,
  let $\ofG=\ofT\ofH_n\ofT^\htp$ with an
  upper Hessenberg matrix $\ofH_n\in\C^{n,n}$ and a unitary matrix
  $\ofT\in\C^{n,n}$.

  Then for $i\in\{1,\dots,n-1\}$
  \begin{align*}
    (\oiA+\oiF_i) \widehat{\vtV}_i = \widehat{\vtV}_{i+1} \underline{\ofH}_i
    \qquad\text{and}\qquad
    (\oiA+\oiF_n) \widehat{\vtV}_n = \widehat{\vtV}_n \ofH_n,
  \end{align*}
  where $\widehat{\vtV}_n=\vtV\ofT$,
  $\widehat{\vtV}_i=\widehat{\vtV}_n\mat{\ofI_i\\0}$,
  $\underline{\ofH}_i = \mat{\ofI_{i+1} & 0} \ofH_n \mat{\ofI_i\\0}$ and
  \[
    \oiF_i=\oiF_i^\adj=-\vtW\widehat{\ofR}_i\widehat{\vtV}_i^\adj
          -\widehat{\vtV}_i\widehat{\ofR}_i^\htp\vtW^\adj
  \]
  with $\widehat{\ofR}_n=\ofR\ofT$ and
  $\widehat{\ofR}_i=\widehat{\ofR}\mat{\ofI_i\\0}$.
  Furthermore, $\nrm{\oiF_i}=\nrm[2]{\widehat{\ofR}_i}$ and
  $\nrm{\oiF_1}\leq\dots\leq\nrm{\oiF_n}$.
\end{thm}

\begin{proof}
  With
  $\ip{\vtW}{\widehat{\vtV}_n}=0$ and
  $\ip{\widehat{\vtV}_n}{\widehat{\vtV}_n}=\ofI_n$,
  equation~\eqref{eq:rec:sel:kry:pert:ass} yields
  \begin{align*}
    (\oiA+\oiF_n)\widehat{\vtV}_n
    =\widehat{\vtV}_n\ofH_n.
  \end{align*}
  A multiplication with $\mat{\ofI_i\\0}\in\R^{n,i}$ from the right results in
  \[
    (\oiA+\oiF_i)\widehat{\vtV}_i = \widehat{\vtV}_i\underline{\ofH}_i
    \quad\text{for}~i\in\{1,\dots,n-1\}.
  \]
  The norm of the perturbation satisfies for $i\in\{1,\dots,n\}$
  \begin{align*}
    \nrm{\oiF_i}^2
    &= \nrm{[\widehat{\vtV}_i,\vtW]
      \mat{0&\widehat{\ofR}_i^\htp\\\widehat{\ofR}_i&0}
      [\widehat{\vtV}_i,\vtW]^\adj}^2
    = \nrm[2]{\mat{0&\widehat{\ofR}_i^\htp\\\widehat{\ofR}_i&0}}^2
    = \nrm[2]{\widehat{\ofR}_i}^2.
  \end{align*}
  The norm inequality follows from the fact that
  \[
    \nrm{\oiF_i}
    =\nrm[2]{\widehat{\ofR}_i}
    =\nrm[2]{\widehat{\ofR}_{i+1}\mat{\ofI_i\\0}}
    \leq\nrm[2]{\widehat{\ofR}_{i+1}}
    =\nrm{\oiF_{i+1}}
    \quad
    \text{for}~i\in\{1,\dots,n-1\}.
  \]
\end{proof}

\begin{rmk}
  If the matrix $\ofT$ in theorem~\ref{thm:rec:sel:kry:pert} satisfies
  $\ofT\ve_1=\ve_1$ then $\widehat{\vtV}_n\ve_1=\vtV\ve_1$ holds trivially.
  A Hessenberg decomposition $\ofG=\ofT\ofH_n\ofT^\htp$ that satisfies
  $\ofT\ve_1=\ve_1$ can be constructed with the Householder reduction to
  Hessenberg form, cf.\ algorithm~7.4.2 in the book of Golub and Van
  Loan~\cite{GolV13}.
\end{rmk}

The following corollary gives insight into the behavior of the constructed
perturbation $\oiF_i$ in theorem~\ref{thm:rec:sel:kry:pert}.

\begin{cor}
  \label{cor:rec:sel:kry:pert}
  Let the assumptions of theorem~\ref{thm:rec:sel:kry:pert} hold with
  $\ofT\ve_1=\ve_1$ and let $\ofH_n=[h_{ij}]_{i,j\in\{1,\dots,n\}}$ and
  $\vv=\vtV\ve_1$. Furthermore, assume that $j\in\{1,\dots,n\}$ is such that
  $h_{i+1,i}\neq 0$ for all $i\in\{1,\dots,j-1\}$.

  Then
  \begin{equation}
    \Span{\widehat{\vtV}_j}
    = \vsK_j(\oiA+\oiF_j,\vv)
    = \vsK_j(\oiA+\oiF_n,\vv)
    = \vsK_j(\oiP_{\Span{\vtV}}\oiA,\vv).
    \label{eq:rec:sel:kry:pert:eq}
  \end{equation}

  If there exists a $j\in\{1,\dots,n-1\}$ with $h_{j+1,j}=0$, then
  $\vsK_j(\oiA+\oiF_j,\vv)$ is invariant under $\oiA+\oiF_j$. In any
  case, $\vsK_n(\oiA+\oiF_n,\vv)$ is invariant under $\oiA+\oiF_n$.
\end{cor}

\begin{proof}
  Almost all statements follow immediately from
  theorem~\ref{thm:rec:sel:kry:pert} with the definitions in
  section~\ref{sec:back:arnoldi} and lemma~\ref{lem:back:arn:equi}. The last
  equality in~\eqref{eq:rec:sel:kry:pert:eq} follows from the
  assumption~\eqref{eq:rec:sel:kry:pert:ass} by noticing that
  \[
    \oiP_{\Span{\vtV}} \oiA\widehat{\vtV}_n
    = \oiP_{\Span{\vtV}} \widehat{\vtV}_n\ofH_n +
    \oiP_{\Span{\vtV}}\vtW\widehat{\ofR}
    =\widehat{\vtV}_n\ofH_n.
  \]
\end{proof}

From corollary~\ref{cor:rec:sel:kry:pert}, it can be seen that the addition of
the perturbation $\oiF_j$ is equivalent to the application of the orthogonal
projection $\oiP_{\Span{\vtV}}$ after each application of the operator $\oiA$.
Thus, the perturbation can be seen as optimal in the sense that it yields the
smallest perturbation in each step by projecting orthogonally on the subspace
$\Span{\vtV}$ where it is known how the operator $\oiA$ acts and discarding
the part where nothing is known about $\oiA$.

As mentioned in section~\ref{sec:rec:per:kry}, Stewart considered a similar
problem in~\cite{Ste02}. There, it was shown how to obtain a perturbation $\oiF$
of minimal norm such that a given subspace $\vsU$ is a Krylov subspace of the
operator $\oiA+\oiF$. Here, the situation is that an orthonormal basis $\vtV$ is
given and the task is to find a perturbation $\oiF$ such that $\Span{\vtV}$ is a
Krylov subspace of the operator $\oiA+\oiF$ with the initial guess $\vtV\ve_1$.
The crucial point is that the perturbation is not fixed but is tailored to each
iteration and that the perturbation's norm is allowed to grow. In fact, unless
$\Span{\vtV}$ is close to an $\oiA$-invariant subspace, the perturbation's norm
$\nrm{\oiF_i}$ can be expected to be very large for $i$ close to $n$ because the
perturbation is forcing the Krylov subspace $\vsK_n(\oiA+\oiF_n,\vv)$ to be
invariant. However, note that the Krylov subspace $\vsK_i(\oiA+\oiF_i,\vv)$ may
already be invariant for $i<n$. The structure of the ``mutable'' perturbation in
theorem~\ref{thm:rec:sel:kry:pert} whose norm is allowed to grow resembles the
situation of inexact Krylov subspace methods in the work of Simoncini and
Szyld~\cite{SimS03}. Their analysis already starts out with a perturbed Arnoldi
relation and focuses on how to loosen the restrictions on the perturbation's
norm. Here, only the abstract assumption~\eqref{eq:rec:sel:kry:pert:ass} is made
and the task is to construct an Arnoldi relation with the given data and initial
vector $\vtV\ve_1$.

\begin{rmk}
  In some cases, an initial vector $\vv\neq 0$ is given that is an element of
  the given subspace $\vsV\DEF\Span{\vtV}$ where the action of the operator
  $\oiA$ is known but is not contained in the span of $\vtV\ve_1$ or is not even
  contained in the given subspace $\vsV$ at all. If $\ip{\vtV}{\vtV}=\ofI_n$ and
  $\vq=\ip{\vtV}{\vv}\neq 0$, then a Householder transformation matrix
  $\ofQ_\vq\in\C^{n,n}$ can be constructed such that
  $\ofQ_\vq\vq=\alpha\nrm[2]{\vq}\ve_1$ with $|\alpha|=1$. Because $\ofQ_\vq$ is
  involutory, i.e., $\ofQ_\vq^2=\ofI_n$, the following holds for
  $\tilde{\vtV}\DEF\vtV\ofQ_\vq$:
  \[
    \tilde{\vtV}\ve_1
    =\vtV\ofQ_\vq\ve_1
    =\conj{\alpha}\frac{\vtV\ip{\vtV}{\vv}}{\nrm[2]{\vq}}
    =\conj{\alpha}\frac{\oiP_\vsV\vv}
      {\nrm{\oiP_\vsV\vv}}.
  \]
  Thus, $\tilde{\vtV}\ve_1$ is a normalized version of $\oiP_\vsV\vv$, the
  vector in $\vsV$ with minimal distance to the initial vector $\vv$ of
  interest. If a relation of the form~\eqref{eq:rec:sel:kry:pert:ass} is given,
  the procedure in theorem~\ref{thm:rec:sel:kry:pert} with $\tilde{\vtV}$,
  $\tilde{\ofG}=\ofQ_\vq\ofG\ofQ_\vq$, $\tilde{\ofR}=\ofR\ofQ_\vq$ instead of
  $\vtV$, $\ofG$ and $\ofR$ can thus be understood as the generation of an
  optimal approximate Krylov subspace for the initial vector $\vv$ with respect
  to the available data. The optimality is meant with regard to each iteration
  as explained in the discussion following corollary~\ref{cor:rec:sel:kry:pert}.
\end{rmk}

In the next theorem, the following situation is analyzed: an Arnoldi relation
for the Krylov subspace
$\vsK_n=\vsK_n(\widehat{\oiA},\widehat{\vv})$ with
$\widehat{\oiA}=\oiP_{\vsU^\perp,\oiA\vsU}\oiA$ and
$\widehat{\vv}=\oiP_{\vsU^\perp,\oiA\vsU}\vv$ for a given $\vv\in\vsH$ is known
and the task is to construct an Arnoldi relation for an approximate Krylov
subspace with the operator $\oiP_{\vsW^\perp,\oiA\vsW}\oiA$ and the initial
vector $\oiP_{\vsW^\perp,\oiA\vsW}\vv$, where
$\vsW\subseteq\vsK_n+\vsU$.  The orthogonal projections
$\oiP_{(\oiA\vsU)^\perp}$ and $\oiP_{(\oiA\vsW)^\perp}$ are not considered
in the following theorem. The reason for this decision lies in the
inapplicability of certain perturbation techniques which becomes apparent in
the course of this section, cf.\ remark~\ref{rem:rec:sel:kry:proj}. The theorem
is preceded by an assumption and a definition with quantities that are
reused later. The definitions may seem technical at first sight but the
computation of the defined quantities can be implemented efficiently in a few
lines of code (implemented as \lstinline{krypy.deflation.Arnoldifyer}
in~\cite{krypy}).

\begin{ass}
  \label{ass:rec:sel:kry:def}
  Let $\oiA\in\vsL(\vsH)$, $\vv\in\vsH$, $\vtU\in\vsH^m$ with
  $\ip{\vtU}{\vtU}=\ofI_m$ such that
  $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$ for $\vsU=\Span{\vtU}$ and let
  $\vtV_{n+1}=[\vtV_n,\vv_{n+1}]\in\vsH^{n+1}$ and $\underline{\ofH}_n$ define
  an Arnoldi relation for
  $\vsK_n=\vsK_n(\oiP_{\vsU^\perp,\oiA\vsU}\oiA,
  \oiP_{\vsU^\perp,\oiA\vsU}\vv)$
  with
  $\vtV_{n+1}\ve_1=\frac{\oiP_{\vsU^\perp,\oiA\vsU}\vv}
  {\nrm{\oiP_{\vsU^\perp,\oiA\vsU}\vv}}$.
  Furthermore, let $\tilde{\ofW}\in\C^{n+m,k}$ such that
  $\theta_{\max}(\vsW,\oiA\vsW)<\frac{\pi}{2}$ for $\vsW=\Span{\vtW}$, where
  $\vtW=[\vtV_n,\vtU]\tilde{\ofW}$.
\end{ass}

\begin{definition}
  \label{def:rec:sel:kry:def}
  Let assumption~\ref{ass:rec:sel:kry:def} hold and let
  $\tilde{\ofW}_\perp\in\C^{n+m,n+m-k}$ be such that
  \[
    \tilde{\ofW}_\perp^\htp\mat{\tilde{\ofW}_\perp\\\tilde{\ofW}}
    =\mat{\ofI_{n+m-k}\\0_{k,n+m-k}}.
  \]
  With $\ofB$, $\underline{\ofB}$, $\ofC$, $\ofE$, $\ofL$ and
  $\underline{\ofI}_n$ as in
  lemmas~\ref{lem:rec:sel:ritz-cg}--\ref{lem:rec:sel:hritz-cg}, the following
  quantities are defined:
  \begin{align*}
    \ofJ&\DEF\mat{\underline{\ofI}_n^\tp & \ofB\\ 0_{m,n+1} & \ofE},&
    \tilde{\ofP}
      &\DEF \ofI_{n+m+1}
      -\ofL\tilde{\ofW}(\tilde{\ofW}^\htp
      \ofJ\ofL\tilde{\ofW})^\inv\tilde{\ofW}^\htp\ofJ,
      \\
    \tilde{\vq}&\DEF\tilde{\ofP}\mat{\nrm{\oiP_{\vsU^\perp,\oiA\vsU}\vv}\\
                                 0_{n,1}\\
                                 \ofE^\inv\ip{\vtU}{\vv}}&
    \text{and}\qquad\vq&\DEF\tilde{\ofW}_\perp^\htp\ofJ\tilde{\vq}.
  \end{align*}
  Furthermore, let the following quantities be given:
  \begin{itemize}[noitemsep]
    \item A Householder transformation $\ofQ_\vq\in\C^{n+m-k,n+m-k}$ such that
      $\ofQ_\vq\vq=\alpha\nrm[2]{\vq}\ve_1$ with $|\alpha|=1$.
    \item A Hessenberg decomposition of
      $\ofQ_\vq\tilde{\ofW}_\perp^\htp\ofJ\tilde{\ofP}\ofL\tilde{\ofW}_\perp
      \ofQ_\vq=\ofT\widehat{\ofH}_{n+m-k}\ofT^\htp$
      with an upper Hessenberg matrix $\widehat{\ofH}_{n+m-k}\in\C^{n+m-k,n+m-k}$ and
      a unitary matrix $\ofT\in\C^{n+m-k,n+m-k}$ such that $\ofT\ve_1=\ve_1$.
    \item A QR decomposition (with column pivoting) of
      \[
        (\oiA\vsU-\vtU\ofE-\vtV_{n+1}\underline{\ofB})\ofP_{QR}
        =[\vtQ_1,\vtQ_2]\mat{\ofR_1 & \ofR_2\\ 0 & 0},
      \]
      where $\ofP_{QR}\in\C^{m,m}$ is a permutation matrix,
      $\vtQ_1\in\vsH^l$, $\vtQ_2\in\vsH^{m-l}$ are such that
      $\ip{[\vtQ_1,\vtQ_2]}{[\vtQ_1,\vtQ_2]}=\ofI_m$ and
      $\ofR_1\in\C^{l,l}$ is nonsingular.
  \end{itemize}
  Then also define
  \begin{align*}
    \ofN&\DEF\mat{1 & \ve_{n+1}^\tp\underline{\ofB}\\
              0_{l,1} & [\ofR_1,\ofR_2]\ofP_{QR}^\tp}
         \mat{0_{m+1,n} & \ofI_{m+1}},&
    \vtZ&\DEF[\vv_{n+1},\vtQ_1],\\
    \widehat{\vtV}_{n+m-k}&\DEF[\vtV_n,\vtU]\tilde{\ofW}_\perp\ofQ_\vq\ofT,&
    \widehat{\vtV}_i&\DEF\widehat{\vtV}_{n+m-k}\mat{\ofI_i\\0},\\
    \widehat{\ofR}_{n+m-k}
      &\DEF\ofN\tilde{\ofP}\ofL\tilde{\ofW}_\perp\ofQ_\vq\ofT,&
    \widehat{\ofR}_i&\DEF\widehat{\ofR}_{n+m-k}\mat{\ofI_i\\0},\\
    \oiF_i&\DEF - \vtZ\widehat{\vtR}_i\widehat{\vtV}_i^\adj
               - \widehat{\vtV}_i\widehat{\vtR}_i^\htp\vtZ^\adj,&
    \widehat{\underline{\ofH}}_j
      &=\mat{\ofI_{j+1}&0}\widehat{\ofH}_{n+m-k}\mat{\ofI_j\\0},
  \end{align*}
  where $i\in\{1,\dots,n+m-k\}$ and $j\in\{1,\dots,n+m-k-1\}$.
\end{definition}

\begin{thm}
  \label{thm:rec:sel:kry:def}
  Let assumption~\ref{ass:rec:sel:kry:def} hold and let the
  definitions from definition~\ref{def:rec:sel:kry:def} be given.

  Then for $i\in\{1,\dots,n+m-k-1\}$
  \begin{align}
    (\tilde{\oiA}+\oiF_i) \widehat{\vtV}_i
    &= \widehat{\vtV}_{i+1} \widehat{\underline{\ofH}}_i
    \label{eq:rec:sel:kry:def:arn1}\\
    \text{and}\qquad
    (\tilde{\oiA}+\oiF_{n+m-k}) \widehat{\vtV}_{n+m-k}
    &= \widehat{\vtV}_{n+m-k} \widehat{\ofH}_{n+m-k}
    \label{eq:rec:sel:kry:def:arn2}
  \end{align}
  with $\tilde{\oiA}=\oiP_{\vsW^\perp,\oiA\vsW}\oiA$.
  Furthermore, $\ip{\widehat{\vtV}_i}{\widehat{\vtV}_i}=\ofI_i$ and
  $\nrm{\oiF_i}=\nrm[2]{\widehat{\ofR}_i}$ hold for $i\in\{1,\dots,n+m\}$ as
  well as $\nrm{\oiF_1}\leq\dots\leq\nrm{\oiF_{n+m-k}}$.

  If there exists a $j\in\{1,\dots,n+m-k-1\}$ such that $h_{i+1,i}\neq 0$ for
  all $i\in\{1,\dots,j\}$, then $\widehat{\vtV}_{j+1}$ and
  $\underline{\widehat{\ofH}}_j$ define an Arnoldi relation for
  \begin{equation}
    \vsK_j(\tilde{\oiA}+\oiF_j,\tilde{\vv}+\vf)
    =\vsK_j(\tilde{\oiA}+\oiF_{n+m-k},\tilde{\vv}+\vf)
    =\vsK_j(\oiP_{\vsK_n+\vsU}\tilde{\oiA},
            \oiP_{\vsK_n+\vsU}\tilde{\vv}),
    \label{eq:rec:sel:kry:def:kry}
  \end{equation}
  where $\tilde{\vv}\DEF\oiP_{\vsW^\perp,\oiA\vsW}\vv$ and
  $\vf\DEF-\oiP_{(\vsK_n+\vsU)^\perp}\tilde{\vv}$ satisfy
  $\tilde{\vv}+\vf\in\vsW^\perp$ and $\nrm{\vf}=\nrm[2]{\ofN\tilde{\vq}}$.
\end{thm}

\begin{proof}
  It is first shown that
  \[
    \tilde{\oiA}\widehat{\vtV}_{n+m-k}
    = \widehat{\vtV}_{n+m-k} \widehat{\ofH}_{n+m-k} +
    \vtZ\widehat{\ofR}_{n+m-k}.
  \]
  Therefore, note the following equations which can be derived with
  simple calculations and the definitions in
  definition~\ref{def:rec:sel:kry:def}:
  \begin{align*}
    \oiA[\vtV_n,\vtU]
      &= [\vtV_{n+1},\oiA\vtU]\ofL, \\
    \ip{[\vtV_n,\vtU]}{[\vtV_{n+1},\oiA\vtU]}
      &= \ofJ,\\
    \oiP_{\vsW^\perp,\oiA\vsW}[\vtV_{n+1},\oiA\vtU]
      &= [\vtV_{n+1},\oiA\vtU]
        - \oiA\vtW\ip{\vtW}{\oiA\vtW}^\inv\ip{\vtW}{[\vtV_{n+1},\oiA\vtU]}\\
      &= [\vtV_{n+1},\oiA\vtU] (\ofI_{n+1+m}
        - \ofL\tilde{\ofW}(\tilde{\ofW}^\htp\ofJ\ofL\tilde{\ofW})^\inv
        \tilde{\ofW}^\htp\ofJ)\\
      &= [\vtV_{n+1},\oiA\vtU]\tilde{\ofP}, \\
    \oiP_{\vsK_n+\vsU}[\vtV_{n+1},\oiA\vtU]
    &= [\vtV_n,\vtU]\ofJ, \\
    \oiA\vtU - \vtV_n\ofB - \vtU\ofE
      &= \oiA\vtU - \vtV_{n+1}\underline{\ofB} - \vtU\ofE
        + \vv_{n+1}\ve_{n+1}^\tp\underline{\ofB} \\
      &= \oiP_{(\vsK_{n+1}+\vsU)^\perp} \oiA\vtU
        + \vv_{n+1}\ve_{n+1}^\tp\underline{\ofB}, \\
    \oiP_{(\vsK_n+\vsU)^\perp}[\vtV_{n+1},\oiA\vtU]
      &= [\vtV_{n+1},\oiA\vtU] - [\vtV_n,\vtU]\ofJ \\
      &= [\vtV_{n+1},\oiA\vtU] - [\vtV_n,\vtU]
        \mat{\underline{\ofI}_n^\tp & \ofB\\ 0_{m,n+1} & \ofE} \\
      &= [\vv_{n+1},\oiA\vtU - \vtV_n\ofB - \vtU\ofE][0_{m+1,n},\ofI_{m+1}] \\
      &= [\vv_{n+1},\oiA\vtU - \vtV_{n+1}\underline{\ofB} - \vtU\ofE]
        \mat{1 & \ve_{n+1}^\tp\underline{\ofB}\\ 0_{m,1} & \ofI_m}
        [0_{m+1,n},\ofI_{m+1}] \\
      &= [\vv_{n+1},\vtQ_1]
        \mat{1 & \ve_{n+1}^\tp\underline{\ofB}\\
             0_{l,1} & [\ofR_1,\ofR_2]\ofP_{QR}^\tp}
        [0_{m+1,n},\ofI_{m+1}]
      = \vtZ \ofN.
  \end{align*}
  The above equations yield with
  $\tilde{\ofW}_\perp\tilde{\ofW}_\perp^\htp\ofJ\tilde{\ofP}=\ofJ\tilde{\ofP}$
  the following equation:
  \begin{align*}
    \tilde{\oiA}\widehat{\vtV}_{n+m-k}
    &= \oiP_{\vsW^\perp,\oiA\vsW}\oiA[\vtV_n,\vtU]
      \tilde{\ofW}_\perp\ofQ_q\ofT
    = \oiP_{\vsW^\perp,\oiA\vsW}[\vtV_{n+1},\oiA\vtU]
      \ofL\tilde{\ofW}_\perp\ofQ_q\ofT \\
    &= [\vtV_{n+1},\oiA\vtU]\tilde{\ofP}
      \ofL\tilde{\ofW}_\perp\ofQ_q\ofT \\
    &= \oiP_{\vsK_n+\vsU}[\vtV_{n+1},\oiA\vtU]\tilde{\ofP}
      \ofL\tilde{\ofW}_\perp\ofQ_q\ofT
      + \oiP_{(\vsK_n+\vsU)^\perp}[\vtV_{n+1},\oiA\vtU]\tilde{\ofP}
      \ofL\tilde{\ofW}_\perp\ofQ_q\ofT \\
    &= [\vtV_n,\vtU]\ofJ\tilde{\ofP}\ofL\tilde{\ofW}_\perp\ofQ_q\ofT
      + \vtZ\ofN \tilde{\ofP} \ofL\tilde{\ofW}_\perp\ofQ_q\ofT \\
    &= [\vtV_n,\vtU]\tilde{\ofW}_\perp\tilde{\ofW}_\perp^\htp
      \ofJ\tilde{\ofP}\ofL\tilde{\ofW}_\perp\ofQ_q\ofT
      + \vtZ\ofN \tilde{\ofP} \ofL\tilde{\ofW}_\perp\ofQ_q\ofT \\
    &= \widehat{\vtV}_{n+m-k}\ofT^\htp\ofQ_\vq\tilde{\ofW}_\perp^\htp\ofJ
      \tilde{\ofP}\ofL\tilde{\ofW}_\perp\ofQ_q\ofT
      + \vtZ\widehat{\ofR}_{n+m-k} \\
    &= \widehat{\vtV}_{n+m-k} \widehat{\ofH}_{n+m-k}
        + \vtZ\widehat{\ofR}_{n+m-k}.
  \end{align*}
  Now theorem~\ref{thm:rec:sel:kry:pert} and
  corollary~\ref{cor:rec:sel:kry:pert} apply and show that
  equations~\eqref{eq:rec:sel:kry:def:arn1}--\eqref{eq:rec:sel:kry:def:arn2}
  hold and that $\widehat{\vtV}_{j+1}$ and $\widehat{\underline{\ofH}}_n$
  actually define Arnoldi relations hold for the Krylov subspaces in
  equation~\eqref{eq:rec:sel:kry:def:kry} under the made assumption on $j$.

  It remains to show that $\tilde{\vv}+\vf\in\vsW^\perp$ and that
  $\nrm{\vf}=\nrm{\ofN\tilde{\vq}}$. Therefore, observe
  that
  \begin{align*}
    \widehat{\vtV}_{n+m-k}\ve_1
    &= [\vtV_n,\vtU]\tilde{\ofW}_\perp\ofQ_\vq\ofT\ve_1
    = [\vtV_n,\vtU]\tilde{\ofW}_\perp\ofQ_\vq\ve_1
    = \frac{\conj{\alpha}}{\nrm[2]{\vq}}
      [\vtV_n,\vtU]\tilde{\ofW}_\perp\vq \\
    &= \frac{\conj{\alpha}}{\nrm[2]{\vq}}
      [\vtV_n,\vtU]\tilde{\ofW}_\perp\tilde{\ofW}_\perp^\htp \ofJ\tilde{\vq}
    = \frac{\conj{\alpha}}{\nrm[2]{\vq}}
      [\vtV_n,\vtU]\ofJ\tilde{\vq} \\
    &= \frac{\conj{\alpha}}{\nrm[2]{\vq}}
      \oiP_{\vsK_n+\vsU}[\vtV_{n+1},\oiA\vtU]\tilde{\vq}
    = \frac{\conj{\alpha}}{\nrm[2]{\vq}}
      \oiP_{\vsK_n+\vsU}[\vtV_{n+1},\oiA\vtU]
      \tilde{\ofP}
      \mat{\nrm{\oiP_{\vsU^\perp,\oiA\vsU}\vv}\\
           0_{n,1}\\
           \ofE^\inv\ip{\vtU}{\vv}} \\
    &= \frac{\conj{\alpha}}{\nrm[2]{\vq}}
      \oiP_{\vsK_n+\vsU}
      \oiP_{\vsW^\perp,\oiA\vsW}[\vtV_{n+1},\oiA\vtU]
      \mat{\nrm{\oiP_{\vsU^\perp,\oiA\vsU}\vv}\\
           0_{n,1}\\
           \ofE^\inv\ip{\vtU}{\vv}} \\
    &= \frac{\conj{\alpha}}{\nrm[2]{\vq}}
      \oiP_{\vsK_n+\vsU}
      \oiP_{\vsW^\perp,\oiA\vsW}
      (\vv_1\nrm{\oiP_{\vsU^\perp,\oiA\vsU}\vv}
      + \oiA\vtU\ofE^\inv\ip{\vtU}{\vv}) \\
    &= \frac{\conj{\alpha}}{\nrm[2]{\vq}}
      \oiP_{\vsK_n+\vsU}
      \oiP_{\vsW^\perp,\oiA\vsW}\vv
    = \frac{\conj{\alpha}}{\nrm[2]{\vq}}
      \oiP_{\vsK_n+\vsU}
      \tilde{\vv}
    = \conj{\alpha}\frac{\tilde{\vv}
                          -\oiP_{(\vsK_n+\vsU)^\perp}\tilde{\vv}}
                          {\nrm[2]{\vq}}
    = \conj{\alpha}\frac{\tilde{\vv}+\vf}{\nrm{\tilde{\vv}+\vf}}.
  \end{align*}
  Also, for any $\vw\in\vsW\subseteq\vsK_n+\vsU$, the perturbed
  initial vector $\tilde{\vv}+\vf=\oiP_{\vsK_n+\vsU}\tilde{\vv}$
  satisfies
  \[
    \ip{\vw}{\tilde{\vv}+\vf}
    =\ip{\vw}{\oiP_{\vsK_n+\vsU}\oiP_{\vsW^\perp,\oiA\vsW}\vv}
    =\ip{\oiP_{\vsK_n+\vsU}\vw}{\oiP_{\vsW^\perp,\oiA\vsW}\vv}
    =\ip{\vw}{\oiP_{\vsW^\perp,\oiA\vsW}\vv}
    =0
  \]
  and thus is orthogonal to $\vsW$.
  The norm equality directly follows by the above equations and the fact that
  $\ip{\vtZ}{\vtZ}=\ofI_{l+1}$:
  \[
    \nrm{\vf}
    = \nrm{\oiP_{(\vsK_n+\vsU)^\perp}\tilde{\vv}}
    = \nrm{\oiP_{(\vsK_n+\vsU)^\perp}[\vtV_{n+1},\oiA\vtU]\tilde{\vq}}
    = \nrm{\vtZ\ofN\tilde{\vq}} = \nrm[2]{\ofN\tilde{\vq}}.
  \]
\end{proof}

\begin{rmk}
  \label{rem:rec:sel:kry:def}
  Theorem~\ref{thm:rec:sel:kry:def} also holds if no deflation space was used
  for the construction of $\vsK_n$, i.e., $\vsU=\{0\}$, or if no
  deflation space is considered for the next linear system, i.e., $\vsW=\{0\}$.
  In assumption~\ref{ass:rec:sel:kry:def} and
  definition~\ref{def:rec:sel:kry:def}, the bases for the deflation spaces can
  simply be chosen as ``empty'' bases with $m=0$ or $k=0$, i.e., $\vtU\in\vsH^0$
  or $\vtW=[\vtV_n,\vtU]\tilde{\ofW}\in\vsH^0$ with $\tilde{\ofW}\in\C^{n+m,0}$.
\end{rmk}

It remains to investigate if the proposed strategy can provide significant
insight into the behavior of the deflated operator
$\oiP_{\vsW^\perp,\oiA\vsW}\oiA$. In the following example, the norms of
the perturbation $\oiF_i$ are computed for the situation of
examples~\ref{ex:rec:per:def:minres} and \ref{ex:rec:sel:pri:minres}.

\begin{ex}
  \label{ex:rec:sel:kry:def}
  If MINRES is applied to the linear system $\oiA\vx=\vb$ from
  example~\ref{ex:rec:per:def:minres} with $\vx_0=0$, the residual norm
  satisfies $\frac{\nrm{\vr_n}}{\nrm{\vb}}<10^{-6}$ after $n=27$ steps, see
  figure~\ref{fig:rec:per:def:minres}. Let $\vtV_{n+1}$ and $\underline{\ofH}_n$
  define a Lanczos relation for the Krylov subspace $\vsK_n(\oiA,\vb)$ that was
  constructed by MINRES at this step.
  Now let $\tilde{\ofW}\in\C^{n,3}$ be such
  that $\vtW=\vtV_n\tilde{\ofW}$ contains the 3 Ritz vectors of $\oiA$ with
  respect to $\vsK_n(\oiA,\vb)$ that correspond to the Ritz values of smallest
  magnitude. With $\vsW=\Span{\vtW}$ and $\vsU=\{0\}$,
  theorem~\ref{thm:rec:sel:kry:def} can be applied to construct a Lanczos
  relation for the Krylov subspace
  $\tilde{\vsK}_i=\vsK_i(\oiP_{\vsW^\perp,\oiA\vsW}\oiA+\oiF_i,
  \oiP_{\vsW^\perp,\oiA\vsW}\vb+\vf)$.

  Of interest are the norms of the perturbations of the right hand side
  $\nrm{\vf}$ and of the operator perturbations $\nrm{\oiF_i}$ in
  each step $i\in\{1,\dots,l\}$, where
  $l=d(\oiP_{\vsW^\perp,\oiA\vsW}\oiA+\oiF_{n-3},
  \oiP_{\vsW^\perp,\oiA\vsW}\vb+\vf)$
  is the grade of $\oiP_{\vsW^\perp,\oiA\vsW}\vb+\vf$ with respect to
  $\oiP_{\vsW^\perp,\oiA\vsW}\oiA+\oiF_{n-3}$.  Figure~\ref{fig:rec:sel:kry:def}
  shows the development of the perturbation's norm for the mentioned choice of
  $\tilde{\ofW}$.

  \begin{figure}[htb]
    \centering
    \setlength{\figureheight}{0.5\textwidth}
    \setlength{\figurewidth}{0.65\textwidth}
    \inputplot{exp_rec_per_def_minres_arnoldify}
    \caption{Norm of the perturbation $\oiF_i$ in
      theorem~\ref{thm:rec:sel:kry:def} for
      example~\ref{ex:rec:sel:kry:def}.}
    \label{fig:rec:sel:kry:def}
  \end{figure}

  Clearly, the norm of the perturbation $\oiF_i$ is small in the first
  iterations but grows up to $\approx 0.1$ at iteration 24 where the Krylov
  subspace $\tilde{\vsK}$ with the perturbed operator and initial vector becomes
  invariant. The perturbation of the right hand side satisfies $\nrm{\vf}\approx
  \inputraw{exp_rec_per_def_minres_arnoldify_bdiff_norm.txt}$.
\end{ex}

After the above treatment of approximate Krylov subspaces, the focus is now
on how to exploit the findings for the assessment of deflation vectors for
the solution of linear systems.  Let $\vtU\in\vsH^m$ be such that
$\ip{\vtU}{\vtU}=\ofI_m$ and $\theta_{\max}(\vsU,\oiA\vsU)<\frac{\pi}{2}$ for
$\vsU=\Span{\vtU}$. If a Krylov subspace method is applied to the linear system
\[
  \oiP_{\vsU^\perp,\oiA\vsU}\oiA\widehat{\vx}
  =\oiP_{\vsU^\perp,\oiA\vsU}\vb,
\]
then the method explicitly or implicitly computes a basis
$\vtV_{n+1}\in\vsH^{n+1}$ and an extended Hessenberg matrix $\underline{\ofH}_n$
which define an Arnoldi relation for the Krylov subspace
\[
  \vsK_n
  =\vsK_n(\oiP_{\vsU^\perp,\oiA\vsU}\oiA,\oiP_{\vsU^\perp,\oiA\vsU}\vb).
\]
Now assume that $\tilde{\ofW}\in\C^{n+m,k}$ is given such that
$\theta_{\max}(\vsW,\oiA\vsW)<\frac{\pi}{2}$ for $\vsW=\Span{\vtW}$, where
$\vtW=[\vtV_n,\vtU]\tilde{\ofW}$. Then assumption~\ref{ass:rec:sel:kry:def} is
satisfied with $\vv=\vb$ and it can easily be verified that all quantities in
definition~\ref{def:rec:sel:kry:def} are available without any additional
applications of the operator $\oiA$ (see also remark~\ref{rmk:rec:sel:ritz}).
Theorem~\ref{thm:rec:sel:kry:def} now provides perturbations $\oiF_i$ for
$i\in\{1,\dots,n+m-k\}$, a basis $\widehat{\vtV}_{n+m-k}$
and a Hessenberg matrix
$\widehat{\ofH}_{n+m-k}=[\widehat{h}_{i,j}]_{i,j\in\{1,\dots,n+m-k\}}$. If
$j$ is the largest index $j\in\{1,\dots,n+m-k-1\}$ such that
$\widehat{h}_{i+1,i}\neq 0$ for all $i\in\{1,\dots,j\}$, then
$\widehat{\vtV}_{i+1}$ and $\widehat{\underline{\ofH}}_i$ define an Arnoldi
relation for the Krylov subspace
\[
  \tilde{\vsK}_i
  =\vsK_i(\oiP_{\vsW^\perp,\oiA\vsW}\oiA+\oiF_i,
    \oiP_{\vsW^\perp,\oiA\vsW}\vb+\vf)
\]
for all $i\in\{1,\dots,j\}$, where the norm of
$\vf=\oiP_{(\vsK_n+\vsU)^\perp}\oiP_{\vsW^\perp,\oiA\vsW}\vb$ is
available from theorem~\ref{thm:rec:sel:kry:def} as
$\nrm{\vf}=\nrm[2]{\ofN\tilde{\vq}}$. Now observe that $\tilde{\vsK}_i$ is the
Krylov subspace that is constructed if the Krylov subspace method is applied to
the linear system
\begin{equation}
  (\oiP_{\vsW^\perp,\oiA\vsW}\oiA+\oiF_i)\tilde{\vx}
  = \oiP_{\vsW^\perp,\oiA\vsW}\vb+\vf.
  \label{eq:rec:sel:kry:ls-pert}
\end{equation}
Note that quantities like the residual norm in the $i$-th step of the MINRES
and GMRES algorithms for the linear system~\eqref{eq:rec:sel:kry:ls-pert} can be
directly retrieved from the available Hessenberg matrix $\widehat{\ofH}_{n+m-k}$
of the Arnoldi relation for $\tilde{\vsK}_i$ without any additional applications
of the operator $\oiA$, see
sections~\ref{sec:back:mr:gmres} and \ref{sec:back:mr:minres}.

Together with the perturbation theorem~\ref{thm:rec:per:kry:full}, the norm of
the residual in the $i$-th step of the Krylov subspace method applied to the
unperturbed linear system
\begin{equation*}
  \oiP_{\vsW^\perp,\oiA\vsW}\oiA\overline{\vx}
  = \oiP_{\vsW^\perp,\oiA\vsW}\vb
\end{equation*}
can be bounded\footnote
{Theorem~\ref{thm:rec:per:kry:full} only addresses
GMRES (and thus also MINRES). However, an extension to the
$\oiA$-norm of the error in the CG method is straightforward.}.
A crucial observation is that the perturbation $\oiF_i$ is sufficient in order
to obtain statements for the $i$-th step because for early iterations $i$, the
norm  $\nrm{\oiF_i}$ can be significantly smaller than the norm of
$\oiF_{n+m-k}$ which can also be used for the generation of the same Krylov
subspace, cf.\ example~\ref{ex:rec:sel:kry:def} and
figure~\ref{fig:rec:sel:kry:def}.

In fact, one can go further and directly obtain bounds for the Krylov subspace
method when it is applied to the linear system that is of main interest in this
section:
\begin{equation}
  \oiP_{\vsW^\perp,\oiB\vsW}\oiB\widehat{\vy}
  = \oiP_{\vsW^\perp,\oiB\vsW}\vc.
  \label{eq:rec:sel:kry:ls-next}
\end{equation}
With $\oiA_{\oiF_i} = \oiP_{\vsW^\perp,\oiA\vsW}\oiA+\oiF_i$ and $\vb_\vf =
\oiP_{\vsW^\perp,\oiA\vsW}\vb+\vf$, the operator and right hand side in
equation~\eqref{eq:rec:sel:kry:ls-next} satisfy
\begin{align*}
  \oiP_{\vsW^\perp,\oiB\vsW}\oiB
    &= \oiA_{\oiF_i} + \widehat{\oiF}_i&
    \qquad\text{with}\quad
    \widehat{\oiF}_i
    &= \oiP_{\vsW^\perp,\oiB\vsW}\oiB - \oiP_{\vsW^\perp,\oiA\vsW}\oiA - \oiF_i\\
  \text{and}\quad
  \oiP_{\vsW^\perp,\oiB\vsW}\vc
    &= \vb_\vf + \widehat{\vf}&
    \qquad\text{with}\quad
    \widehat{\vf}
    &= \oiP_{\vsW^\perp,\oiB\vsW}\vc - \oiP_{\vsW^\perp,\oiA\vsW}\vb - \vf.
\end{align*}

The next theorem is required in order to bound the terms
$\nrm{\widehat{\oiF}_i}$ and $\nrm{\widehat{\vf}}$ in the above equations. The
result can be seen as a complement to theorem~\ref{thm:rec:per:proj:diff} by
providing a computable bound on the norm of the difference
$\oiP_{\vsW^\perp,\oiB\vsW} - \oiP_{\vsW^\perp,\oiA\vsW}$ in terms of the
operator $\oiA$ and the difference $\oiF=\oiB-\oiA$.

\begin{thm}
  \label{thm:rec:sel:kry:projdiff}
  Let $\oiA,\oiB\in\vsL(\vsH)$ and $\oiF=\oiB-\oiA$. Assume that $\vtW\in\vsH^m$
  is given such that $\ip{\vtW}{\vtW}=\ofI_m$ and
  \begin{equation}
    \sigma_{\min}(\ip{\vtW}{\oiA\vtW})>\nrm[2]{\ip{\vtW}{\oiF\vtW}}.
    \label{eq:rec:sel:projdiff:ass}
  \end{equation}
  Then $\theta_{\max}(\vsW,\oiA\vsW)<\frac{\pi}{2}$ and
  $\theta_{\max}(\vsW,\oiB\vsW)<\frac{\pi}{2}$ and the following holds:
  \[
    \nrm{\oiP_{\vsW^\perp,\oiB\vsW} - \oiP_{\vsW^\perp,\oiA\vsW}}
    \leq \nrm{\oiP_{\vsW^\perp,\oiA\vsW}}
    \frac{\nrm{\restr{\oiF}{\vsW}}}
    {\sigma_{\min}(\ip{\vtW}{\oiA\vtW}) - \nrm[2]{\ip{\vtW}{\oiF\vtW}}}.
  \]
\end{thm}

\begin{proof}
  First note that $\theta_{\max}(\vsW,\oiA\vsW)<\frac{\pi}{2}$ follows
  immediately from theorem~\ref{thm:back:proj_sing} since $\ip{\vtW}{\oiA\vtW}$
  has to be nonsingular by assumption~\eqref{eq:rec:sel:projdiff:ass}. Now
  observe that the smallest singular value of $\ip{\vtW}{\oiB\vtW}$ satisfies
  \[
    \sigma_{\min}(\ip{\vtW}{\oiB\vtW})
    = \sigma_{\min}(\ip{\vtW}{\oiA\vtW} + \ip{\vtW}{\oiF\vtW})
    \geq \sigma_{\min}(\ip{\vtW}{\oiA\vtW}) - \nrm[2]{\ip{\vtW}{\oiF\vtW}}
    > 0
  \]
  also by assumption~\eqref{eq:rec:sel:projdiff:ass} and thus
  $\theta_{\max}(\vsW,\oiB\vsW)<\frac{\pi}{2}$. Then the projections are
  well defined and
  \begin{align*}
    \nrm{\oiP_{\vsW^\perp,\oiB\vsW} - \oiP_{\vsW^\perp,\oiA\vsW}}
    &= \nrm{\oiP_{\oiB\vsW,\vsW^\perp} - \oiP_{\oiA\vsW,\vsW^\perp}}
    = \sup_{\vx\in\C^m,\vx\neq 0}
      \frac
      {\nrm{\oiP_{\oiB\vsW,\vsW^\perp}\vtW\vx -
        \oiP_{\oiA\vsW,\vsW^\perp}\vtW\vx}}
      {\nrm[2]{\vx}} \\
    &= \sup_{\vx\in\C^m,\vx\neq 0}
      \frac
      {\nrm{\oiB\vtW\ip{\vtW}{\oiB\vtW}^\inv\vx
        -\oiA\vtW\ip{\vtW}{\oiA\vtW}^\inv\vx}}
      {\nrm[2]{\vx}} \\
    &= \sup_{\vy\in\C^m,\vy\neq 0}
      \frac
      {\nrm{\oiB\vtW\vy
        -\oiA\vtW\ip{\vtW}{\oiA\vtW}^\inv\ip{\vtW}{\oiB\vtW}\vy}}
      {\nrm[2]{\ip{\vtW}{\oiB\vtW}\vy}}\\
    &= \sup_{\vy\in\C^m,\vy\neq 0}
      \frac
      {\nrm{\oiP_{\vsW^\perp,\oiA\vsW}\oiB\vtW\vy}}
      {\nrm[2]{\ip{\vtW}{\oiB\vtW}\vy}}
    \leq \sup_{\vy\in\C^m,\vy\neq 0}
      \frac
      {\nrm{\oiP_{\vsW^\perp,\oiA\vsW}\oiB\vtW\vy}}
      {\sigma_{\min}(\ip{\vtW}{\oiB\vtW})\nrm[2]{\vy}} \\
    &= \frac
      {\nrm{\oiP_{\vsW^\perp,\oiA\vsW}\oiB\vtW}}
      {\sigma_{\min}(\ip{\vtW}{\oiB\vtW})}
    \leq \frac
      {\nrm{\oiP_{\vsW^\perp,\oiA\vsW}\oiB\vtW}}
      {\sigma_{\min}(\ip{\vtW}{\oiA\vtW})- \nrm[2]{\ip{\vtW}{\oiF\vtW}}} \\
    &= \frac
      {\nrm{\oiP_{\vsW^\perp,\oiA\vsW}(\oiA+\oiF)\vtW}}
      {\sigma_{\min}(\ip{\vtW}{\oiA\vtW})- \nrm[2]{\ip{\vtW}{\oiF\vtW}}} \\
    &= \frac
      {\nrm{\oiP_{\vsW^\perp,\oiA\vsW}\oiF\vtW}}
      {\sigma_{\min}(\ip{\vtW}{\oiA\vtW})- \nrm[2]{\ip{\vtW}{\oiF\vtW}}} \\
    &\leq \nrm{\oiP_{\vsW^\perp,\oiA\vsW}}
      \frac
      {\nrm{\restr{\oiF}{\vsW}}}
      {\sigma_{\min}(\ip{\vtW}{\oiA\vtW})- \nrm[2]{\ip{\vtW}{\oiF\vtW}}}.
  \end{align*}
\end{proof}

The following corollary is a straightforward application of the above theorem.

\begin{cor}
  \label{cor:rec:sel:kry:projdiff}
  Let the assumptions of theorem~\ref{thm:rec:sel:kry:projdiff} hold and let
  \[
    \eta\DEF
    \frac{\nrm{\restr{\oiF}{\vsW}}}
    {\sigma_{\min}(\ip{\vtW}{\oiA\vtW}) - \nrm[2]{\ip{\vtW}{\oiF\vtW}}}.
  \]

  Then
  \[
    \nrm{\oiP_{\vsW^\perp,\oiB\vsW}\oiB - \oiP_{\vsW^\perp,\oiA\vsW}\oiA}
    \leq
    \nrm{\oiP_{\vsW^\perp,\oiA\vsW}}\big(
      \eta(\nrm{\oiA}+\nrm{\oiF}) + \nrm{\oiF}
    \big).
  \]
  If $\vb,\vf\in\vsH$ and $\vc\DEF\vb+\vf$, then analogously
  \[
    \nrm{\oiP_{\vsW^\perp,\oiB\vsW}\vc - \oiP_{\vsW^\perp,\oiA\vsW}\vb}
    \leq
    \nrm{\oiP_{\vsW^\perp,\oiA\vsW}}\big(
      \eta(\nrm{\vb}+\nrm{\vf}) + \nrm{\vf}
    \big).
  \]
\end{cor}

\begin{proof}
  Only the first inequality is shown because the proof of the second
  inequality is completely analogous.
  \begin{align*}
    \nrm{\oiP_{\vsW^\perp,\oiB\vsW}\oiB - \oiP_{\vsW^\perp,\oiA\vsW}\oiA}
    &= \nrm{(\oiP_{\vsW^\perp,\oiB\vsW}-\oiP_{\vsW^\perp,\oiA\vsW})\oiB +
      \oiP_{\vsW^\perp,\oiA\vsW}(\oiB-\oiA)} \\
    &\leq
      \nrm{\oiP_{\vsW^\perp,\oiB\vsW}-\oiP_{\vsW^\perp,\oiA\vsW}}\nrm{\oiA+\oiF}
      + \nrm{\oiP_{\vsW^\perp,\oiA\vsW}}\nrm{\oiF} \\
    &\leq \nrm{\oiP_{\vsW^\perp,\oiA\vsW}} \big(
        \eta(\nrm{\oiA}+\nrm{\oiF}) + \nrm{\oiF}
      \big).
  \end{align*}
\end{proof}

The following theorem finally wraps up the above results and shows how the
convergence of a Krylov subspace method for the linear system
\[
  \oiP_{\vsW^\perp,\oiB\vsW}\oiB\widehat{\vy}
  =\oiP_{\vsW^\perp,\oiB\vsW}\vc
\]
can be characterized for a given deflation subspace
$\vsW\subseteq\vsK_n+\vsU$ after the Krylov subspace
$\vsK_n$ has been generated by a Krylov subspace method applied to the
linear system
\[
  \oiP_{\vsU^\perp,\oiA\vsU}\oiA\widehat{\vx}
  =\oiP_{\vsU^\perp,\oiA\vsU}\vb.
\]
To the knowledge of the author, no similar statements have been shown in the
literature.

\begin{thm}
  \label{thm:rec:sel:kry:gmres}
  Assume that $\oiA\vx=\vb$ and $\oiB\vy=\vc$ are linear systems with
  nonsingular operators $\oiA,\oiB\in\vsL(\ofH)$, right hand sides
  $\vb,\vc\in\vsH$ and assume that assumption~\ref{ass:rec:sel:kry:def} is
  fulfilled with $\vv=\vb$. Furthermore, let
  $\widehat{\ofH}=\widehat{\ofH}_{n+m-k}$, $\oiF_i$, $\widehat{\ofR}_i$ for
  $i\in\{1,\dots,n+m-k\}$, $\ofN$, $\vq$ and $\tilde{\vq}$ be as in
  theorem~\ref{thm:rec:sel:kry:def} for $i\in\{1,\dots,n+m-k\}$.  Let
  $l=d(\widehat{\ofH},\ve_1)$ be the grade of $\widehat{\ofH}$ with respect to
  $\ve_1$ and let for $i\in\{1,\dots,l\}$ the $i$-th residual of GMRES applied
  to the linear system
  \begin{equation}
    \widehat{\ofH}\vz = \nrm[2]{\vq}\ve_1
    \label{eq:rec:sel:kry:ls-small}
  \end{equation}
  be denoted by
  $\tilde{\vr_i}=p_i(\widehat{\ofH})\nrm[2]{\vq}\ve_1$,
  where $p_i\in\Polys_{n,0}$.

  Furthermore, let $\oiG=\oiB-\oiA$ and $\vg=\vc-\vb$, assume that
  $\sigma_{\min}(\ip{\vtW}{\oiA\vtW})>\nrm[2]{\ip{\vtW}{\oiG\vtW}}$
  holds and define
  \[
    \eta\DEF\frac{\nrm{\restr{\oiG}{\vsW}}}
      {\sigma_{\min}(\ip{\vtW}{\oiA\vtW})-\nrm[2]{\ip{\vtW}{\oiG\vtW}}}.
  \]

  Then the $i$-th residual $\widehat{\vr}_i$ of the GMRES method applied to the
  linear system
  \begin{equation}
    \oiP_{\vsW^\perp,\oiB\vsW}\oiB\widehat{\vy}=\oiP_{\vsW^\perp,\oiB\vsW}\vc
    \label{eq:rec:sel:kry:gmres:ls}
  \end{equation}
  satisfies
  \begin{equation}
    \nrm{\widehat{\vr}_i}
    \leq
      \nrm[2]{\tilde{\vr}_i}
      + \frac{|\partial\Lambda_\delta(\widehat{\oiA}_i)|}{2\pi\delta}
      \left(
        \frac{\varepsilon_i}{\delta-\varepsilon_i}
        (\nrm[2]{\vq}+\beta)
        +\beta
      \right)
      \sup_{\lambda\in\Lambda_\delta(\widehat{\oiA}_i)}|p_i(\lambda)|
    \label{eq:rec:sel:kry:gmres}
  \end{equation}
  for all $\delta>\varepsilon_i$, where
  \begin{align*}
    \widehat{\oiA}_i
      &\DEF\restr{(\oiP_{\vsW^\perp,\oiA\vsW}\oiA+\oiF_i)}{\vsW^\perp},\\
    \varepsilon_i&\DEF
      \nrm{\oiP_{\vsW^\perp,\oiA\vsW}}\big(
        \eta(\nrm{\oiA}+\nrm{\oiG})+\nrm{\oiG}
      \big)
      + \nrm[2]{\widehat{\ofR}_i} \\
    \text{and}\quad
    \beta&\DEF
      \nrm{\oiP_{\vsW^\perp,\oiA\vsW}}\big(
        \eta(\nrm{\vb}+\nrm{\vg})+\nrm{\vg}
      \big)
      + \nrm[2]{\widehat{\ofN}\tilde{\vq}}.
  \end{align*}
\end{thm}

\begin{proof}
  Let $\widehat{\vtV}_{n+m-k}$ be as in theorem~\ref{thm:rec:sel:kry:def}.
  It then follows from proposition~\ref{pro:back:arnoldi-poly} that
  \[
    \nrm[2]{\tilde{\vr}_i}
    = \nrm[2]{p_i(\widehat{\ofH})\nrm[2]{\vq}\ve_1}
    = \nrm{\widehat{\vtV}_{n+m-k}p_i(\widehat{\ofH})\nrm[2]{\vq}\ve_1}
    %= \nrm{p_i(\tilde{\oiA}_i)\widehat{\vtV}_{n+m-k}\nrm[2]{\vq}\ve_1}
    = \nrm{p_i(\tilde{\oiA}_i) \tilde{\vb}}
  \]
  with
  \[
    \tilde{\vb}
    =\oiP_{\vsK_n+\vsU}\oiP_{\vsW^\perp,\oiA\vsW}\vb
    =\oiP_{\vsW^\perp,\oiA\vsW}\vb + \vf,
  \]
  where $=\oiP_{(\vsK_n+\vsU)^\perp}\vf\oiP_{\vsW^\perp,\oiA\vsW}\vb$
  is as in theorem~\ref{thm:rec:sel:kry:def}. Note that $p_i$ is the GMRES
  residual polynomial for the $i$-th step of GMRES for the linear system
  $\tilde{\oiA}_i\tilde{\vx}=\tilde{\vb}$ and observe that
  \begin{align*}
    \oiP_{\vsW^\perp,\oiB\vsW}\oiB&=\tilde{\oiA}_i + \tilde{\oiF}_i&
    \quad\text{with}\quad
    \tilde{\oiF}_i&\DEF
      \oiP_{\vsW^\perp,\oiB\vsW}\oiB
      -\oiP_{\vsW^\perp,\oiA\vsW}\oiA
      -\oiF_i \\
    \text{and}\quad
    \widehat{\vc}
    \DEF\oiP_{\vsW^\perp,\oiB\vsW}\vc&=\tilde{\vb} + \tilde{\vf}&
    \quad\text{with}\quad
    \tilde{\vf}&\DEF
      \oiP_{\vsW^\perp,\oiB\vsW}\vc
      -\oiP_{\vsW^\perp,\oiA\vsW}\vb
      -\vf.
  \end{align*}
  Note that the application of corollary~\ref{cor:rec:sel:kry:projdiff} yields
  with the norm equalities for $\oiF_i$ and $\vf$ in
  theorem~\ref{thm:rec:sel:kry:def} the inequalities
  \begin{align*}
    \nrm{\tilde{\oiF}_i}
      &\leq \nrm{\oiP_{\vsW^\perp,\oiA\vsW}} \big(
        \eta(\nrm{\oiA}+\nrm{\oiG})+\nrm{\oiG}
      \big) + \nrm{\oiF_i} \\
      &= \nrm{\oiP_{\vsW^\perp,\oiA\vsW}} \big(
        \eta(\nrm{\oiA}+\nrm{\oiG})+\nrm{\oiG}
      \big) + \nrm[2]{\widehat{\ofR}_i}
      = \varepsilon_i\\
    \text{and}\quad
    \nrm{\tilde{\vf}}
      &\leq \nrm{\oiP_{\vsW^\perp,\oiA\vsW}} \big(
        \eta(\nrm{\vb}+\nrm{\vg}+\nrm{\vg}
      \big) + \nrm{\vf}\\
      &= \nrm{\oiP_{\vsW^\perp,\oiA\vsW}} \big(
        \eta(\nrm{\vb}+\nrm{\vg}+\nrm{\vg}
      \big) + \nrm[2]{\ofN\tilde{\vq}}
      = \beta.
  \end{align*}
  Because $\tilde{\vb},\widehat{\vc}\in\vsW^\perp$ and
  $\vsR(\tilde{\oiA}_i),\vsR(\oiP_{\vsW^\perp,\oiB\vsW}\oiB),
  \vsR(\tilde{\oiF}_i)\subseteq\vsW^\perp$, the operators can be restricted to
  the subspace $\vsW^\perp$, i.e.,
  \begin{align*}
    \widehat{\oiA}_i\DEF
      \restr{\tilde{\oiA}_i}{\vsW^\perp}
      :\vsW^\perp\lra\vsW^\perp,
    \qquad
    \widehat{\oiF}_i
      \DEF\restr{\tilde{\oiF}_i}{\vsW^\perp}
      :\vsW^\perp\lra\vsW^\perp \\
    \text{and}\quad
    \widehat{\oiB}\DEF
      \restr{\oiP_{\vsW^\perp,\oiB\vsW}\oiB}{\vsW^\perp}
      =\widehat{\oiA}_i+\widehat{\oiF}_i
      :\vsW^\perp\lra\vsW^\perp.
  \end{align*}
  The $i$-th residual $\overline{\vr}_i$ of the GMRES method applied to the
  linear system $\widehat{\oiA}_i\widehat{\vx}=\tilde{\vb}$ thus satisfies
  \[
    \nrm{\overline{\vr}_i}
    =\nrm{p_i(\widehat{\oiA}_i)\tilde{\vb}}
    =\nrm[2]{\tilde{\vr}_i}
  \]
  while the $i$ residual $\widehat{\vr}_i$ of the GMRES method applied to
  the linear system~\eqref{eq:rec:sel:kry:gmres:ls} equals the $i$-th residual
  of the GMRES method applied to the linear system
  $\widehat{\oiB}\widehat{\vy}=\widehat{\vc}$. Now
  theorem~\ref{thm:rec:per:kry:full} can be applied in order to obtain
  \begin{align*}
    \nrm{\widehat{\vr}_i}
    &\leq \nrm{\overline{\vr}_i}
      + \frac{|\partial\Lambda_\delta(\widehat{\oiA}_i)|}{2\pi\delta}
      \left(
        \frac{\varepsilon_i}{\delta-\varepsilon_i}\nrm{\widehat{\vc}}
        + \nrm{\widehat{\vc}-\tilde{\vb}}
      \right)
      \sup_{\lambda\in\Lambda_\delta(\widehat{\oiA}_i)}|p_i(\lambda)| \\
    &= \nrm[2]{\tilde{\vr}_i}
      + \frac{|\partial\Lambda_\delta(\widehat{\oiA}_i)|}{2\pi\delta}
      \left(
        \frac{\varepsilon_i}{\delta-\varepsilon_i}
        \nrm{\tilde{\vb}+\tilde{\vf}}
        +\nrm{\tilde{\vf}}
      \right)
      \sup_{\lambda\in\Lambda_\delta(\widehat{\oiA}_i)}|p_i(\lambda)| \\
    &\leq \nrm[2]{\tilde{\vr}_i}
      + \frac{|\partial\Lambda_\delta(\widehat{\oiA}_i)|}{2\pi\delta}
      \left(
        \frac{\varepsilon_i}{\delta-\varepsilon_i}
        (\nrm{\tilde{\vb}}+\beta)
        + \beta
      \right)
      \sup_{\lambda\in\Lambda_\delta(\widehat{\oiA}_i)}|p_i(\lambda)|\\
    &= \nrm[2]{\tilde{\vr}_i}
      + \frac{|\partial\Lambda_\delta(\widehat{\oiA}_i)|}{2\pi\delta}
      \left(
        \frac{\varepsilon_i}{\delta-\varepsilon_i}
        (\nrm[2]{\vq}+\beta)
        + \beta
      \right)
      \sup_{\lambda\in\Lambda_\delta(\widehat{\oiA}_i)}|p_i(\lambda)|.
  \end{align*}
\end{proof}

\begin{rmk}
  \label{rem:rec:sel:kry:comp}
  An algorithm for computing the right hand side of
  bound~\eqref{eq:rec:sel:kry:gmres} is implemented in~\cite{krypy} as
  \lstinline{krypy.deflation.bound_pseudo}.  A representation of the polynomial
  $p_i$ is obtained by computing harmonic Ritz values of $\widehat{\ofH}$, cf.\
  theorem~\ref{thm:back:gmres}.  Due to the fact that $p_i$ is holomorphic, its
  maximum is attained on the boundary of the pseudospectrum, i.e.,
  $\sup_{\lambda\in\Lambda_\delta(\widehat{\oiA}_i)}|p_i(\lambda)|
  =\sup_{\lambda\in\partial\Lambda_\delta(\widehat{\oiA}_i)}|p_i(\lambda)|$.
  The pseudospectrum package PseudoPy~\cite{pseudopy} is used to approximate
  $\partial\Lambda_\delta(\widehat{\oiA}_i)$ by
  $\partial\Lambda_\delta(\widehat{\ofH})$, see the discussion in the remaining
  part of this subsection.  The boundary
  $\partial\Lambda_\delta(\widehat{\ofH})$ is approximated by polygons and the
  polynomial $p_i$ is then evaluated on the polygons' vertices in order to
  obtain an approximation of the supremum in bound~\eqref{eq:rec:sel:kry:gmres}.
\end{rmk}

\begin{rmk}
  \label{rem:rec:sel:kry:proj}
  Skimming through the proof of theorem~\ref{thm:rec:sel:kry:gmres} reveals why
  only the projections $\oiP_{\vsW^\perp,\oiA\vsW}$ and
  $\oiP_{\vsW^\perp,\oiB\vsW}$ are considered in this subsection: if the
  projections $\oiP_{(\oiA\vsW)^\perp}$ and $\oiP_{(\oiB\vsW)^\perp}$ are used,
  then the restrictions of the deflated operators
  $\restr{\oiP_{(\oiA\vsW)^\perp}\oiA}{\vsW^\perp}$ and
  $\restr{\oiP_{(\oiB\vsW)^\perp}\oiB}{\vsW^\perp}$ are operators from
  $\vsW^\perp$ to $(\oiA\vsW)^\perp$ and $(\oiB\vsW)^\perp$, respectively. Thus,
  the restricted operators cannot be used in the above setting because
  $(\restr{\oiP_{(\oiA\vsW)^\perp}\oiA}{\vsW^\perp})^i$ and
  $(\restr{\oiP_{(\oiB\vsW)^\perp}\oiB}{\vsW^\perp})^i$ are not well defined in
  general. Instead, the unrestricted operators could be used but then the
  spectra and thus also the pseudospectra in~\eqref{eq:rec:sel:kry:gmres}
  contain zero if $\vsW\neq\{0\}$ which clearly renders the bound useless.
\end{rmk}

\begin{rmk}
  In finite precision arithmetic, the computed Arnoldi or Lanczos relations
  may deviate significantly from their exact arithmetic counterparts. As
  mentioned in section~\ref{sec:back:round-off}, this applies particularly to
  methods that are based on Lanczos recurrences.
  Theorem~\ref{thm:rec:sel:kry:pert} and the descending
  theorems~\ref{thm:rec:sel:kry:def} and \ref{thm:rec:sel:kry:gmres} rely on the
  orthogonality of the provided bases. In experiments, minor deviations from
  orthogonality do not seem to have an excessive effect on the computed
  quantities but the impact of the complete loss of orthogonality on the above
  quantities is unexplored.
\end{rmk}

Note that Sifuentes, Embree and Morgan~\cite{SifEM13} also applied the
pseudospectral bound from their article to deflated methods, see theorem~4.5
in~\cite{SifEM13}. However, their approach is fundamentally different and serves
a very different purpose.  In~\cite{SifEM13}, the goal is to analyze
\emph{restarted} GMRES with a deflation-based preconditioner that uses
approximate invariant subspaces. In contrast, the goal is here to obtain
estimates for deflated GMRES for a linear system with the data that has been
computed while solving a \emph{different} linear system.  In~\cite{SifEM13}, it
is assumed that the GMRES convergence behavior is known in the case where the
preconditioner is built with an \emph{exact} invariant subspace.
Clearly, this information is not available in practice and
theorem~\ref{thm:rec:sel:kry:gmres} does not have this requirement. Instead,
the new theorem shows how residual norms and polynomials can be computed
cheaply from \emph{available} data.
Furthermore, a condition on the separation of certain eigenvalues has to be
fulfilled in~\cite{SifEM13} while theorem~\ref{thm:rec:sel:kry:gmres} only
depends on a condition that is required to guarantee a well-defined projection
for the next linear system.
In contrast to~\cite{SifEM13}, the new theorem is not restricted to Ritz
vectors as deflation vectors but holds for any vectors from the provided
subspaces.
Both approaches have in common that they require the pseudospectrum of a
potentially large matrix or operator. The efficient approximation of this
pseudospectrum is dealt with in the following.

With respect to the applicability of the bound in
theorem~\ref{thm:rec:sel:kry:gmres}, one puzzle still remains
to be solved: the pseudospectrum $\Lambda_\delta(\widehat{\oiA}_i)$ is not
available in practice. Nothing is known about the behavior of the operator
$\oiA$ or $\widehat{\oiA}_i$ on the subspace $(\vsK_n+\vsU)^\perp$ and
as forecasted in the beginning of section~\ref{sec:rec:sel}, it is unlikely that
a precise and entirely non-heuristic prediction of the convergence behavior is
possible with the very limited data that is available in practice. Nevertheless,
an approximation can fill the gap in applications because the action of
$\widehat{\oiA}_i$ is known on the subspace $(\vsK_n+\vsU)\cap\vsW^\perp$.

If $\widehat{\vtV}_{n+m-k}\in\vsH^{n+m-k}$ is as defined in
theorem~\ref{thm:rec:sel:kry:def}, then by construction
$\Span{\widehat{\vtV}_{n+m-k}}=(\vsK_n+\vsU)\cap\vsW^\perp$. Instead
of asking for the full $\varepsilon_i$-pseudospectrum of $\widehat{\oiA}_i$, a
projection technique can be employed as proposed by Toh and Trefethen
in~\cite{TohT96}; see also chapters 40 and 46 in the book of Trefethen and
Embree~\cite{TreE05}. In the following, the definition of the pseudospectrum of
rectangular matrices as given in~\cite{TreE05} is important.

\begin{definition}
  \label{def:rec:sel:kry:pseudo-rect}
  Let $\ofA\in\C^{m,n}$ with $m\geq n$ and let $\varepsilon>0$. The
  $\varepsilon$-pseudospectrum of $\ofA$ is defined by
  \[
    \Lambda_\varepsilon(\ofA)
    =\{\lambda\in\C~|~\sigma_{\min}
      (\lambda\underline{\ofI}_n-\ofA)<\varepsilon\},
  \]
  where $\underline{\ofI}_n=\mat{\ofI_n\\0_{m-n,n}}$.
\end{definition}

The following theorem characterizes the pseudospectrum of rectangular matrices
and can be found in~\cite{TreE05}.

\begin{thm}
  \label{thm:rec:sel:kry:pseudo-rect}
  Let $\ofA\in\C^{m,n}$ with $m\geq n$ and let $\varepsilon>0$. Then
  \begin{enumerate}
    \item $\Lambda_\varepsilon\left(\ofA\mat{\ofI_k\\0_{m-k,k}}\right)
      \subseteq\Lambda_\varepsilon(\ofA)$ for $k\in\{1,\dots,m-1\}$.
    \item $\Lambda_\varepsilon(\ofA)
      \subseteq\Lambda_\varepsilon\left(\mat{\ofI_k&0_{k,n-k}}\ofA\right)$
      for $k\in\{1,\dots,n-1\}$.
  \end{enumerate}
\end{thm}

\begin{proof}
  See theorem~46.2 in \cite{TreE05}.
\end{proof}

The following corollary applies the above theorem to the situation of a
decomposition as given in theorem~\ref{thm:rec:sel:kry:pert}. The idea is not
new and has been used with a regular Arnoldi relation in~\cite{TohT96}.

\begin{cor}
  \label{cor:rec:sel:kry:pseudo-rect}
  Let $\vtV\in\vsH^n$ and $\vtW\in\vsH^m$ such that
  $\ip{[\vtV,\vtW]}{[\vtV,\vtW]}=\ofI_{n+m}$ and assume that
  \[
    \oiA\vtV = \vtV\ofG + \vtW\ofR
  \]
  for $\oiA\in\vsL(\vsH)$, $\ofG\in\C^{n,n}$ and $\ofR\in\C^{m,n}$.

  Then for any $\varepsilon>0$
  \[
    \Lambda_\varepsilon\left(\mat{\ofG\\\ofR}\right)
    \subseteq\Lambda_\varepsilon(\oiA).
  \]
\end{cor}

\begin{proof}
  Let $N=\dim(\oiA)$ and $\vtZ\in\vsH^{N-m-n}$ such that
  $\ip{[\vtV,\vtW,\vtZ]}{[\vtV,\vtW,\vtZ]}=\ofI_N$. Then
  \[
    \Lambda_\varepsilon(\oiA)
    =\Lambda_\varepsilon\left(\ip{[\vtV,\vtW,\vtZ]}{\oiA[\vtV,\vtW,\vtZ]}\right)
  \]
  and by theorem~\ref{thm:rec:sel:kry:pseudo-rect}
  \[
    \Lambda_\varepsilon(\ip{[\vtV,\vtW,\vtZ]}{\oiA\vtV})
    =\Lambda_\varepsilon\left(
      \ip{[\vtV,\vtW,\vtZ]}{\oiA[\vtV,\vtW,\vtZ]}\mat{\ofI_n\\0_{N-n,n}}
    \right)
    \subseteq\Lambda_\varepsilon(\oiA).
  \]
  Now notice that
  \[
    \ip{[\vtV,\vtW,\vtZ]}{\oiA\vtV}=\mat{\ofG\\\ofR\\0_{N-n-m,n}}
    \quad\text{and}\quad
    \sigma_{\min}\left(\mat{\ofG\\\ofR\\0_{N-n-m,n}}\right)
    =\sigma_{\min}\left(\mat{\ofG\\\ofR}\right).
  \]
  Thus the statement follows with definition~\ref{def:rec:sel:kry:pseudo-rect}.
\end{proof}

With corollary~\ref{cor:rec:sel:kry:pseudo-rect}, an approximation to the
pseudospectrum of $\widehat{\oiA}_i$ can be computed in the setting of
theorem~\ref{thm:rec:sel:kry:gmres} as demonstrated in the following theorem.

\begin{thm}
  \label{thm:rec:sel:kry:gmres-pseudo}
  Let the assumptions of theorem~\ref{thm:rec:sel:kry:gmres} hold.

  Then
  \[
    \Lambda_\varepsilon\left(
      \mat{\widehat{\ofH}\\ \ofS_i}
    \right)
    \subseteq \Lambda_\varepsilon(\widehat{\oiA}_i)
  \]
  holds for any $\varepsilon>0$ with
  $\ofS_i\DEF\widehat{\ofR}_{n+m-k}\diag(0_{i,i},\ofI_{n+m-k-i})$.
\end{thm}

\begin{proof}
  It follows from theorem~\ref{thm:rec:sel:kry:gmres} that
  \begin{align*}
    \widehat{\oiA}_i\widehat{\vtV}_{n+m-k}
    &=(\widehat{\oiA}_{n+m-k} + \oiF_i-\oiF_{n+m-k})\widehat{\vtV}_{n+m-k} \\
    &=\left(\widehat{\oiA}_{n+m-k}
    - \vtZ\widehat{\ofR}_i\widehat{\vtV}_i^\adj
    + \vtZ\widehat{\ofR}_{n+m-k}\widehat{\vtV}_{n+m-k}^\adj
    \right)\widehat{\vtV}_{n+m-k} \\
    &=\left(\widehat{\oiA}_{n+m-k}
    + \vtZ(\widehat{\ofR}_{n+m-k}-[\widehat{\ofR}_i,0_{n+m-k-i}])
    \widehat{\vtV}_{n+m-k}^\adj
    \right)\widehat{\vtV}_{n+m-k}\\
    &=\left(\widehat{\oiA}_{n+m-k}
    + \vtZ\ofS_i\widehat{\vtV}_{n+m-k}^\adj
    \right)\widehat{\vtV}_{n+m-k}
    =\widehat{\vtV}_{n+m-k} \widehat{\ofH} + \vtZ\ofS_i.
  \end{align*}
  Now corollary~\ref{cor:rec:sel:kry:pseudo-rect} applies with
  $\vtV=\widehat{\vtV}_{n+m-k}$ and $\vtW=\vtZ$ and the proof is complete.
\end{proof}

Theorem~\ref{thm:rec:sel:kry:gmres-pseudo} shows a way to approximate the
pseudospectrum of $\widehat{\oiA}_i$ from the inside. Although parts of the
pseudospectrum may be missing, theorem~\ref{thm:rec:sel:kry:gmres-pseudo} uses
all information about $\widehat{\oiA}_i$ that is known, namely the behavior on
the subspace
$\Span{\widehat{\vtV}_{n+m-k}}=(\vsK_n+\vsU)\cap\vsW^\perp$.
In many cases, such a
projection scheme onto a small subspace approximates the actual pseudospectrum
very well. Examples and counter-examples with large-scale matrices and
projections onto Krylov subspaces (without deflation) can be found
in~\cite[chapter~40]{TreE05}.

Another approximation to the pseudospectrum can be obtained by the
pseudospectrum of
$\widehat{\ofH}=\ip{\widehat{\vtV}_{n+m-k}}{\oiA\widehat{\vtV}_{n+m-k}}$.
However, $\Lambda_\varepsilon(\widehat{\ofH})$ does not satisfy the containment
property in theorem~\ref{thm:rec:sel:kry:gmres-pseudo} in general, i.e.,
\[
  \Lambda_\varepsilon(\widehat{\ofH})
  \not\subseteq\Lambda_\varepsilon(\widehat{\oiA}_i).
\]
Nevertheless, $\Lambda_\varepsilon(\widehat{\ofH})$ can provide a meaningful
approximation to $\Lambda_\varepsilon(\widehat{\oiA}_i)$. If $\oiA$ is normal or
self-adjoint, then $\widehat{\ofH}$ is normal or Hermitian,
respectively, and $\Lambda_\varepsilon(\widehat{\ofH})$ becomes trivial to
compute with the spectrum $\Lambda(\widehat{\ofH})$:
\begin{align*}
  \Lambda_\varepsilon(\widehat{\ofH})
  &= \bigcup_{\mu\in\Lambda(\ofH)} \{\lambda\in\C~|~|\lambda-\mu|<\varepsilon\}
  \qquad\text{if}~\widehat{\ofH}~\text{is normal}\\
  \text{and}\quad
  \Lambda_\varepsilon(\widehat{\ofH})
  &= \bigcup_{\mu\in\Lambda(\ofH)} \{\lambda\in\R~|~|\lambda-\mu|<\varepsilon\}
  \qquad\text{if}~\widehat{\ofH}~\text{is Hermitian}.
\end{align*}

The above approximations of pseudospectra are implemented in
PseudoPy~\cite{pseudopy}. For a non-normal matrix, the resolvent norm is sampled
in a neighborhood of the spectrum which is guaranteed to contain the
pseudospectrum's boundary that is of interest. The actual computation of
bound~\eqref{eq:rec:sel:kry:gmres} is described in
remark~\ref{rem:rec:sel:kry:comp}.

\begin{figure}[htbp]
  \setlength{\figureheight}{0.42\textwidth}
  \setlength{\figurewidth}{0.6\textwidth}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \centering
    \inputplot{exp_rec_per_def_minres_approx}
    \caption{Deflation spaces $\vsW_3$, $\vsW_2$ and $\vsW_1$ and no deflation
      (from left to right).}
    \label{fig:rec:sel:kry:minres:4}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \centering
    \inputplot{exp_rec_per_def_minres_approx_10}
    \caption{Deflation spaces from~(\protect\subref{fig:rec:sel:kry:minres:4})
      and $\vsW_4,\dots,\vsW_9$. }
    \label{fig:rec:sel:kry:minres:10}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \centering
    \inputplot{exp_rec_per_def_minres_approx_pert}
    \caption{Same as~(\protect\subref{fig:rec:sel:kry:minres:4}) but for a
      perturbed matrix $\oiA+\oiG$ and right hand side $\vb+\vg$ with
      $\nrm{\oiG}=10^{-7}$ and $\nrm{\vg}=0.1$.}
    \label{fig:rec:sel:kry:minres:4:pert}
  \end{subfigure}
  \caption{Approximate Krylov subspace bound~\eqref{eq:rec:sel:kry:gmres}
    (dashed) and actual convergence (light solid) of MINRES for
    example~\ref{ex:rec:sel:kry:minres}.
  }
  \label{fig:rec:sel:kry:minres}
\end{figure}

\begin{ex}
  \label{ex:rec:sel:kry:minres}
  Similar to the experiment with a priori bounds in
  example~\ref{ex:rec:sel:pri:minres}, the approximate Krylov subspace
  bound~\eqref{eq:rec:sel:kry:gmres} is applied to the linear system from
  example~\ref{ex:rec:per:def:minres}. It is again assumed that 27 MINRES
  iterations have been performed for the linear system $\oiA\vx=\vb$ and that
  the task is to find an optimal choice of Ritz vectors from the constructed
  Krylov subspace for deflation. Let $\vsW_i$ denote the subspace that is
  spanned by the $i$ Ritz vectors that correspond to the $i$ Ritz values of
  smallest magnitude.

  In figure~\ref{fig:rec:sel:kry:minres:4}, the
  bound~\eqref{eq:rec:sel:kry:gmres} is plotted together with the actual
  convergence history of MINRES without deflation and with the deflation
  subspaces $\vsW_1$, $\vsW_2$ and $\vsW_3$. The bound is able to accurately
  capture the convergence behavior for all cases. For $\vsW_1$ and $\vsW_2$, the
  bound is not able to give a meaningful result beyond the steps 17 and 13
  because the second summand in~\eqref{eq:rec:sel:kry:gmres} grows too large
  such that the residual norm would start to increase. With $\vsW_3$ this
  happens only after the bound dropped below $10^{-14}$ (see
  figure~\ref{fig:rec:sel:kry:minres:10}) which is surprising given that the
  original linear system was only solved up to the tolerance $10^{-6}$.

  Figure~\ref{fig:rec:sel:kry:minres:10} shows that no significant improvement
  can be achieved by choosing more than 3 Ritz vectors. Note that the
  bound~\eqref{eq:rec:sel:kry:gmres} is still able to capture the initial phase
  of convergence with 4 or more Ritz vectors before drifting apart from the
  actual convergence curve.

  Besides the ability to bound the convergence behavior for the same linear
  system, theorem~\ref{thm:rec:sel:kry:gmres} allows to make statements for
  a perturbed linear system $\oiB\vy=\vc$ with $\oiB=\oiA+\oiG$ and
  $\vc=\vb+\vg$. The assumption
  $\sigma_{\min}(\ip{\vtW}{\oiA\vtW})>\nrm{\ip{\vtW}{\oiG\vtW}}$ is, e.g.,
  fulfilled by a random matrix $\oiG$ of norm $10^{-7}$.
  Figure~\ref{fig:rec:sel:kry:minres:4:pert} shows the bound and actual
  convergence histories where $\vg$ is chosen as a random vector of norm $0.1$.
  Note that there is no visible difference of the actual convergence histories
  to the corresponding ones in figure~\ref{fig:rec:sel:kry:minres:4}.  The run
  with 3 Ritz vectors still provides a meaningful bound for the actual behavior
  but the bound does not give much insight for 2 or less Ritz vectors.
\end{ex}

In the next section~\ref{sec:rec:cd}, bound~\eqref{eq:rec:sel:kry:gmres} is
evaluated for a less academic linear system with a non-symmetric matrix that
stems from a convection-diffusion problem.


\section{Numerical experiments with GMRES and a convection-diffusion problem}
\label{sec:rec:cd}

In this section, the results of the preceding section are applied to the GMRES
method for the solution of a convection-diffusion model problem. The considered
partial differential equation is
\begin{equation}
\begin{aligned}
  -\nu\Laplace u + \vw\cdot\nabla u &= 0
  \quad\text{in}\quad
  \Omega\DEF]-1,1[\times]-1,1[\\
  \text{and}\quad
  u&=q
  \quad\text{on}\quad\partial\Omega,
\end{aligned}
  \label{eq:rec:cd}
\end{equation}
where $u:\overline{\Omega}\lra\R$ is the sought solution for the diffusion
parameter $\nu>0$, the velocity field $\vw=[\vw_1,\vw_2]^\tp:\Omega\lra\R^2$ and
the Dirichlet boundary function $q:\partial\Omega\lra\R$. Here, the reference
problem~3.1.4 from the book of Elman, Silvester and Wathen~\cite{ElmSW05} is
used as a basis for numerical experiments. The velocity field is a recirculating
flow defined by $\vw(x,y)\DEF[2y(1-x^2),-2x(1-y^2)]^\tp$ and the boundary
conditions are given by
\begin{equation}
  q(x,y)=
  \begin{cases}
    1&\text{if}~x=1,\\
    0&\text{otherwise.}
  \end{cases}
  \label{eq:rec:cd:bound}
\end{equation}
The diffusion parameter is chosen as $\nu=\frac{1}{200}$ which renders the
equation convection-dominated. A finite element discretization is obtained for
equation~\eqref{eq:rec:cd} with linear Lagrange elements on a regular
triangulation exhibiting $1301$ vertices. In order to avoid non-physical
oscillations in the approximate solution, the discretization incorporates the
streamline diffusion stabilization (SUPG) as described in~\cite{ElmSW05}. The
finite element implementation is based on the Python package DOLFIN from the
FEniCS project~\cite{LogW10,LogWH12}. Figure~\ref{fig:rec:cd:sol} shows the
discrete solution for the described configuration.

\begin{figure}[htb]
  \centering
  \setlength\figurewidth{0.5\textwidth}
  \setlength\figureheight{0.5\figurewidth}
  \input{figures/exp_conv_diff_sol.tikz}
  \caption{Three-dimensional surface plot of the discrete solution
    of~\eqref{eq:rec:cd} on a mesh with $1301$ vertices.}
  \label{fig:rec:cd:sol}
\end{figure}

Besides requiring special attention in the discretization, the domination of the
convection term also poses a challenge for Krylov subspace methods. The finite
element discretization of~\eqref{eq:rec:cd} with streamline diffusion yields a
non-symmetric and highly non-normal matrix $\ofA_h\in\R^{N,N}$ and a
corresponding right hand side $\vb_h\in\R^N$ that incorporates the Dirichlet
boundary conditions. The typical convergence behavior of GMRES for the linear
system $\ofA_h\vx=\vb_h$ is an initial phase of slow convergence which is
followed by a fast residual reduction. A characteristic quantity of
convection-diffusion problems is the mesh P\'eclet number $P_h$ which is defined
by $P_h\DEF\frac{h\nrm{\vw}}{2\nu}$ for a regular mesh with grid size $h$ and
constant velocity norm $\nrm{\vw}$. In the model problem considered here, the
maximal element P\'eclet number $P_{h_i}=\frac{h_i\nrm{\vw}}{2\nu}$ is $\approx
16$.

In~\cite{Ern00}, Ernst analyzed the convergence behavior of GMRES for
convection-dominated convection-diffusion model problems and showed that the
condition number of the eigenvector basis of the resulting matrix grows
exponentially with both the mesh P\'eclet number and grid size. Therefore, the
spectral bound~\eqref{eq:back:gmres:spectral} gives no insight into the actual
convergence behavior of GMRES\@. It was observed that the initial phase of slow
convergence can be characterized by the field of values
bound~\eqref{eq:back:gmres:fov} while the second phase of faster convergence
appears to be dominated by spectral properties. Furthermore, Ernst conjectured
that the length of the initial phase of slow convergence is determined by the
time the boundary values take to be transported along the longest streamline
through the domain. In~\cite{LieS05}, Liesen and Strako\v{s} illustrated and
explained the drastic influence of the right hand side $\vb_h$ on the initial
period of slow convergence. Like in equation~\eqref{eq:rec:cd}, the forcing term
is zero in~\cite{LieS05} and thus the right hand side $\vb_h$ is determined by
the boundary values.

For the experiments in this section, an incomplete LU (ILU) decomposition of
$\ofA_h$ is used as a preconditioner $\ofM\approx\ofA_h^\inv$. Instead of
$\ofA_h\vx=\vb_h$, GMRES is applied to the linear system
\begin{equation}
  \ofA\vx=\vb
  \label{eq:rec:cd:ls}
\end{equation}
with $\ofA=\ofM\ofA_h$, $\vb=\ofM\vb_h$ and the initial guess $\vx_0$
is zero except for the boundary unknowns where it satisfies the boundary
conditions.

The convergence history of GMRES applied to~\eqref{eq:rec:cd:ls} is
plotted as the solid red line in figure~\ref{fig:rec:cd:defl} and clearly shows
that the slow initial convergence is not remedied by the use of the ILU
preconditioner. If Ritz pairs are computed from the Krylov subspace that has
been generated by GMRES and a few Ritz vectors are used as deflation vectors for
the \emph{same} linear system, then the initial phase of near-stagnation is
relieved or even completely removed, see the light colored curves in
figure~\ref{fig:rec:cd:defl}. The approximate Krylov subspace
bound~\eqref{eq:rec:sel:kry:gmres} is able to reproduce the convergence behavior
for deflated GMRES accurately in the early phase until it diverges around
$10^{-6}$. However, the first summand $\nrm[2]{\tilde{\vr}_n}$ (dotted)
in~\eqref{eq:rec:sel:kry:gmres} still lies on the actual convergence curve far
beyond the point where the bound~\eqref{eq:rec:sel:kry:gmres} diverges.

\begin{figure}[ht]
  \setlength{\figureheight}{0.55\textwidth}
  \setlength{\figurewidth}{0.65\textwidth}
  \centering
  \inputplot{exp_conv_diff_gmres}
  \caption{Convergence histories for (deflated) GMRES applied to the linear
    system~\eqref{eq:rec:cd:ls}. The solid red curve represents the original run
    (without deflation) from which Ritz pairs are computed. The curves in light
    colors represent GMRES with deflation of one up to 14 Ritz vectors (from
    right to left) corresponding to the Ritz values of smallest magnitude.
    Bound~\eqref{eq:rec:sel:kry:gmres} and its first summand
    $\nrm[2]{\tilde{\vr}_n}$ are plotted as dashed and dotted lines,
    respectively. Note that the dotted curves coincide with the light curves.
  }
  \label{fig:rec:cd:defl}
\end{figure}

An interesting question is how the bound~\eqref{eq:rec:sel:kry:gmres} behaves in
a sequence of linear systems. Similar to the previous sections, only two
subsequent linear systems are considered here. First, it is analyzed how
perturbations of the right hand side $\vb$ affect the bound before considering
perturbations in the matrix $\ofA$.

In order to perturb the right hand side, the boundary
function~\eqref{eq:rec:cd:bound} is modified with $\gamma\geq 0$ to
\[
  q_\gamma(x,y)=
  \begin{cases}
    1 + \gamma (1-y^2)& \text{if}~x=1,\\
    0 & \text{otherwise.}
  \end{cases}
\]
The resulting (preconditioned) right hand side is denoted by $\vc_\gamma$ and
the difference by $\vg_\gamma=\vb-\vc_\gamma$. In
figure~\ref{fig:rec:cd:defl:g}, the bound is evaluated for the linear system
$\ofA\vx_\gamma=\vc_\gamma$, where $\gamma$ takes the values $10^{-3}$ and
$10^{-2}$ (dashed lines). The light colored curves represent the actual GMRES
residual norms for the perturbed right hand side and do not differ significantly
from the unperturbed counterparts in figure~\ref{fig:rec:cd:defl}.

In the next experiment, the right hand side remains the same but the matrix is
perturbed by replacing the velocity field $\vw$ in~\eqref{eq:rec:cd} by
$\vw_\omega\DEF(1+\omega)\vw$ for a $\omega\in\R$. The resulting matrix
$\ofB_{h,\omega}$ is preconditioned with the matrix $\ofM$ that is based on the
unperturbed matrix $\ofA_h$. Thus, the linear system $\ofB_\omega\vx_\omega=\vb$ has to
be solved, where $\ofB_\omega=\ofM\ofB_{h,\omega}$. The difference of the
matrices is denoted by $\ofG_\omega=\ofA-\ofB_\omega$.
Figure~\ref{fig:rec:cd:defl:G} shows that already very small perturbations in
the matrix lead to crude estimations with bound~\eqref{eq:rec:sel:kry:gmres}.

\begin{figure}[htbp]
  \setlength\figurewidth{0.49\textwidth}
  \setlength\figureheight{\figurewidth}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \inputplot{exp_conv_diff_gmres_g_0}
    \hfill
    \inputplot{exp_conv_diff_gmres_g_1}
    \caption{Perturbed right hand side: $\ofA\vx_\gamma=\vc_\gamma$ with
      $\gamma=10^{-3}$ (left, $\nrm{\vg_\gamma}\approx 5\cdot 10^{-3}$)
      and
      $\gamma=10^{-2}$ (right, $\nrm{\vg_\gamma}\approx 5\cdot 10^{-2}$).
    }
    \label{fig:rec:cd:defl:g}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \inputplot{exp_conv_diff_gmres_G_0}
    \hfill
    \inputplot{exp_conv_diff_gmres_G_1}
    \caption{Perturbed matrix: $\ofB_\omega\vx_\omega=\vb$ with
      $\omega=10^{-6}$ (left, $\nrm{\ofG_\omega}\approx 5\cdot 10^{-6}$)
      and
      $\omega=10^{-5}$ (right, $\nrm{\ofG_\omega}\approx 5\cdot 10^{-5}$).
    }
    \label{fig:rec:cd:defl:G}
  \end{subfigure}
  \caption{Bound~\eqref{eq:rec:sel:kry:gmres} (dashed) and actual residual norms
  (light solid) with perturbed right hand side $\vc_\gamma$ (top) and perturbed
  matrix $\ofB_\omega$ (bottom). The deflation vectors are chosen as the
  $0,\dots,10$ Ritz vectors corresponding to the Ritz values of smallest
  magnitude.
  }
  \label{fig:rec:cd:defl:Gg}
\end{figure}

Note that the second summand in inequality~\eqref{eq:rec:sel:kry:gmres} depends
linearly on $\beta$ and that the range $]\epsilon_i,\infty[$ for the level
$\delta$ of the pseudospectrum is thus not affected by the choice of the right
hand side. In contrast, a perturbation of the matrix enlarges $\epsilon_i$ and
thus restricts the range for $\delta$. Usually, large values of $\delta$
minimize \eqref{eq:rec:sel:kry:gmres} in early iterations while small values
close to $\epsilon_i$ are required for later iterations because the polynomial
$p_i$ is of higher degree and therefore grows faster away from its zeros.
This observation explains why the bound is much more sensitive to perturbations
of the matrix than of the right hand side.

The above experiments show that the bound~\eqref{eq:rec:sel:kry:gmres} often
severely overestimates the residual norms due to the presence of the
pseudospectral term. Because the first summand $\nrm[2]{\tilde{\vr}_n}$ captures
the behavior in the above experiments far beyond the point where the
pseudospectral term begins to dominate, it is reasonable to use only
$\nrm[2]{\tilde{\vr}_n}$ as an indicator for the effectiveness of a given set of
deflation vectors. Furthermore, the computation of $\nrm[2]{\tilde{\vr}_n}$ is
cheap because the extraordinary expensive computation of the pseudospectrum and the
construction and evaluation of the polynomials in~\eqref{eq:rec:sel:kry:gmres}
is not required. This strategy is combined with timings of the involved
operations in chapter~\ref{ch:nls} for the solution of nonlinear Schr\"odinger
equations. Clearly, dropping the pseudospectral term in
inequality~\eqref{eq:rec:sel:kry:gmres} renders the bound ignorant of
perturbations in the matrix and right hand side. Recall that
example~\ref{ex:rec:per:kry:stag} and figure~\ref{fig:rec:per:kry:stag} show
that a small perturbation of the matrix or right hand side may result in a
dramatic change of the convergence behavior of Krylov subspace methods.


\chapter{Numerical solution of nonlinear Schr\"odinger equations}
\label{ch:nls}

In~\cite{GauS13}, Schl\"{o}mer and the author investigated a recycling MINRES
variant for the numerical solution of nonlinear Schr\"{o}dinger equations.
Nonlinear Schr\"odinger equations are used to describe a wide variety of
physical systems like superconductivity, quantum condensates, nonlinear
acoustics~\cite{SomGD79}, nonlinear optics~\cite{GedSB97}, and
hydrodynamics~\cite{NorBF93}. In the numerical solution of nonlinear
Schr\"odinger equations with Newton's method, a linear system has to be solved
with the Jacobian operator for each Newton update. Because the Jabobian operator
is self-adjoint with respect to a non-Euclidean inner product and indefinite in
general, the MINRES method can be used for the computation of the Newton
updates. However, the spectrum of the Jacobian operators becomes unfavorable
once the Newton iterates approach a nonzero solution and the MINRES residual
norms stagnate for several iterations. The number of required MINRES iterations
as well as the overall time consumption can be significantly reduced with a
recycling strategy that uses Ritz vectors as deflation vectors as proposed in
chapter~\ref{ch:rec}.  One important instance of nonlinear Schr\"odinger
equations is the Ginzburg--Landau equation that models phenomena of certain
superconductors.  The nonlinear Schr\"odinger equations and the Ginzburg--Landau
equation are only discussed briefly here. An in-depth description with
further references can be found in~\cite{GauS13} and the articles by Schl\"omer,
Avitabile and Vanroose~\cite{SchAV12} and Schl\"omer and Vanroose~\cite{SchV13}.
The presented numerical results in this chapter and the used software package
\emph{PyNosh}~\cite{pynosh} for the solution of nonlinear Schr\"odinger
equations have been developed jointly by Schl\"omer and the author and have been
published to a great extent in~\cite{GauS13}.


\section{Nonlinear Schr\"odinger equations}
\label{sec:nls:nls}

For $d\in\{2,3\}$ and an open domain $\Omega\subseteq\R^d$, the general
nonlinear Schr\"odinger equation can be derived by minimizing the Gibbs energy
in a physical system. Let $\oiS:\vsX\lra\vsY$ be defined by
\begin{equation}
  \oiS(\psi) \DEF (\oiK+\oiV+g|\psi|^2)\psi
  \quad\text{in}~\Omega,
  \label{eq:nls:nls:op}
\end{equation}
where $\vsX\subseteq L^2(\Omega)$ is the natural energy space of the problem and
$\vsY\subseteq L^2(\Omega)$. The space $\vsX$ can incorporate appropriate
boundary conditions in the setting of a bounded domain. The linear operator
$\oiK\in\vsL(\vsX,\vsY)$ is assumed to be self-adjoint and positive
semidefinite with respect to the inner product $\ipdots[L^2(\Omega)]$,
$\oiV:\Omega\lra\R$ is a scalar potential and $g>0$ is the nonlinearity
parameter.  A state $\psi^\star:\Omega\lra\C$ is called a solution of the
nonlinear Schr\"odinger equation if
\begin{equation}
  \oiS(\psi^\star)=0
  \label{eq:nls:nls:sol}
\end{equation}
and solutions of interest are nontrivial solutions, i.e., $\nrm{\psi^\star}\neq
0$. The magnitude $|\psi^\star|^2$ typically describes a particle density or,
more generally, a probability distribution. It follows directly from
\begin{equation}
  \oiS(\e^{\iu\varphi}\psi) = \e^{\iu \varphi} \oiS(\psi)
  \label{eq:nls:nls:eq}
\end{equation}
that a solution $\psi^\star$ defines an equivalence class
$\{\e^{\iu\varphi}\psi^\star~|~\varphi\in\R\}$ of physically equivalent
solutions.

If a good-enough initial guess $\psi_0\in\vsX$ is given, Newton's method can be
used to generate a sequence of iterates $(\psi_i)_{i\in\N_+}$ which converges
quadratically towards a solution $\psi^\star$ of the nonlinear Schr\"odinger
equation~\eqref{eq:nls:nls:eq}. In each step of Newton's method, a
linear system has to be solved with the Jacobian operator
$\oiJ_\psi:\vsX\lra\vsY$ of $\oiS$ at $\psi$ which is defined by
\begin{equation}
  \oiJ_\psi\phi\DEF(\oiK+\oiV + 2g|\psi|^2)\phi + g\psi^2\conj{\phi}.
  \label{eq:nls:nls:jac}
\end{equation}
The functions $\phi$ are complex-valued in general and the Jacobian operator
$\oiJ_\psi$ is not linear if $\vsX$ and $\vsY$ are treated as vector spaces over
the field $\C$ because of the complex conjugation. However, the Jacobian
$\oiJ_\psi$ is linear if $\vsX$ and $\vsY$ are defined as vector spaces over the
field $\R$ with the corresponding inner product
\begin{equation}
  \ipdots[\R]\DEF\Real\ipdots[L^2(\Omega)].
  \label{eq:nls:nls:ip}
\end{equation}
This definition is in accordance with the fact that the specific complex
argument of a solution $\psi^\star$ is of no physical relevance, cf.\
equation~\eqref{eq:nls:nls:eq}. With an initial guess $\psi_0\in\vsX$, the
following linear systems have to be solved for $i\in\N_+$:
\begin{equation}
  \oiJ_{\psi_i}\delta_i = -\oiS(\psi_i),
  \quad\text{where}\quad\psi_{i+1}=\psi_i+\delta_i.
  \label{eq:nls:nls:ls}
\end{equation}

The adjoint operator of the Jacobian operator $\oiJ_\psi$ can be characterized
with a result by Schl\"omer, Avitabile and Vanroose~\cite{SchAV12} and is of
importance for the numerical solution of the linear systems in Newton's method.

\begin{cor}
  Let $\vsX,\vsY\subseteq L^2(\Omega)$ and let $\psi\in\vsX$. Then
  $\oiJ_\psi:\vsX\lra\vsY$ as defined as above is self-adjoint with respect to
  the inner product $\ipdots[\R]$, cf.\ equation~\eqref{eq:nls:nls:ip}.
\end{cor}

\begin{proof}
  The statement immediately follows from lemma~3.1 in~\cite{SchAV12}.
\end{proof}

The spectrum of $\oiJ_\psi$ thus is real but $\oiJ_\psi$ is indefinite, in
general. Furthermore, it follows from~\eqref{eq:nls:nls:eq} that for any
$\psi\in\vsX$ the equation
\[
  \oiJ_\psi(\iu\psi)
  = (\oiK+\oiV+2g|\psi|^2)\iu\psi + g\psi^2\conj{\iu\psi}
  = (\oiK+\oiV+2g|\psi|^2)\iu\psi - g|\psi|^2 \iu\psi
  = \iu \oiS(\psi)
\]
holds. For a non-trivial solution $\psi^\star$ of the nonlinear Schr\"odinger
equation~\eqref{eq:nls:nls:sol}, the Jacobian operator $\oiJ_{\psi^\star}$ thus
is singular. The singularity of the Jacobian operator in a solution $\psi^\star$
is again a direct consequence of the fact that $\psi^\star$ defines an
equivalence class of physically equivalent solutions.

If $\oiJ_{\psi^\star}$ is positive semidefinite, the solution $\psi^\star$ is
called a stable solution. In practice, solutions with low Gibbs energies tend to
be stable whereas highly energetic solutions tend to be unstable, i.e., the
Jacobian operator $\oiJ_{\psi^\star}$ exhibits negative eigenvalues.

The singularity of the Jacobian operator imposes additional challenges to the
numerical solution of nonlinear Schr\"odinger equations:
\begin{enumerate}
  \item In the area of attraction of a solution $\psi^\star$, the quadratic
    convergence of Newton's method cannot be expected if the Jacobian operator
    $\oiJ_{\psi^\star}$ is singular and only linear convergence is guaranteed,
    cf.\ Decker, Keller and Kelley~\cite{DecKK83}, Deuflhard~\cite{Deu11}
    and Kelley~\cite{Kel03}.
  \item Although no linear system has to be solved with an exactly singular
    Jacobian operator, the spectrum of $\oiJ_{\psi_i}$ will exhibit at least one
    eigenvalue of small magnitude when $\psi_i$ approaches a solution
    $\psi^\star$. The presence of one or a few small magnitude eigenvalues may
    seriously impede the convergence of Krylov subspace methods, cf.\
    sections~\ref{sec:back:cg} and \ref{sec:back:mr}.
\end{enumerate}

In order to alleviate the effects of a singular Jacobian operator, several
approaches have been proposed in the literature. Detailed treatments of this
case can be found, e.g., in the works of Reddien~\cite{Red78}, Decker and
Kelley~\cite{DecK80}, Decker, Keller and Kelley~\cite{DecKK83} and
Griewank~\cite{Gri85}. A widely used approach is known as \emph{bordering} and
effectively modifies the original problem such that the singularity is
eliminated, cf.\ Keller~\cite{Kel83} and Griewank~\cite{Gri85}. In the setting
of nonlinear Schr\"odinger equations this amounts to prescribing the complex
argument in a consistent way. The bordering approach has been applied to the
Ginzburg--Landau equation in~\cite{SchAV12} and is naturally generalizable to
nonlinear Schr\"odinger equations. Although bordering yields nonsingular
Jacobian operators, the approach has the major disadvantage that the choice of
an appropriate preconditioner is not clear for the bordered system even if a
good preconditioner is available for the original problem. Thus bordering is
impractical for discretizations of PDEs with a large number of unknowns.
Furthermore, Schl\"omer and the author noticed in~\cite{GauS13} that the
singularity of $\oiJ_{\psi^\star}$ in a solution $\psi^\star$ does not lead to a
loss of the quadratic convergence in numerical solutions of the Ginzburg--Landau
equation, see figure~\ref{fig:nls:exp:newtonres}. Therefore, the bordering
approach is not further pursued here.

The more severe consequence of the singularity of the Jacobian operator
$\oiJ_{\psi^\star}$ in a solution $\psi^\star$ is that the numerical solution of
the linear systems~\eqref{eq:nls:nls:ls} with Krylov subspace methods is
complicated by the fact that at least one eigenvalue of the Jacobian operator
$\oiJ_{\psi_i}$ approaches the origin as the Newton iterates $\psi_i$ approach a
solution. In the numerical experiments in section~\ref{sec:nls:exp}, it can be
observed that the MINRES residual norms stagnate for several iterations and it
is shown that recycling strategies are able to remove these phases of stagnation
and significantly reduce the number of MINRES iterations as well as the overall
time consumption.


\section{Ginzburg--Landau equation and discretization}
\label{sec:nls:gl}

An important instance of nonlinear Schr\"odinger
equations~\eqref{eq:nls:nls:sol} is the Ginzburg--Landau equation
that models the physical phenomenon of superconductivity for extreme-type-II
superconductors. Here, only the basic equations and their most important
properties are presented. An extensive description with further pointers to
literature can be found, e.g., in the article of Du, Gunzburger and
Peterson~\cite{DuGP93}. For $d\in\{2,3\}$ and an open and bounded domain
$\Omega\subseteq\R^d$ that describes the superconducting material, the
Ginzburg--Landau equations are
\begin{equation}
  0 =
  \begin{cases}
    \oiK\psi - \psi(1-|\psi|^2) &\text{in}~\Omega,\\
    \textup{n} \cdot (-\iu\nabla - A)\psi &\text{on}~\partial\Omega,
  \end{cases}
  \label{eq:nls:gl}
\end{equation}
where the linear operator $\oiK:\vsX\lra\vsY$ between $\vsX,\vsY\subset L^2(\Omega)$
is defined by
\begin{equation*}
  \oiK\phi\DEF(-\iu\nabla - A)^2\phi
\end{equation*}
with a given magnetic vector potential $A\in H_{\R^d}^2(\Omega)$. The operator
$\oiK$ is often referred to as the kinetic energy operator and describes the
energy of a charged particle when it is exposed to the magnetic field $B=\nabla
\times  A$. Solutions $\psi^\star$ of the Ginzburg--Landau
equation~\eqref{eq:nls:gl} describe the density $|\psi^\star|^2$ of electric
charge carriers (also referred to as \emph{Cooper pair} density) and are known
to fulfill $0\leq|\psi^\star|\leq1$ almost everywhere, cf.~\cite{DuGP93}. For
two-dimensional domains, solutions $\psi^\star$ typically exhibit isolated zeros
referred to as \emph{vortices} while lines of zeros are the typical solution
pattern for three-dimensional domains, see figure~\ref{fig:nls:exp:solutions}.


As outlined by Schl\"omer and Vanroose in~\cite{SchV13}, the
operator $\oiK$ is self-adjoint and positive semidefinite with respect to the
inner product $\ipdots[L^2(\Omega)]$ on the space
\[
  \vsX=\{\psi\in H_\C^2(\Omega)~|~\textup{n}
    \cdot(-\iu\nabla-A)\psi=0~\text{almost everywhere on}~\partial\Omega\}.
\]
Note that $\vsX$ is the subspace of $H_\C^2(\Omega)$ that satisfies the boundary
conditions in~\eqref{eq:nls:gl}. Furthermore, $\oiK$ was shown to be positive
definite in~\cite{SchV13} if and only if the magnetic field $B$ does not vanish.
Hence, the operator $\oiK$ and thus the Ginzburg--Landau equation fit in the
setting of the nonlinear Schr\"odinger equations~\eqref{eq:nls:nls:op} with
$\oiV\equiv-1$ and $g=1$. In each iteration of Newton's method, a linear
system~\eqref{eq:nls:nls:ls} has to be solved with the Jacobian
operator~\eqref{eq:nls:nls:jac}.

The discretization of the Ginzburg--Landau equation has to be carried out
carefully in order to retain important properties of the infinite-dimensional
problem such as self-adjointness of the Jacobian operator and gauge
invariance of solutions, i.e., invariance with respect to the complex argument.
In~\cite{SchV13}, a finite volume discretization of the Ginzburg--Landau
equation is presented that preserves these properties. The discretized Jacobian
operator $\oiJ_\vv:\C^n\lra\C^n$ at a discrete state
$\vv=[\vv_1,\dots,\vv_n]\in\C^n$ is is defined with $\vz\in\C^n$ by
\begin{equation}
  \oiJ_\vv \vz
  \DEF\left(\ofD^\inv\ofK
    - \ofI_n + 2|\ofD_\vv|\right) \vz
    + \ofD_\vv \conj{\vz},
  \label{eq:nls:gl:jac}
\end{equation}
where $\ofK\in\C^{n,n}$ is a Hermitian and positive-definite matrix,
$\ofD=\diag(|\Omega_1|,\dots,|\Omega_n|)$ is the diagonal matrix with the
volumes of the Voronoi regions $\Omega_1,\dots,\Omega_n$ that result from a
Voronoi tesselation of a disctretization of the domain $\Omega$. The remaining
matrices that constitute the Jacobian operator~\eqref{eq:nls:gl:jac} are defined
by $\ofD_\vv\DEF\diag(\vv_1^2,\dots,\vv_n^2)$ and
$|\ofD_\vv|\DEF\diag(|\vv_1|^2,\dots,|\vv_n|^2)$. The discrete Jacobian
operator~\eqref{eq:nls:gl:jac} is self-adjoint with respect to the discrete inner
product $\ipdots[\R]$ which is defined for $\vv,\vw\in\C^n$ by
\begin{equation}
  \ip[\R]{\vv}{\vw}\DEF\Real\left(\vv^\htp\ofD\vw\right).
  \label{eq:nls:gl:ip}
\end{equation}
This inner product corresponds to the natural inner
product~\eqref{eq:nls:nls:ip} of the infinite-dimensional problem.

A remark seems to be appropriate here because the definition of a linear
operator in the form~\eqref{eq:nls:gl:jac} along with the inner
product~\eqref{eq:nls:gl:ip} may seem odd at first sight. As discussed in
section~\ref{sec:nls:nls}, the vector space $\C^n$ is treated as a vector space
over the field $\R$, thus effectively doubling the dimension to $2n$. An element
$\vv\in\C^n$ can be represented in the basis
$\ve_1,\dots,\ve_n,\iu\ve_1,\dots,\iu\ve_n$ as the vector
$[\Real\vv,\Imag\vv]\in\R^{2n}$ and the Jacobian operator $\oiJ_\vv$ can
therefore be represented as a matrix in $\R^{2n,2n}$ with respect to this basis.
However, the equivalent real-valued formulation is not further considered here
and instead the natural complex-valued formulation is used.

Summarizing, if an initial guess $\vz_0$ is given for Newton's method, then the
linear systems
\begin{equation*}
  \oiJ_{\vz_i}\delta_i = - \oiS(\vz_i)
  \quad\text{with}\quad
  \vz_{i+1}=\vz_i+\delta_i
\end{equation*}
have to be solved for $i\in\N$ until the Newton residual $\oiS(\vz_i)$ satisfies
$\nrm{\oiS(\vz_i)}<\epsilon$ for a prescribed tolerance $\epsilon$. Note that
the norm $\nrmdots$ is the norm that is induced by the natural inner
product~\eqref{eq:nls:gl:ip}.


\section{Numerical experiments with recycling MINRES}
\label{sec:nls:exp}

In this section, numerical experiments are conducted with the Ginzburg--Landau
equation from section~\ref{sec:nls:gl} in two and in three dimensions.

\begin{setup}[2D]
  \label{set:nls:2d}
  The domain is chosen as the circle
  $\Omega^{\text{2D}}\DEF\{\vx\in\R^2~|~\nrm[2]{\vx}<5\}$ and the magnetic
  vector potential is defined as
  \[
    A^{\text{2D}}([x_1,x_2]^\tp)
    \DEF m\times
    \frac{[x_1,x_2,0]^\tp-\vx_0}{\nrm[2]{[x_1,x_2,0]^\tp-\vx_0}^3}
  \]
  which corresponds to the magnetic field that is generated by a dipole at
  $\vx_0\DEF[0,0,5]^\tp$ with orientation $m\DEF[0,0,1]^\tp$. A Delaunay
  triangulation for $\Omega^{2D}$ with $n=3299$ points was created with
  \emph{Triangle}~\cite{She02}. The initial guess $\vz_0\in\C^n$ is chosen as
  the interpolation of $\psi_0^{\text{2D}}(x,y)=\cos(\pi x)$. With this setup,
  Newton's method takes 27 iterations until the Newton residual norm falls below
  $10^{-10}$. The Newton residual norms and the found approximate solution
  $\psi^{\text{2D}}\DEF\vz_{27}$ are visualized in
  figures~\ref{fig:nls:exp:newtonres} and \ref{fig:nls:exp:solutions}.
\end{setup}

\begin{setup}[3D]
  \label{set:nls:3d}
  The domain is chosen as the three-dimensional ``L-shape''
  $\Omega^{\text{3D}}\DEF\{\vx\in\R^3~|~\nrm[\infty]{\vx}<5\}
  \setminus\R_{\geq}^3$ and the magnetic field is chosen as the constant
  function $B^{\text{3D}}\equiv \frac{1}{\sqrt{3}}[1,1,1]^\tp$ which is
  represented by the magnetic vector potential
  $A^{\text{3D}}(\vx)\DEF\frac{1}{2} B^{\text{3D}}\times \vx$. The domain was
  discretized using \emph{Gmsh}~\cite{GeuR09} with 72166 points. The initial
  guess is chosen as the constant function $\psi_0^{\text{3D}}\equiv 1$. After
  22 iterations of Newton's method, the residual norm drops below $10^{-10}$.
  The history of the Newton residual norms and the final approximate solution
  $\psi^{\text{3D}}\DEF\vz_{22}$ are again visualized in
  figures~\ref{fig:nls:exp:newtonres} and \ref{fig:nls:exp:solutions}.
\end{setup}

\begin{figure}[htbp]
  \centering
  \setlength\figurewidth{0.39\textwidth}
  \setlength\figureheight{0.75\figurewidth}
  \input{figures/pynosh_newton_res_2d.tex}
  \hfill
  \input{figures/pynosh_newton_res_3d.tex}
  \caption{Newton residual history for the two-dimensional
    setup~\ref{set:nls:2d} (left) and three-dimensional setup~\ref{set:nls:3d}
    (right), each with bordering
  % insert after completion -- hardly compatible with 'externalize'
  \eqref{fig:nls:exp:newtonres:bord}
  and without
  % insert after completion -- hardly compatible with 'externalize'
  \eqref{fig:nls:exp:newtonres:nobord}.
  With initial guesses
  $\psi_0^{\text{2D}}(x,y)=\cos(\pi x)$ and $\psi_0^{\text{3D}}\equiv 1$,
  respectively, the Newton process delivered the solutions as
  highlighted in figure~\ref{fig:nls:exp:solutions} in 22 and 27 steps, respectively.}
  \label{fig:nls:exp:newtonres}
\end{figure}

\begin{figure}[htpb]
  \centering
  \setlength\figurewidth{0.37\textwidth}
  \setlength\figureheight{\figurewidth}
  \subcaptionbox{Cooper-pair density $|\psi^{\text{2D}}|^2$.}
    {\input{figures/pynosh_solution_abs_2d.tex}}
  \hspace{0.1\textwidth}
  \subcaptionbox{Cooper-pair density $|\psi^{\text{3D}}|^2$ at the surface of
    the domain.}
    {\input{figures/pynosh_solution_abs_3d.tex}}\\[0.2\figureheight]
  \subcaptionbox{$\arg\psi^{\text{2D}}$.}
    {\input{figures/pynosh_solution_arg_2d.tex}}
  \hspace{0.1\textwidth}
  \subcaptionbox{Isosurface with $|\psi^{\text{3D}}|^2=0.1$ (see (b)),
    $\arg\psi^{\text{3D}}$ at the back sides of the cube.}
    {\input{figures/pynosh_solution_arg_3d.tex}}
  \caption{Solutions of the two-dimensional setup~\ref{set:nls:2d} (left)
    and three-dimensional setup~\ref{set:nls:3d} (right) as found in the
    Newton process illustrated in figure~\ref{fig:nls:exp:newtonres}.}
  \label{fig:nls:exp:solutions}
\end{figure}

All experimental results presented in this section can be reproduced from the
data published with the free and open source Python packages
\emph{PyNosh}~\cite{pynosh} and \emph{KryPy}~\cite{krypy}. \emph{PyNosh}
provides solvers for nonlinear Schr\"odinger equations as outlined in
section~\ref{sec:nls:nls} and the above test
setups~\ref{set:nls:2d} and \ref{set:nls:3d} for the Ginzburg--Landau equation.

Since the Jacobian operators~\eqref{eq:nls:gl:jac} are self-adjoint with respect
to the inner product~\eqref{eq:nls:gl:ip}, the MINRES method can be used. As
proposed by Schl\"omer and Vanroose~\cite{SchV13}, a self-adjoint and
positive-definite preconditioner is used that is based on an algebraic multigrid
solver (AMG) and is able to reduce the number of MINRES iterations dramatically.
Let $\AMG_k(\ofB)$ denote the matrix that applies $k$ cycles of an AMG scheme
with a Hermitian and positive-definite matrix $\ofB$, i.e.,
$\AMG_k(\ofB)\approx\ofB^\inv$. A review of applicable AMG methods can be found
in the article of St\"uben~\cite{Stu01}. Here, the smoothed aggregation AMG
solver with one pre- and one post-smoothing step with symmetric Gauss--Seidel
from the Python package \emph{PyAMG}~\cite{pyamg} is used. The matrix
$\AMG_k(\ofB)$ then also is Hermitian and positive definite. In the setting of
the Ginzburg--Landau equation, a preconditioner for the Jacobian
operator~\eqref{eq:nls:gl:jac} can be constructed by approximately inverting the
matrix
\[
  \ofC_\vv=\ofD^\inv\ofK + 2|\ofD_\vv|=\ofD^\inv(\ofK+2\ofD|\ofD_\vv|)
\]
which is self-adjoint with respect to the inner product~\eqref{eq:nls:gl:ip} and
also positive definite if $A\nequiv 0$ or $\nrm{\psi}\neq 0$. Because
$\ofK+2\ofD|\ofD_\vv|$ is
Hermitian and positive definite with respect to the Euclidean inner product, the
preconditioner can then be defined as
\[
  \oiM_\vv\DEF\AMG_k(\ofK+2\ofD|\ofD_\vv|)\ofD\approx\ofC_\vv^\inv
\]
and it can be verified that $\oiM_\vv$ is self-adjoint and positive definite
with respect to the inner product~\eqref{eq:nls:gl:ip}. More details on this
preconditioner can be found in~\cite{SchV13}.

For a Newton step $i\in\N$, the preconditioned MINRES
algorithm~\ref{alg:back:pminres} is applied to the preconditioned linear
system
\[
  \oiM_i\oiJ_i\delta_i= \oiM_i\vb_i,
\]
where $\oiM_i\DEF\oiM_{\vz_i}$, $\oiJ_i\DEF\oiJ_{\vz_i}$ and
$\vb_i\DEF-\oiS(\vz_i)$. Note that the preconditioner implicitly changes the
inner product in the MINRES algorithm to $\ipdots[\oiM_i^\inv]$ which is
defined for $\vv,\vw\in\C^n$ by
\[
  \ip[\oiM_i^\inv]{\vv}{\vw}=\ip[\R]{\vv}{\oiM_i^\inv\vw}.
\]

\begin{figure}[htbp]
  \setlength{\figureheight}{\textwidth}
  \setlength{\figurewidth}{0.5\textwidth}
  \centering
  \inputplot{exp_pynosh_2d_vanilla_ritzvalues}
  \medskip
  \inputplot{exp_pynosh_3d_vanilla_ritzvalues}
  \caption{Ritz values of $\oiM_i\oiJ_i$ for the two-dimensional
    setup~\ref{set:nls:2d} (top) and the three-dimensional
    setup~\ref{set:nls:3d} (bottom) with respect to the constructed Krylov
    subspace after convergence of the MINRES method without deflation for all
    Newton steps.}
  \label{fig:nls:exp:ritz}
\end{figure}

\begin{figure}[htbp]
  \setlength{\figureheight}{0.5\textwidth}
  \setlength{\figurewidth}{\figureheight}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \inputplot{exp_pynosh_2d_vanilla_resnorms}
    \hfill
    \inputplot{exp_pynosh_3d_vanilla_resnorms}
    \caption{Without deflation.}
    \label{fig:nls:exp:hist:vanilla}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \inputplot{exp_pynosh_2d_fixed_resnorms}
    \hfill
    \inputplot{exp_pynosh_3d_fixed_resnorms}
    \caption{Deflation of 12 Ritz vectors corresponding to the Ritz values of
    smallest magnitude.}
    \label{fig:nls:exp:hist:12}
  \end{subfigure}
\end{figure}

\begin{figure}[htbp]
  \ContinuedFloat
  \setlength{\figureheight}{0.5\textwidth}
  \setlength{\figurewidth}{0.5\textwidth}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \inputplot{exp_pynosh_2d_apriori_resnorms}
    \hfill
    \inputplot{exp_pynosh_3d_apriori_resnorms}
    \caption{Deflation of a variable number of Ritz vectors based on the
      MINRES a priori bound~\eqref{eq:back:minres:bound}.}
    \label{fig:nls:exp:hist:apriori}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \inputplot{exp_pynosh_2d_approx_resnorms}
    \hfill
    \inputplot{exp_pynosh_3d_approx_resnorms}
    \caption{Deflation of a variable number of Ritz vectors based on
      approximate Krylov subspaces.}
    \label{fig:nls:exp:hist:approx}
  \end{subfigure}
  \caption{MINRES convergence histories for all Newton steps for the 2D
    setup~\ref{set:nls:2d} (left) and the 3D setup~\ref{set:nls:3d} (right). The
    shade of the curve corresponds to the Newton step: light red is the first
    Newton step while dark red is the last Newton step.}
  \label{fig:nls:exp:hist}
\end{figure}

In figure~\ref{fig:nls:exp:hist}, the convergence histories of MINRES with
several recycling strategies are plotted for all Newton steps of the 2D setup
(left) and the 3D setup (right). The tolerance for the relative residual norm is
$10^{-10}$ throughout all Newton steps. In order to facilitate the notation,
$\nrm{\vr_n}$ and $\nrm{\vb}$ denote the preconditioned residual and right hand
side in the corresponding norm that is induced by the preconditioner. The
underlying algorithm for all used recycling strategies in the experiments is
algorithm~\ref{alg:rec:def:seq}.

\paragraph{No deflation.}
The convergence of standard MINRES is depicted in
figure~\ref{fig:nls:exp:hist:vanilla}. Towards the end of the Newton process,
numerous phases of stagnation occur (dark red curves). In the last Newton
step for the 2D setup, the residual norm ultimately stagnates above $10^{-9}$.
In order to get some insight into the spectrum of the preconditioned Jacobi
operators $\oiM_i\oiJ_i$, the Ritz values with respect to the constructed Krylov
subspace at the end of each MINRES run are visualized in
figure~\ref{fig:nls:exp:ritz}.

\paragraph{12 vectors.}
In a series of numerical experiments in~\cite[figure~3.4]{GauS13}, it was
analyzed how many Ritz vectors corresponding to the Ritz values of smallest
magnitude lead to the minimal overall time consumption with deflated MINRES for
each Newton step. In later Newton steps, the optimal number of deflation vectors
was found to be around 12 in both the 2D and the 3D setup. Deflation does not
pay off in early Newton steps due to the large changes of the Jacobian operator
and the right hand side. Figure~\ref{fig:nls:exp:hist:12} shows the convergence
history of MINRES if 12 deflation vectors are used throughout Newton's method.
Except for the second Newton step, all phases of stagnation are removed. For
most Newton steps, the number of MINRES iterations is reduced to 40 in the 2D
setup and 80 or less in the 3D setup. Figure~\ref{fig:nls:exp:timings:solve}
compares the timings of the recycling MINRES strategies that are considered in
this section. The fixed number of 12 deflation vectors leads to a significant
reduction of the solution time beyond the 12th Newton step in both setups.
In the last Newton step, the effective run time of MINRES is reduced by roughly
40\%. However, no reduction is achieved for earlier Newton steps and the
solution time even increases in the second Newton step. The second Newton step
stands out in the 2D and 3D setup and also exhibits a special convergence
behavior without deflation and the other recycling strategies that are discussed
below. The distribution of Ritz values in figure~\ref{fig:nls:exp:ritz}
indicates a large number of negative and positive eigenvalues of the
preconditioned Jacobian $\oiM_2\oiJ_2$ in the second Newton step but this
behavior is not fully understood.

\begin{figure}[htbp]
  \setlength{\figureheight}{0.33\textwidth}
  \setlength{\figurewidth}{0.5\textwidth}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \inputplot{exp_pynosh_2d_solve}
    \hfill
    \inputplot{exp_pynosh_3d_solve}
    \caption{Solution time of MINRES.}
    \label{fig:nls:exp:timings:solve}
  \end{subfigure}

  \bigskip

  \begin{subfigure}{\textwidth}
    \inputplot{exp_pynosh_2d_factory}
    \hfill
    \inputplot{exp_pynosh_3d_factory}
    \caption{Time spent on the construction of the deflation vectors (not
      included in~(\protect\subref{fig:nls:exp:timings:solve})).}
    \label{fig:nls:exp:timings:factory}
  \end{subfigure}

  \bigskip

  \begin{subfigure}[b]{\textwidth}
    \inputplot{exp_pynosh_2d_numvecs}
    \hfill
    \inputplot{exp_pynosh_3d_numvecs}
    \caption{Number of used deflation vectors.}
    \label{fig:nls:exp:numvecs}
  \end{subfigure}
  \caption{Timings and number of deflation vectors for the MINRES variants
    in figure~\ref{fig:nls:exp:hist}.}
  \label{fig:nls:exp:timings}
\end{figure}

\medskip

The preceding strategy has two major drawbacks:
\begin{enumerate}
  \item The determination of the number of deflation vectors requires user
    interaction.
  \item The number of deflation vectors is fixed and is not dynamically adapted
    to the actual situation during the Newton process.
\end{enumerate}
Instead of the fixed number of deflation vectors, the strategies from
section~\ref{sec:rec:sel} can be employed. Algorithm~\ref{alg:rec:sel:sa}
provides a basic selection strategy for Ritz vectors that iteratively evaluates
a cost function $\omega$ and picks the set of evaluated Ritz vectors with
minimal cost. The algorithm first evaluates the empty set (no deflation vectors)
and in each iteration, the Ritz vectors corresponding to the extremal Ritz
values (smallest negative, largest negative, smallest negative and largest
positive Ritz value) are evaluated and the one with minimal cost is added for
all further evaluations. Here, the strategy is modified such that only the Ritz
vectors corresponding to the Ritz value of smallest magnitude are considered and
the maximum number of selected Ritz vectors is set to 15 for the 2D setup and 20
for the 3D setup. The strategy is implemented as
\lstinline{krypy.recycling.generators.RitzSmall} in~\cite{krypy}.

In the following, two choices for the cost function $\omega$ are explored. Both
cost functions first estimate the number $m$ of MINRES iterations that are
required until the relative residual norm falls below the prescribed tolerance
$10^{-10}$. Then the cost function uses timings in order to estimate the overall
time that is required for $m$ iterations of deflated MINRES\@. The timings are
gathered on the fly in the preceding MINRES run and include the most expensive
operations in the MINRES method, i.e., the application of the operator $\oiJ_i$,
the preconditioner $\oiM_i$, the evaluation of the inner product $\ipdots[\R]$
and the vector update (\lstinline{daxpy}). It is important to note that the time
estimation includes the time that is required to setup the projection for
deflation as well as the time that is required to apply the projection in each
step of the MINRES method. Also note that the use of timings may result in
slightly different measurements and thus different decisions of the
strategies in each run because the operating system's
scheduler has to deal with varying constellations of concurrent processes in
general.  Although this renders the timing-based algorithms non-deterministic,
the effect of different decisions has not been observed in experiments if the
system load does not change dramatically throughout the different runs. When
comparing results
of the timing-based strategies, it should be kept in mind that the algorithms
adapt to the performance of the specific machine and the implementations of
underlying libraries such as NumPy, SciPy, LAPACK and BLAS.


\paragraph{A priori bound.}
In section~\ref{sec:rec:sel:pri}, it was proposed to use an a priori bound
like the MINRES bound~\eqref{eq:back:minres:bound} in order to assess a given
set of Ritz vectors that are considered for deflation.
Theorem~\ref{thm:rec:sel:pri} provides inclusion intervals for the
nonzero eigenvalues of the deflated operator $\oiP_{\vsW^\perp,\oiB\vsW}\oiB$
based on Ritz pairs of the operator $\oiA$ and the difference $\oiF=\oiB-\oiA$.
The inclusion intervals are defined as intervals around the Ritz values that
correspond to the Ritz vectors that are not used for deflation.
Unfortunately, in this particular application, either the stringent
assumption~\eqref{eq:rec:sel:pri:ass1} of theorem~\ref{thm:rec:sel:pri} on the
spectral gap is not fulfilled (especially in the first Newton steps) or the
resulting eigenvalue inclusion intervals contain zero which clearly renders the
MINRES bound~\eqref{eq:back:minres:bound} useless. In practice, the Ritz
values that correspond to the Ritz vectors that are not used for deflation can
serve as a guideline for the spectrum of the deflated operator and can be used
with the a priori MINRES bound~\eqref{eq:back:minres:bound} to estimate the
number of required MINRES steps. This naive heuristic may provide too optimistic
estimations. For example, if all but one Ritz vectors are chosen as deflation
vectors, the remaining single Ritz value will indicate convergence in one step
via the $\kappa$-bound~\eqref{eq:back:cg:kappa} (which then also holds for the
MINRES residual norm). Although the time estimation then includes the time that
is needed for the application of $\oiJ_i$ and $\oiM_i$ to all but one Ritz
vectors, the resulting timings sometimes indicate to pick all Ritz vectors in
experiments. For this reason, the use of deflation is penalized by
multiplying the estimated time for the projection setup and application with a
factor $\rho>1$. Both the described simple strategy and the interval-based
strategy with the bound from theorem~\ref{thm:rec:sel:pri} are implemented
in~\cite{krypy} as \lstinline{krypy.recycling.evaluators.RitzApriori}.
Figure~\ref{fig:nls:exp:hist:apriori} shows the convergence
histories of MINRES with the simple a priori bound strategy and $\rho=2$. The
convergence behavior is comparable to the one in
figure~\ref{fig:nls:exp:hist:12} where 12 deflation vectors were used throughout
the Newton process.

In figure~\ref{fig:nls:exp:numvecs}, the number of
selected Ritz vectors for deflation is visualized for all Newton steps. In the
2D setup, the number grows to 15 after the second Newton step and then drops to
zero before it starts to settle between 10 and 14 in the last 15 Newton steps.
The alternating pattern between 10 and 14 is not fully understood but may be
related to alternating locations of the smallest eigenvalues of the
preconditioned Jacobian $\oiM_i\oiJ_i$ which is indicated by the changes of the
smallest Ritz values in figure~\ref{fig:nls:exp:ritz}.

Figure~\ref{fig:nls:exp:timings:solve} reveals that the overall solution time
with the a priori bound strategy is almost identical to the recycling strategy
with the manually adjusted number of 12 deflation vectors. The
computation time that is spent on the evaluation of the different selections of
Ritz vectors is marginal and dominated by the explicit formation of the selected
Ritz vectors from the Arnoldi basis and the previous deflation basis. Thus, the
curves of the a priori strategy almost coincide with the static choice of 12
vectors in figure~\ref{fig:nls:exp:timings:factory}.


\paragraph{Approximate Krylov subspaces.}
In section~\ref{sec:rec:sel:kry}, an alternative to the recycling strategy based
on a priori bounds was proposed. Let $\vsK_n$ denote the Krylov subspace that
has been constructed by $n$ iterations of deflated GMRES for the linear system
$\oiA\vx=\vb$ with the $m$-dimensional deflation subspace $\vsU$. If a
$k$-dimensional deflation subspace candidate $\vsW\subseteq\vsK_n+\vsU$ is
given, then theorem~\ref{thm:rec:sel:kry:gmres} provides a bound on the residual
norms $\nrm{\widehat{\vr}_i}$ of deflated GMRES for the linear system
$\oiB\vy=\vc$ with deflation subspace $\vsW$ based on the data that has been
computed by deflated GMRES for the linear system $\oiA\vx=\vb$. The
bound~\eqref{eq:rec:sel:kry:gmres} essentially consists of two terms. The first
one is the residual norm $\nrm[2]{\tilde{\vr}_i}$ that can be computed by GMRES
for the small linear system~\eqref{eq:rec:sel:kry:ls-small} with a (square)
upper Hessenberg matrix $\widehat{\ofH}\in\C^{n+m-k,n+m-k}$. The residual norms
$\nrm[2]{\tilde{\vr}_i}$ equal the residual norms of a perturbed linear system
and can be cheaply computed from $\widehat{\ofH}$ because
it already is an upper Hessenberg matrix and the right hand side is a multiple
of $\ve_1$. The second term in~\eqref{eq:rec:sel:kry:gmres} is based on the
pseudospectrum of an operator $\widehat{\oiA}_i$ and the differences
$\oiG=\oiB-\oiA$ and $\vg=\vc-\vb$. The pseudospectrum of $\widehat{\oiA}_i$
cannot be computed in practice but can be approximated by the pseudospectrum of
$\widehat{\ofH}$, see the discussion following
theorem~\ref{thm:rec:sel:kry:gmres}. The matrix $\widehat{\ofH}$ is Hermitian in
the situation of nonlinear Schr\"odinger equations because $\oiM_i\oiJ_i$ is
self-adjoint. Therefore, the $\delta$-pseudospectrum
$\Lambda_\delta(\widehat{\ofH})$ is easy to compute because it is the union of
symmetric intervals of length $2\delta$ around the eigenvalues of
$\widehat{\ofH}$, i.e.,
\[
  \Lambda_\delta(\widehat{\ofH})
  = \bigcup_{\lambda\in\Lambda(\widehat{\ofH})} [\lambda-\delta,\lambda+\delta].
\]
Although the pseudospectrum can be obtained cheaply in this case, the second
term in the bound~\eqref{eq:rec:sel:kry:gmres} has to be evaluated for a wide
range of the parameter $\delta$ because the bound holds for all
$\delta>\epsilon_i$ and the optimal $\delta$ is usually not known. For each
$\delta$ the maximum of a polynomial $p_i\in\Polys_{n,0}$ with known roots has
to be computed on the pseudospectrum. Besides being exceedingly expensive,
the second term in~\eqref{eq:rec:sel:kry:gmres} often rises above
the first term $\nrm[2]{\tilde{\vr}_i}$ although the actual convergence behavior
is still captured by $\nrm[2]{\tilde{\vr}_i}$; see section~\ref{sec:rec:cd} for
a discussion. For the above reasons, the
strategy for the experiments in this section only determines the first iteration
$i$ such that $\nrm[2]{\tilde{\vr}_i}<10^{-10}$ and then estimates the expected
time as described above. Note that the exact solution of the linear
system~\eqref{eq:rec:sel:kry:ls-small} is found after at most $n+m-k$ steps
because $\widehat{\ofH}\in\C^{n+m-k,n+m-k}$. Note that if all but $j$ Ritz
vectors are considered for deflation then $\tilde{\vr}_j=0$. Therefore, the use
of deflation is penalized as in the case of the a priori bound with the penalty
factor $\rho=2$. It can be seen in figure~\ref{fig:nls:exp:numvecs} that fewer
vectors are chosen with this strategy than with the a priori strategy in almost
all Newton steps. The convergence histories in
figure~\ref{fig:nls:exp:hist:approx} record slightly more MINRES iterations in
some Newton steps but the timings in figure~\ref{fig:nls:exp:timings:solve} show
that the time consumption is similar to the a priori based strategy and the
static choice of 12 vectors. The time that is spent on the construction
of the deflation vectors is larger for the approximate Krylov subspace strategy,
see figure~\ref{fig:nls:exp:timings:factory}. However, it should be noted that
the approximate Krylov subspace strategy only operates with ``small'' matrices
and no applications of the operator $\oiM_i\oiJ_i$ or evaluations of the inner
product $\ipdots[\R]$ are necessary. Thus the time for the selection of Ritz
vectors becomes negligible if the application of $\oiM_i\oiJ_i$ or the
evaluation of inner products consume more time while the number of MINRES
iterations stays roughly the same. This can be observed for the 3D setup in
figure~\ref{fig:nls:exp:timings:solve} with
figure~\ref{fig:nls:exp:timings:factory}.

\medskip

Summarizing, the proposed recycling strategies based on a priori bounds and
approximate Krylov subspaces are able to achieve roughly the same performance in
terms of the overall consumed time as the manually determined optimal choice
of 12 vectors, cf.\ figure~\ref{fig:nls:exp:timings:solve}. The bounds
from sections~\ref{sec:rec:sel:pri} and \ref{sec:rec:sel:kry} result in too
pessimistic estimates and therefore trimmed variants have been used
for the experiments in this section.


\chapter{Discussion and outlook}
\label{ch:con}

In this thesis, several contributions have been made to the mathematical theory
as well as the practical realization of recycling Krylov subspace methods that
are based on deflation. This concluding discussion gives a very brief summary of
the main contributions and points out possibilities for further investigations
that arise from the analysis and experiments in this thesis.

By showing equivalences between several deflated and augmented methods in
section~\ref{sec:rec:def} and characterizing the well-definedness of these
methods, the author wishes to draw attention to the \emph{mathematical} analysis
of deflated methods -- away from the urge to introduce ``new'' methods that
often appear as complicated \emph{algorithms} but are essentially equivalent to
already existing ones. Of course, this does not apply to algorithmic
contributions that are superior, e.g., with respect to their behavior in the
presence of round-off errors. Furthermore, the results show that well-defined
deflated methods can be constructed without hassle by using the most robust
of the already existing implementations of Krylov subspace methods.

Due to their generality, the provided perturbation results for projections in
section~\ref{sec:rec:per:proj} may prove to be useful beyond deflated Krylov
subspace methods. For example, the results can be applied in
infinite-dimensional Hilbert spaces.

In the context of eigenvalue-based a priori bounds for CG and MINRES, the
quadratic residual bound for all eigenvalues of the deflated operator in
theorems~\ref{thm:rec:per:def:quad} and \ref{thm:rec:sel:pri} can provide
significant insight, cf.\
example~\ref{ex:rec:sel:pri:minres}. In the particular application to the
Ginzburg--Landau equation in section~\ref{sec:nls:exp}, either the tough
assumptions of the theorem are not satisfied or the eigenvalue inclusion
intervals contain zero which makes a meaningful automatic selection of Ritz
vectors impossible. Therefore, a simplified selection strategy is employed in
section~\ref{sec:nls:exp} that uses a priori bounds in conjunction with Ritz
values alone. For the Ginzburg--Landau equation, the strategy is able to reduce
the overall computation time like the manually determined optimal number of
deflation vectors. In future works, an unexplored path can be taken by using a
priori bounds for Krylov subspace methods with the new characterization of the
full spectrum of the deflated operator in
theorem~\ref{thm:rec:per:def:spectrum}.

The novel approach from section~\ref{sec:rec:sel:kry} of using an approximate
Krylov subspace in order to estimate the convergence behavior yields positive
results for MINRES and, more interestingly, for GMRES with a non-normal
operator. Unlike asymptotic bounds, the approximate Krylov subspace bound is
able to capture an altering convergence behavior accurately in
example~\ref{ex:rec:sel:kry:minres} and the convection-diffusion problem in
section~\ref{sec:rec:cd}. The experiments reveal that the first term of
bound~\eqref{eq:rec:sel:kry:gmres} describes the convergence behavior well
beyond the point where the pseudospectral term causes the bound to grow. The
pseudospectral approach seems to severely overestimate the effects of
perturbations and it remains to be explored if another approach can do better.
However, note that example~\ref{ex:rec:per:kry:stag} shows that a small
perturbation of the operator or the right hand side may actually result in a
drastic change of the convergence behavior of Krylov subspace methods. Because
the pseudospectral term is exceedingly expensive and often overestimates the
residual norms, only the cheaply computable residual norms from the approximate
Krylov subspace are used in the experiments in section~\ref{sec:nls:exp} for the
automatic selection of Ritz vectors for recycling. Analogous to the strategy
based on a priori bounds, also the approximate Krylov subspace strategy yields
the same overall time consumption as with the manually determined optimal number
of deflation vectors.

For practical applications, the automatic selection of deflation vectors based
on the proposed bounds is attractive. Given candidates of deflation vectors, the
proposed recycling strategies pick the choice that yields the best overall time
estimation according to an a priori bound or the approximate Krylov subspace
bound. In order to estimate the time accurately, the implementation keeps track
of the timings for the most expensive operations. In prior algorithms in the
literature, the number of used deflation vectors had to be specified manually.
Besides requiring the user to specify the number of vectors, a fixed number of
deflation vectors may also lead to an increased overall computation time for a
poor choice of vectors. For the experiments in this thesis, the Ritz vectors
corresponding to the Ritz values of smallest magnitude performed best as
deflation vectors and were used with the proposed strategies.  Harmonic Ritz
vectors did not lead to a better performance. However, other choices may perform
better and the challenging question of the optimal choice of deflation vectors
from a given subspace remains open. General statements about the optimal choice
of deflation vectors seem unrealistic without a better understanding of the
convergence behavior of the underlying Krylov subspace methods. For a sequence
of linear systems, the situation is even more complicated because the
convergence behavior cannot be deduced with the help of simple measures such
as the norm of the difference between subsequent operators and right hand sides.
As explained in section~\ref{sec:rec:sel}, a heuristic should be expected at
some point in order to obtain an efficient method. The results in
section~\ref{sec:nls:exp} show that the developed strategies drive the existing
heuristics forward and result in efficient methods that require no manually
tuned parameters.

Besides the mathematical results, the author wishes to make recycling Krylov
subspace methods accessible to a broader audience by providing the free software
package KryPy~\cite{krypy} which allows to use a recycling solver with a few
lines of code, see appendix~\ref{ch:krypy}.


\appendix
\chapter{Documentation of the KryPy software package}
\label{ch:krypy}

The KryPy~\cite{krypy} software package is free software and contains
implementations of standard and deflated versions of CG, MINRES and GMRES as
well as all recycling strategies that are discussed in this thesis. All methods
are very flexible, e.g., by allowing to use non-Euclidean inner products and
several orthogonalization strategies like Lanczos, (iterated) Gram--Schmidt or
Householder orthogonalization. Furthermore, KryPy contains a rich toolbox for
Krylov subspace methods such as methods for the computation of Ritz pairs,
angles between subspaces, convergence bounds and stable implementations of
projections that are of importance in deflated methods. This chapter provides
minimalistic code snippets in order to get started with KryPy. The full
documentation of KryPy can be found at \url{http://krypy.readthedocs.org}.

\paragraph{Requirements.}
Python (2.7 or $\geq$3.2) and the modules NumPy ($\geq$1.7) and SciPy
($\geq$0.12) are required to run KryPy. Optionally, PseudoPy ($\geq$1.2.1)
enables the computation of the pseudospectral bounds from this thesis.

\paragraph{Installation.}
KryPy can be installed easily with the Python package installer by issuing
\lstinline{pip install krypy}. Alternatively, it can be installed by downloading
the source code from \url{https://github.com/andrenarchy/krypy} and then running
\lstinline{python setup.py install}.

\paragraph{First steps.}
The following code uses MINRES to solve a linear system with an indefinite
diagonal matrix:
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize]
from numpy import diag, linspace, ones, eye
from krypy.linsys import LinearSystem, Minres

# construct the linear system
A = diag(linspace(1, 2, 20))
A[0, 0] = -1e-5
b = ones(20)
linear_system = LinearSystem(A, b, self_adjoint=True)

# solve the linear system (approximate solution is solver.xk)
solver = Minres(linear_system)
\end{lstlisting}

\paragraph{Deflation.}
The vector $\ve_1$ can be used as a deflation vector to get rid of the small
negative eigenvalue $-10^{-5}$:
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize]
from krypy.deflation import DeflatedMinres
dsolver = DeflatedMinres(linear_system, U=eye(20, 1))
\end{lstlisting}

\paragraph{Recycling.}
The deflation subspace can also be determined automatically with a recycling
strategy. Just for illustration, the same linear system is solved twice in the
following code:
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize]
from krypy.recycling import RecyclingMinres

# get recycling solver with approximate Krylov subspace strategy
rminres = RecyclingMinres(vector_factory='RitzApproxKrylov')

# solve twice
rsolver1 = rminres.solve(linear_system)
rsolver2 = rminres.solve(linear_system)
\end{lstlisting}
The convergence histories can be plotted by
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize]
from matplotlib.pyplot import semilogy, show, legend
semilogy(solver.resnorms, label='original')
semilogy(dsolver.resnorms, label='exact deflation', ls='dotted')
semilogy(rsolver2.resnorms, label='automatic recycling',
         ls='dashed')
legend()
show()
\end{lstlisting}
which results in the following figure.
\begin{center}
  \setlength{\figureheight}{0.5\textwidth}
  \setlength{\figurewidth}{0.65\textwidth}
  \inputplot{exp_krypy}
\end{center}

\setlength{\emergencystretch}{0.7em}
\printbibliography[heading=bibintoc]{}

\end{document}
